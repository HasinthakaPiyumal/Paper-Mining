[
  {
    "Pattern Name": "LLM-Augmented Knowledge Base",
    "Problem": "Traditional knowledge graphs for recommender systems are often sparse, limited, and expensive to construct or complete, leading to ignored user preferences and suboptimal recommendations. They also lack comprehensive cross-domain information.",
    "Context": "Recommender systems that rely on knowledge graphs for rich semantic information, improved accuracy, and explainability.",
    "Solution": "Leverage Large Language Models (LLMs) for their ability to retrieve factual knowledge, complete missing facts, and construct knowledge graphs (including entity discovery, coreference resolution, and relation extraction) from text corpora. LLMs can also distill common sense facts into knowledge graphs.",
    "Result": "More extensive, up-to-date, and comprehensive knowledge graphs, leading to enhanced recommendation accuracy, relevance, personalization, and improved cross-domain recommendation capabilities.",
    "Related Patterns": [
      "LLM as Semantic Content Encoder",
      "Tool-Augmented LLM"
    ],
    "Uses": "Recommender systems, cross-domain recommendations, knowledge graph completion, knowledge graph construction."
  },
  {
    "Pattern Name": "LLM as Semantic Content Encoder",
    "Problem": "Traditional content-based recommenders struggle to capture deep semantic representations and world knowledge from textual features, leading to limited understanding of item properties and user preferences. This also impacts cold-start and cross-domain recommendation scenarios.",
    "Context": "Recommender systems that need to process and understand textual content (e.g., item descriptions, reviews, news articles) to infer user preferences and item properties.",
    "Solution": "Utilize pretrained Large Language Models (LLMs) as powerful semantic encoders to transform textual data into rich, context-aware feature embeddings. This involves adapting LLMs through fine-tuning or task-specific pretraining to align with recommendation objectives. Techniques like knowledge distillation and model optimization can be employed to reduce inference latency for online serving.",
    "Result": "Enhanced understanding and interpretation of textual content, improved feature representations, better capture of user interests and item properties, and alleviation of cold-start and cross-domain recommendation challenges.",
    "Related Patterns": [
      "Instruction Tuning for Recommendation",
      "LLM-Augmented Knowledge Base"
    ],
    "Uses": "Content-based recommender systems, news recommendation, sequential recommendation, tag recommendation, cold-start recommendation, cross-domain recommendation."
  },
  {
    "Pattern Name": "Instruction Tuning for Recommendation",
    "Problem": "General-purpose Large Language Models (LLMs) are not inherently optimized for diverse recommendation tasks and may perform poorly in zero-shot or few-shot scenarios without specific adaptation.",
    "Context": "Adapting general LLMs to effectively perform various recommendation tasks (e.g., rating prediction, item recommendation, sequential recommendation, personalized search) and generalize across domains.",
    "Solution": "Formulate recommendation tasks as instruction-following procedures. This involves designing various instruction templates to accommodate different recommendation tasks, generating high-quality instruction data by converting user interaction history, retrieved candidates, and potentially LLM-generated reasoning features into natural language instructions, and then fine-tuning LLMs with this instruction data.",
    "Result": "LLMs demonstrate superior performance in few-shot learning and cross-domain generalization for recommendation tasks, effectively handling a wide range of user information requirements.",
    "Related Patterns": [
      "LLM as Semantic Content Encoder",
      "LLM as Direct Recommender (via In-Context Learning)",
      "LLM as Conversational Recommender Agent",
      "LLM as Personalized AIGC Creator"
    ],
    "Uses": "Sequential recommendation, rating prediction, item recommendation, personalized search, cross-domain recommendation, domain adaptation for conversational agents."
  },
  {
    "Pattern Name": "LLM as Explainable Recommender",
    "Problem": "Traditional recommender systems are often black boxes, leading to a lack of user trust. Existing explanation methods are often inflexible, lack diversity, coherence, and generalizability across different recommendation models.",
    "Context": "Recommender systems where user trust and understanding of recommendations are crucial, and explanations need to be natural, customized, and model-agnostic.",
    "Solution": "Leverage Large Language Models (LLMs) for generating natural language explanations. LLMs can craft customized, precise, and adaptable explanations by harnessing their understanding of human language, context, and complex syntax. They utilize in-context learning (zero-shot, few-shot, Chain-of-Thought prompting) to generate explanations in real-time, incorporating user feedback and fostering human-machine alignment. This approach provides model-agnostic interpretations, explaining the reasoning behind recommendations from various underlying models.",
    "Result": "Improved transparency, persuasiveness, and reliability of recommendations; enhanced user trust and satisfaction; more diverse, coherent, and adaptable explanations; and a versatile, scalable interpretational framework.",
    "Related Patterns": [
      "Chain-of-Thought Prompting for Reasoning",
      "LLM as Direct Recommender (via In-Context Learning)"
    ],
    "Uses": "Explainable recommender systems, drug recommendations, general AI model interpretation."
  },
  {
    "Pattern Name": "LLM as Direct Recommender (via In-Context Learning)",
    "Problem": "Adapting traditional recommendation models often requires extensive tuning and data. There's a need for flexible, quick adaptation to new recommendation tasks or domains, especially in zero-shot or few-shot scenarios.",
    "Context": "Recommender systems requiring rapid deployment or adaptation to new tasks/domains, or operating with limited explicit training data.",
    "Solution": "Utilize Large Language Models (LLMs) with their in-context learning capabilities to directly generate recommendations. This involves providing natural language instructions and/or a few input-output pairs (shots) as demonstrations within the prompt. LLMs then generate recommendations (e.g., rating predictions, item rankings) without explicit fine-tuning. For ranking tasks, a candidate generation module might be integrated to narrow down items before LLM reranking.",
    "Result": "Enables zero-shot and few-shot recommendations, quick adaptation to new tasks, and leverages LLMs' commonsense knowledge. Performance can be improved with multi-step reasoning strategies like Chain-of-Thought prompting.",
    "Related Patterns": [
      "Chain-of-Thought Prompting for Reasoning",
      "Instruction Tuning for Recommendation",
      "LLM as Explainable Recommender"
    ],
    "Uses": "Rating prediction, ranking prediction, sequential recommendation, direct recommendation, open-domain recommendations (movies, books)."
  },
  {
    "Pattern Name": "Chain-of-Thought Prompting for Reasoning",
    "Problem": "Large Language Models (LLMs) may struggle with complex tasks requiring multi-step reasoning or logical deduction, leading to suboptimal or incorrect conclusions.",
    "Context": "Tasks that can be broken down into intermediate steps, where explicit reasoning paths can guide the LLM towards a more accurate final answer.",
    "Solution": "Employ Chain-of-Thought (CoT) prompting strategies. This involves providing the LLM with prompts that include previous intermediate reasoning steps, or explicitly instructing the LLM to 'think step by step.' This can be combined with multi-step prompt designs (e.g., NIR for recommendation).",
    "Result": "Elicits emergent reasoning abilities in LLMs, enabling them to solve complex tasks by breaking them into subproblems, improving accuracy and performance in tasks like mathematical word problems or multi-step recommendation.",
    "Related Patterns": [
      "LLM as Explainable Recommender",
      "LLM as Direct Recommender (via In-Context Learning)",
      "Tool-Augmented LLM"
    ],
    "Uses": "Complex task solving, mathematical word problems, multi-step recommendation, automated selection, general reasoning tasks."
  },
  {
    "Pattern Name": "LLM for AutoML and Architecture Search",
    "Problem": "Automated Machine Learning (AutoML) and Neural Architecture Search (NAS) are computationally expensive and complex, requiring iterative sampling and evaluation of architectures or features.",
    "Context": "Optimizing ML models (e.g., recommender systems) by automatically searching for optimal embedding sizes, features, feature interactions, or model architectures.",
    "Solution": "Leverage Large Language Models (LLMs) for their generative, memorization, and reasoning capabilities to assist or perform AutoML tasks. This includes generating network architectures directly, acting as black-box agents to propose better-performing architectures based on previous trials, or integrating LLMs into existing search strategies (e.g., genetic algorithms) as mutation and crossover operators to generate candidate architectures or modifications.",
    "Result": "Reduces the search space, generates reasonable architectures, and potentially improves the efficiency and effectiveness of AutoML processes for recommender systems and other ML tasks.",
    "Related Patterns": [],
    "Uses": "Neural Architecture Search (NAS), automated feature selection, automated feature interaction search, general AutoML."
  },
  {
    "Pattern Name": "LLM as Conversational Recommender Agent",
    "Problem": "Building effective conversational recommender systems (CRS) requires real-time understanding of user intent, adaptation to feedback, and handling domain-specific knowledge and long conversation contexts. General LLMs lack awareness of private domain data and have token limits for long dialogues.",
    "Context": "Developing interactive recommender systems that engage users in natural language dialogue to uncover preferences and provide personalized recommendations.",
    "Solution": "Employ Large Language Models (LLMs) as the core of conversational recommender agents. This involves domain adaptation through fine-tuning LLMs with private, domain-specific dialogue data (potentially generated by LLM-based user simulators). It also includes tool integration, treating traditional recommendation models as external tools that the LLM can invoke to obtain recommendations, and memory augmentation by incorporating memory modules or user profile modules to store and retrieve meaningful, enduring facts about users from long conversations, overcoming token limits.",
    "Result": "Enables LLMs to act as intelligent conversational agents, providing personalized recommendations, understanding user intent in real-time, adapting to feedback, and handling domain-specific knowledge and long dialogue histories.",
    "Related Patterns": [
      "Tool-Augmented LLM",
      "Memory-Augmented LLM",
      "Instruction Tuning for Recommendation"
    ],
    "Uses": "Conversational recommender systems, personalized assistance, customer service chatbots."
  },
  {
    "Pattern Name": "Memory-Augmented LLM",
    "Problem": "Large Language Models (LLMs) have limited context windows (token limits), making it challenging to maintain coherence and leverage historical information in long conversations or when dealing with extensive user profiles.",
    "Context": "LLM-based applications (e.g., conversational agents, personalized systems) that require retaining and utilizing information from extended interactions or large knowledge bases beyond the immediate prompt.",
    "Solution": "Augment LLMs with external memory modules. This involves extracting meaningful and enduring facts about users from historical conversations and storing them in a dedicated user memory (e.g., a factual statement database). When processing new user queries, relevant facts are retrieved from the memory based on text similarity or other indexing mechanisms and incorporated into the LLM's prompt to provide context and enhance its long-dialogue memory capability.",
    "Result": "Overcomes LLM token limits, improves comprehension and coherence in long conversations, enables better utilization of historical user information, and enhances the accuracy of personalized responses.",
    "Related Patterns": [
      "LLM as Conversational Recommender Agent",
      "Tool-Augmented LLM"
    ],
    "Uses": "Conversational AI, personalized assistants, long-context understanding, user profile management."
  },
  {
    "Pattern Name": "Tool-Augmented LLM",
    "Problem": "Large Language Models (LLMs) have impressive general knowledge and reasoning but lack specific, up-to-date, or private domain knowledge, struggle with complex computations, and cannot directly interact with external systems or real-world environments. This limits their task-solving capabilities.",
    "Context": "LLM-based systems needing to perform complex tasks that require specialized knowledge, real-time data, external computation, or interaction with other software/APIs.",
    "Solution": "Augment LLMs with external tools, where the LLM acts as a controller or orchestrator. The LLM comprehends user input, breaks down complex tasks into subtasks, and decides which specialized tools (e.g., search engines, recommendation engines, calculators, databases, other AI models, APIs) to invoke. It generates both reasoning paths and task-specific actions alternately (e.g., ReAct), delegating action execution to tools and using external feedback to guide further reasoning. Finally, the LLM integrates the outputs from the tools to complete the end-to-end task and present a coherent response. Advanced approaches may even empower LLMs to directly generate new tools.",
    "Result": "Enhances LLMs' task-solving capabilities, provides access to external, up-to-date, and domain-specific knowledge, enables complex computations, and allows interaction with real-world systems, overcoming limitations like hallucinations and lack of domain awareness.",
    "Related Patterns": [
      "LLM as Conversational Recommender Agent",
      "Chain-of-Thought Prompting for Reasoning",
      "LLM-Augmented Knowledge Base",
      "Memory-Augmented LLM"
    ],
    "Uses": "Conversational AI, personalized systems, complex task automation, question answering, code generation, visual tasks, web browsing, scientific reasoning, robotics."
  },
  {
    "Pattern Name": "LLM as Personalized AIGC Creator",
    "Problem": "Traditional recommender systems only suggest existing items. Creating customized, appealing content (e.g., ad titles, descriptions, images, music) that precisely matches individual user interests and preferences is labor-intensive and difficult to scale. User feedback for content generation is often sparse.",
    "Context": "E-commerce, online advertising, customer service, and other domains where personalized, dynamically generated content can enhance user engagement and experience.",
    "Solution": "Leverage Large Language Models (LLMs) and AI-Generated Content (AIGC) techniques to create personalized content. LLMs reason about user personalized intent and interests from instructions or feedback, then generate various forms of content (text, images, multimodal) based on their knowledge and the inferred user intent. Strategies like Reinforcement Learning from Human Feedback (RLHF) or iterative conversational feedback are employed to fine-tune LLMs, allowing them to better capture explicit user preferences and guide content generation. LLMs' cross-modal knowledge bases can be utilized for realistic and diverse content creation.",
    "Result": "Efficient and accessible creation of highly customized and appealing content, improved user engagement, alleviation of sparse feedback problems in content generation, and enhanced personalized experiences across various business scenarios.",
    "Related Patterns": [
      "Instruction Tuning for Recommendation",
      "LLM as Conversational Recommender Agent"
    ],
    "Uses": "Online advertising, e-commerce product descriptions, customer service chatbots, personalized media generation (images, music)."
  }
]