[
  {
    "Pattern Name": "ReAct (Reasoning and Acting)",
    "Problem": "Large Language Models (LLMs) struggle with complex tasks requiring both dynamic reasoning and interaction with external environments. Traditional approaches often separate reasoning (e.g., Chain-of-Thought) and acting, leading to issues like hallucination, error propagation in reasoning, or a lack of abstract reasoning and working memory in acting.",
    "Context": "LLMs applied to tasks requiring both language-based reasoning and interaction with external environments or tools (e.g., APIs, simulated worlds).",
    "Solution": "Augment the LLM's action space to include both domain-specific actions and freeform language 'thoughts' (reasoning traces). These thoughts are interleaved with actions and observations, allowing the model to dynamically plan, track progress, handle exceptions, and incorporate external information (act to reason) while also using reasoning to guide actions (reason to act).",
    "Result": "Improved performance, interpretability, trustworthiness, and robustness across diverse language reasoning and decision-making tasks, overcoming limitations of reasoning-only or acting-only approaches. It enables dynamic planning, self-correction, and grounding in external facts.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting",
      "Self-Consistency (CoT-SC)",
      "Combining Internal and External Knowledge",
      "Inner Monologue (IM)",
      "SayCan"
    ],
    "Uses": "Question Answering (HotpotQA), Fact Verification (Fever), Text-based Games (ALFWorld), Web Navigation (WebShop), Human-in-the-loop behavior correction, general task solving for agentic LLMs."
  },
  {
    "Pattern Name": "Chain-of-Thought (CoT) Prompting",
    "Problem": "Large Language Models (LLMs) often struggle with complex reasoning tasks, producing incorrect direct answers without showing their intermediate steps, making their reasoning opaque and prone to errors.",
    "Context": "LLMs used for tasks requiring multi-step reasoning, such as arithmetic, commonsense reasoning, symbolic reasoning, or complex question answering.",
    "Solution": "Prompt the LLM to generate a series of intermediate reasoning steps (a 'chain of thought') before providing the final answer. This is typically achieved by providing a few in-context examples that demonstrate this step-by-step reasoning process.",
    "Result": "Elicits emergent reasoning capabilities in LLMs, significantly improving performance on complex reasoning tasks and making the model's decision-making process more interpretable. However, it can suffer from hallucination and lack of grounding in external facts.",
    "Related Patterns": [
      "ReAct (Reasoning and Acting)",
      "Self-Consistency (CoT-SC)",
      "Least-to-Most Prompting",
      "Zero-shot Chain-of-Thought (Zero-shot CoT)",
      "Selection-Inference",
      "STaR (Self-Taught Reasoner)",
      "Faithful Reasoning",
      "Scratchpads",
      "Combining Internal and External Knowledge"
    ],
    "Uses": "Arithmetic, commonsense reasoning, symbolic reasoning, question answering, fact verification."
  },
  {
    "Pattern Name": "Self-Consistency (CoT-SC)",
    "Problem": "Chain-of-Thought (CoT) reasoning can be brittle, and a single reasoning path generated by an LLM might lead to an incorrect answer due to minor errors or suboptimal choices.",
    "Context": "Improving the robustness and accuracy of Chain-of-Thought reasoning in LLMs for complex tasks.",
    "Solution": "Generate multiple diverse Chain-of-Thought reasoning paths by sampling from the LLM (e.g., using a higher decoding temperature). Then, select the most consistent answer by taking a majority vote among the final answers derived from these multiple reasoning paths.",
    "Result": "Significantly boosts performance over vanilla CoT by leveraging the diversity of reasoning paths and reducing reliance on a single, potentially flawed, trace, leading to more robust and accurate results.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting",
      "ReAct (Reasoning and Acting)",
      "Combining Internal and External Knowledge"
    ],
    "Uses": "Reasoning tasks where CoT is applied, question answering, fact verification."
  },
  {
    "Pattern Name": "Combining Internal and External Knowledge",
    "Problem": "LLMs possess strong internal knowledge and reasoning capabilities (e.g., CoT) but can hallucinate or provide outdated information. Conversely, external tools (e.g., ReAct's actions with APIs) provide factual, up-to-date information but might lack flexible reasoning structure or suffer from search errors.",
    "Context": "Knowledge-intensive tasks where both the LLM's internal knowledge and access to external, up-to-date information are crucial for accurate and robust problem-solving.",
    "Solution": "Integrate approaches that leverage internal LLM knowledge (like Self-Consistency) with approaches that interact with external tools (like ReAct). The model dynamically switches between these methods based on heuristics, such as backing off to internal reasoning if external tool interaction fails, or consulting external tools if internal reasoning is not confident.",
    "Result": "Achieves superior overall performance by synergizing the strengths of both internal and external knowledge sources, leading to more factual, grounded, and robust reasoning while mitigating individual weaknesses.",
    "Related Patterns": [
      "ReAct (Reasoning and Acting)",
      "Self-Consistency (CoT-SC)",
      "Chain-of-Thought (CoT) Prompting"
    ],
    "Uses": "Question Answering (HotpotQA), Fact Verification (Fever)."
  },
  {
    "Pattern Name": "Inner Monologue (IM)",
    "Problem": "Embodied agents need to reason about their actions and environment state to achieve goals, but simple action prediction might lack sufficient context, high-level planning, or a mechanism for internal state tracking.",
    "Context": "Embodied agents operating in interactive environments, requiring internal state tracking and goal-oriented behavior.",
    "Solution": "Inject feedback from the environment as an 'inner monologue' into the agent's input. This monologue is typically limited to observations of the environment state and what needs to be completed for the goal, providing a form of internal reflection to motivate actions.",
    "Result": "Motivates actions and provides some form of internal state representation, improving performance in embodied tasks. However, it can be less flexible and comprehensive than more advanced reasoning patterns like ReAct, as it's often limited to reactive feedback rather than proactive, diverse reasoning.",
    "Related Patterns": [
      "ReAct (Reasoning and Acting)",
      "SayCan"
    ],
    "Uses": "Robotic action planning, embodied agents in interactive environments."
  },
  {
    "Pattern Name": "Least-to-Most Prompting",
    "Problem": "Complex reasoning tasks are difficult for LLMs to solve directly in a single step or with simple Chain-of-Thought, especially when they involve many interdependent subproblems.",
    "Context": "LLMs tackling multi-step, complicated reasoning problems that can be naturally decomposed into a sequence of simpler subproblems.",
    "Solution": "Decompose the main problem into a series of simpler, dependent subproblems. The LLM first solves the initial subproblem, then uses its solution to help formulate and solve the next subproblem, and so on, until the final solution is reached. This sequential problem-solving leverages the LLM's ability to build upon previous results.",
    "Result": "Enables LLMs to tackle more complex reasoning tasks by breaking them down into manageable steps, significantly improving accuracy and capability on problems that are otherwise intractable.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting"
    ],
    "Uses": "Complicated reasoning tasks, multi-step problem solving."
  },
  {
    "Pattern Name": "Zero-shot Chain-of-Thought (Zero-shot CoT)",
    "Problem": "Standard Chain-of-Thought (CoT) prompting typically requires a few in-context examples to demonstrate the reasoning process, which can be costly to create or unavailable in certain scenarios.",
    "Context": "Applying Chain-of-Thought reasoning with LLMs when no specific in-context examples are provided or practical to include in the prompt.",
    "Solution": "Elicit Chain-of-Thought reasoning by simply adding a generic phrase like 'Let's think step by step' to the prompt, without providing any example reasoning traces. This phrase acts as an instruction to the LLM to generate intermediate steps.",
    "Result": "Enables CoT reasoning in a zero-shot setting, making the technique more broadly applicable and reducing the need for example engineering, while still eliciting reasoning capabilities.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting"
    ],
    "Uses": "Any task where CoT is applicable but few-shot examples are not practical or available."
  },
  {
    "Pattern Name": "Selection-Inference",
    "Problem": "Multi-step logical reasoning with LLMs can be prone to errors and lack transparency, making it hard to verify the reasoning process and ensure faithfulness to the given premises.",
    "Context": "LLMs performing interpretable and accurate logical reasoning, especially in multi-step scenarios where the derivation of conclusions needs to be clear.",
    "Solution": "Decompose the reasoning process into two distinct steps: a 'selection' step where the LLM identifies and extracts only the relevant information or premises from a larger context, and an 'inference' step where it draws conclusions based *only* on the previously selected information.",
    "Result": "Improves interpretability and accuracy for logical reasoning by structuring the process and making the intermediate steps explicit and verifiable, enhancing the faithfulness of the reasoning.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting",
      "Faithful Reasoning"
    ],
    "Uses": "Interpretable logical reasoning, multi-step reasoning."
  },
  {
    "Pattern Name": "STaR (Self-Taught Reasoner)",
    "Problem": "Acquiring large datasets of high-quality, human-annotated reasoning rationales for fine-tuning LLMs is expensive and time-consuming, limiting the scalability of reasoning improvements.",
    "Context": "Improving LLM reasoning capabilities through fine-tuning, especially when human-annotated rationales are scarce or difficult to obtain at scale.",
    "Solution": "Bootstrap the reasoning process by having the LLM generate its own rationales for problem-solving. These self-generated rationales are then filtered (e.g., by checking if they lead to correct answers) and used to fine-tune the model, iteratively improving its reasoning abilities without extensive human annotation.",
    "Result": "Enables LLMs to learn and improve their reasoning capabilities with less reliance on expensive human-annotated rationales, making the process more scalable and efficient for enhancing reasoning.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting"
    ],
    "Uses": "Improving reasoning in LLMs, generating rationales for fine-tuning."
  },
  {
    "Pattern Name": "Faithful Reasoning",
    "Problem": "Ensuring that LLM-generated multi-step reasoning is faithful to the premises and does not introduce ungrounded or hallucinated facts, which is a common challenge in complex reasoning.",
    "Context": "Multi-step reasoning tasks where faithfulness, verifiability, and adherence to given information at each step are critical.",
    "Solution": "Decompose the multi-step reasoning process into several distinct steps, with each step potentially performed by a dedicated or specialized LLM. This modular approach allows for more control and verification at each stage, ensuring that the reasoning remains faithful to the input and intermediate derivations.",
    "Result": "Improves the faithfulness and accuracy of multi-step reasoning by enforcing a structured and verifiable process, reducing the likelihood of hallucination and ungrounded conclusions.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting",
      "Selection-Inference"
    ],
    "Uses": "Multi-step reasoning requiring high faithfulness and verifiability."
  },
  {
    "Pattern Name": "Scratchpads",
    "Problem": "LLMs often struggle with multi-step computation problems, making errors in intermediate steps or failing to track complex calculations accurately.",
    "Context": "LLMs performing tasks that involve explicit multi-step computations, arithmetic, or symbolic manipulation where intermediate steps are crucial.",
    "Solution": "Fine-tune the LLM to explicitly generate and use intermediate computation steps, much like a human uses a scratchpad. This involves training the model to output these intermediate steps alongside the final answer, making the computation process transparent and learnable.",
    "Result": "Improves performance on multi-step computation problems by making the intermediate steps explicit and learnable, enhancing the model's ability to track and execute complex calculations accurately.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting"
    ],
    "Uses": "Multi-step computation, arithmetic, symbolic reasoning."
  },
  {
    "Pattern Name": "SayCan",
    "Problem": "LLMs can generate high-level plans for embodied agents, but these plans often lack grounding in the physical environment, leading to unexecutable, unsafe, or inefficient actions.",
    "Context": "Using LLMs for robotic action planning in real-world or simulated embodied environments where physical constraints, affordances (what actions are possible), and visual grounding are important.",
    "Solution": "Combine LLM-generated action predictions with an 'affordance model' that is grounded in the visual or physical environment. The LLM suggests possible actions based on high-level goals, and the affordance model then re-ranks or filters these actions based on their physical feasibility and likelihood of success in the current environment state.",
    "Result": "Enables LLMs to generate grounded, executable, and safe plans for robots, bridging the gap between high-level language instructions and low-level physical actions by incorporating real-world constraints.",
    "Related Patterns": [
      "Inner Monologue (IM)",
      "ReAct (Reasoning and Acting)"
    ],
    "Uses": "Robotic action planning, embodied AI, grounding language in physical actions."
  }
]