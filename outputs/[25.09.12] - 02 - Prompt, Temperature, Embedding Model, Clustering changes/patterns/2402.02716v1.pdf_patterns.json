[
  {
    "Pattern Name": "Task Decomposition",
    "Problem": "Complex, multi-step tasks are difficult for LLM agents to plan directly in a single step.",
    "Context": "LLM-based agents facing complicated, real-world tasks with variability.",
    "Solution": "Adopt the 'divide and conquer' idea, decomposing the complicated task into several simpler subtasks, and then sequentially planning for each subtask. This can be done by decomposing all subtasks first or interleaving decomposition with subtask planning.",
    "Result": "Simplifies complex tasks, making them manageable for LLM agents. Reduces the risk of task forgetting and hallucinations (decomposition-first) or improves fault tolerance (interleaved decomposition).",
    "Related Patterns": [
      "Multiplan Selection",
      "External Planner-Aided Planning",
      "Reflection and Refinement",
      "Memory-Augmented Planning"
    ],
    "Uses": "General LLM agent planning for complex tasks. Specific methods include:\n- **Decomposition-First:** HuggingGPT (multimodal tasks), PlanandSolve (mathematical, commonsense, symbolic reasoning), ProgPrompt (robot task planning).\n- **Interleaved Decomposition:** Chain-of-Thought (CoT) series (reasoning about complex problems), ReAct (decoupling reasoning and planning), Visual ChatGPT (image processing capabilities), PAL (mathematical and symbolic reasoning by generating code), Program-of-Thought (PoT) (formalizing reasoning as programming)."
  },
  {
    "Pattern Name": "Multiplan Selection",
    "Problem": "A single plan generated by an LLM is likely to be suboptimal or even infeasible due to task complexity and LLM's inherent uncertainty.",
    "Context": "LLM agents generating plans for complex tasks where plan quality and feasibility are critical.",
    "Solution": "Generate various alternative plans for a task (multiplan generation) and then employ a task-related search algorithm to select the optimal plan (optimal plan selection).",
    "Result": "Provides a broader exploration of potential solutions, increasing the likelihood of finding an optimal or feasible plan.",
    "Related Patterns": [
      "Task Decomposition",
      "External Planner-Aided Planning",
      "Reflection and Refinement",
      "Memory-Augmented Planning"
    ],
    "Uses": "Improving plan quality and robustness for LLM agents. Specific methods include:\n- **Multiplan Generation:** Self-consistency (sampling distinct reasoning paths), Tree-of-Thought (ToT) (sample and propose strategies), Graph-of-Thought (GoT) (extends ToT with thought aggregation), LLMMCTS, RAP (LLM as heuristic policy for MCTS).\n- **Optimal Plan Selection:** Majority vote (Self-consistency), Tree search algorithms (BFS, DFS in ToT), Monte Carlo Tree Search (LLMMCTS, RAP), A* algorithm (LLM-A*)."
  },
  {
    "Pattern Name": "External Planner-Aided Planning",
    "Problem": "LLMs struggle with intricate constraints (e.g., mathematical problems, generating admissible actions) and efficiency issues, despite their powerful reasoning and task decomposition capabilities.",
    "Context": "LLM agents needing to generate plans that adhere to strict constraints or require high efficiency, especially in domains with well-defined planning models.",
    "Solution": "Integrate LLMs with external planners (symbolic or neural). The LLM primarily formalizes tasks and provides additional reasoning information, while the external planner handles the constrained planning or efficient execution.",
    "Result": "Elevates the planning procedure by addressing issues of efficiency and infeasibility of generated plans. Combines LLM's semantic understanding and code generation capabilities with the external planner's precision, theoretical completeness, stability, and interpretability.",
    "Related Patterns": [
      "Task Decomposition",
      "Multiplan Selection",
      "Reflection and Refinement",
      "Memory-Augmented Planning"
    ],
    "Uses": "Planning in environments with intricate constraints, mathematical problem-solving, generating admissible actions, improving planning efficiency. Specific methods include:\n- **Symbolic Planner Integration:** LLMP, LLMDP, LLMPDDL (formalizing tasks into PDDL and using solvers like FastDownward, BFS, LPG), LLMASP (transforming problems into ASP and using CLINGO).\n- **Neural Planner Integration:** CALM (combining language model with RL-based neural planner for action generation/reranking), SwiftSage (dual-process theory with DT model for fast thinking and LLM for slow thinking)."
  },
  {
    "Pattern Name": "Reflection and Refinement",
    "Problem": "LLM agents may make errors, get stuck in thought loops, or suffer from hallucinations during planning due to limited feedback and insufficient reasoning abilities for complex problems.",
    "Context": "LLM agents operating in environments where errors can occur, and fault tolerance and error correction are critical.",
    "Solution": "Encourage the LLM to reflect on failures or generated plans, generate feedback (self-reflection or external validation), and then refine the plan iteratively. This process mimics reinforcement learning updates through textual feedback.",
    "Result": "Enhances fault tolerance and error correction capabilities, allowing agents to correct errors and break out of loops. Improves plan quality and reliability over iterations.",
    "Related Patterns": [
      "Task Decomposition",
      "Multiplan Selection",
      "External Planner-Aided Planning",
      "Memory-Augmented Planning"
    ],
    "Uses": "Improving robustness and reliability of LLM agent planning. Specific methods include: SelfRefine (iterative generation, feedback, refinement), Reflexion (evaluator assesses trajectories, LLM generates self-reflections upon error detection), CRITIC (uses external tools like Knowledge Bases and Search Engines for validation and self-correction), InteRecAgent (ReChain mechanism for self-correction), LEMA (gathers mistaken planning samples, uses powerful LLM for correction, then finetunes)."
  },
  {
    "Pattern Name": "Memory-Augmented Planning",
    "Problem": "LLM agents need to leverage valuable information (commonsense knowledge, past experiences, domain-specific knowledge) to enhance planning capabilities and support growth, but LLMs have context length limitations and may 'forget' information.",
    "Context": "LLM agents requiring access to long-term or external knowledge to inform planning, especially for tasks requiring accumulated experience or domain-specific facts.",
    "Solution": "Enhance planning with an extra memory module where valuable information is stored and retrieved when planning, serving as auxiliary signals. This can involve external retrieval or embedding memories into model parameters.",
    "Result": "Enhances planning capabilities, growth, and fault tolerance by providing access to stored knowledge and experiences, overcoming context length limitations.",
    "Related Patterns": [
      "Task Decomposition",
      "Multiplan Selection",
      "External Planner-Aided Planning",
      "Reflection and Refinement"
    ],
    "Uses": "Improving planning with historical data, commonsense knowledge, and domain-specific information. Specific methods include:\n- **RAG-based Memory:** Generative Agents, MemoryBank, TiM, RecMind (storing and retrieving text memories), MemGPT (multi-level storage abstraction), REMEMBER (Q-value table for positive/negative memories).\n- **Embodied Memory (Finetuning-based):** CALM (finetuning GPT2 with action trajectories), TDT (finetuning Text Decision Transformer with MDP data), AgentTuning (finetuning LLaMA with plan trajectories in dialogue form)."
  }
]