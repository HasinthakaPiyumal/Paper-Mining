[
  {
    "Pattern Name": "Retrieval-Augmented Generation (RAG)",
    "Problem": "Large pretrained language models (LLMs) exhibit limitations in knowledge-intensive tasks, including: 1) difficulty in accessing and precisely manipulating factual knowledge stored implicitly in their parameters, 2) a tendency to hallucinate or generate factually incorrect information, 3) lack of provenance or explainability for their predictions, 4) inability to easily expand or revise their world knowledge without costly retraining, and 5) lagging performance compared to task-specific architectures on knowledge-intensive tasks.",
    "Context": "Designing AI systems for knowledge-intensive Natural Language Processing (NLP) tasks such as open-domain question answering, abstractive summarization, fact verification, and complex text generation. This pattern is applicable when factual accuracy, up-to-date information, and interpretability are critical, and when leveraging large pretrained language models while needing to overcome their inherent limitations regarding external, dynamic knowledge.",
    "Solution": "Combine a pretrained *parametric memory* (a seq2seq language model, e.g., BART) with an *explicit nonparametric memory* (a dense vector index of external knowledge, e.g., Wikipedia). A pretrained neural retriever (e.g., Dense Passage Retriever - DPR) is used to dynamically access relevant information from the nonparametric memory based on the input query. The retriever converts the input into a dense vector and performs Maximum Inner Product Search (MIPS) against the document index. The generator model then conditions its output on both the original input sequence and the retrieved documents. The entire system is finetuned end-to-end, treating the retrieved documents as latent variables. The document encoder and index can be kept fixed during finetuning for efficiency, while the query encoder and generator are updated. Two main approaches for marginalizing over retrieved documents are: \n1. **RAG-Sequence:** The model uses the same retrieved document to generate the complete output sequence, marginalizing over the top-K retrieved documents to compute the sequence probability.\n2. **RAG-Token:** The model can draw a different latent document for each target token during generation, allowing it to combine information from multiple documents. \nThe nonparametric memory is designed to be human-readable (raw text) and human-writable, facilitating interpretability and dynamic updates.",
    "Result": "1) **Improved Factual Accuracy & Reduced Hallucinations:** Generates more factual, specific, and diverse language, significantly reducing hallucinations compared to parametric-only models. \n2) **State-of-the-Art Performance:** Achieves state-of-the-art results on various knowledge-intensive NLP tasks, including open-domain question answering. \n3) **Dynamic Knowledge Updates:** Enables easy and dynamic updating of the model's world knowledge by simply replacing or editing the nonparametric memory (index hotswapping) without requiring costly retraining of the parametric model. \n4) **Enhanced Interpretability:** The explicit nature of the nonparametric memory (raw text documents) allows for inspection of the accessed knowledge, providing a form of interpretability. \n5) **Flexibility:** Combines the generation flexibility of closed-book models with the performance of open-book retrieval-based approaches. \n6) **Unified Architecture:** Provides a general-purpose finetuning recipe applicable across a wide range of seq2seq tasks.",
    "Related Patterns": [],
    "Uses": [
      "Open-domain Question Answering (QA)",
      "Abstractive Question Answering",
      "Jeopardy Question Generation",
      "Fact Verification",
      "General knowledge-intensive Natural Language Processing (NLP) tasks"
    ]
  }
]