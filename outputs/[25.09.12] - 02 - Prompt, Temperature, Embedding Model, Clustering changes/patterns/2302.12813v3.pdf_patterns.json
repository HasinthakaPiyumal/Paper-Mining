[
  {
    "Pattern Name": "Knowledge-Grounded Generation",
    "Problem": "Large Language Models (LLMs) tend to hallucinate and lack access to up-to-date or domain-specific external knowledge, leading to ungrounded or inaccurate responses, especially for mission-critical applications.",
    "Context": "Building AI systems that require LLMs to generate factually accurate and reliable responses based on external, dynamic, or proprietary information, particularly when fine-tuning the LLM is prohibitively expensive or impossible (black-box LLMs).",
    "Solution": "Augment the LLM's input prompt with consolidated evidence retrieved from external knowledge sources. This involves a modular 'Knowledge Consolidator' component that:\n1.  **Retrieves:** Generates search queries based on user input and dialog history, then calls APIs (e.g., web search, task-specific databases) to fetch raw evidence.\n2.  **Links:** Enriches raw evidence by identifying and linking entities to related contextual information (e.g., Wikipedia descriptions).\n3.  **Chains:** Prunes irrelevant information and forms coherent 'evidence chains' that are most relevant to the query.\nThis consolidated evidence is then explicitly included in the prompt provided to the LLM.",
    "Result": "LLM generates responses that are factually grounded in external knowledge, significantly reducing hallucinations and improving the accuracy and trustworthiness of its outputs without requiring model fine-tuning.",
    "Related Patterns": [
      "Automated Feedback Loop for LLM Refinement",
      "Agentic Iterative Self-Refinement"
    ],
    "Uses": "Open-domain question answering, information-seeking dialog, customer service, factual content generation, any application where LLM responses must be verifiable and accurate."
  },
  {
    "Pattern Name": "Automated Feedback Loop for LLM Refinement",
    "Problem": "Initial LLM-generated responses may not consistently meet desired quality criteria (e.g., factuality, coherence, task-specific alignment, safety) and require iterative improvement without direct human intervention for every turn.",
    "Context": "Developing robust AI agents or systems that leverage black-box LLMs where direct model parameter modification is not feasible, and continuous self-correction is needed to align outputs with specific requirements.",
    "Solution": "Implement a 'Utility' module that evaluates candidate LLM responses and generates actionable feedback. This module:\n1.  **Evaluates:** Assigns a utility score to a candidate response based on a set of task-specific utility functions (e.g., model-based functions trained on human preferences for fluency, informativeness, factuality; or rule-based functions checking compliance).\n2.  **Generates Feedback:** If the response's utility score falls below a threshold, a verbalized feedback message is generated (e.g., 'The response is inconsistent with the knowledge. Please generate again.'). This feedback can be generated by a text generation model or rule-based natural language generator.\nThis feedback is then used to revise the prompt for the next LLM generation attempt.",
    "Result": "Enables the LLM system to self-correct and iteratively refine its responses, leading to higher quality, better-aligned, and more reliable outputs by guiding the LLM towards desired characteristics.",
    "Related Patterns": [
      "Knowledge-Grounded Generation",
      "Agentic Iterative Self-Refinement"
    ],
    "Uses": "Reducing hallucination, improving factual consistency, aligning responses with conversational goals, enhancing safety and compliance, self-criticism for LLMs."
  },
  {
    "Pattern Name": "Agentic Iterative Self-Refinement",
    "Problem": "Complex, multi-step tasks or dynamic conversational scenarios often require an AI system to adapt, learn, and refine its actions and responses over time, especially when dealing with the inherent variability and potential for errors in LLM outputs.",
    "Context": "Designing AI agents that can engage in long-horizon interactions, perform multi-hop reasoning, or handle tasks requiring continuous adaptation and improvement, often by orchestrating multiple AI components and external tools.",
    "Solution": "Structure the AI system as an agent operating within a Markov Decision Process (MDP) framework, employing a continuous loop of observation, action, and refinement. Key components include:\n1.  **Working Memory:** Maintains the current dialog state, including user queries, retrieved evidence, candidate responses, utility scores, and feedback.\n2.  **Policy:** Selects the next optimal action (e.g., acquire evidence, query LLM, send final response) based on the current state. This policy can be rule-based or learned (e.g., via reinforcement learning).\n3.  **Action Executor:** Carries out the selected action, which may involve:\n    -   **Knowledge Consolidator:** Retrieving and processing external knowledge.\n    -   **Prompt Engine:** Constructing prompts for the LLM, incorporating context, knowledge, and feedback.\n4.  **Utility Module:** Evaluates LLM responses and generates feedback.\nThe agent iteratively revises its prompts and re-queries the LLM based on the feedback and consolidated knowledge until a satisfactory response is achieved or a stopping condition is met.",
    "Result": "Creates a robust and adaptive AI agent capable of handling complex tasks, performing multi-hop reasoning, and significantly improving the quality and groundedness of LLM responses through continuous self-correction and external knowledge integration.",
    "Related Patterns": [
      "Knowledge-Grounded Generation",
      "Automated Feedback Loop for LLM Refinement"
    ],
    "Uses": "Complex conversational AI, multi-step task execution, autonomous agents, systems requiring dynamic adaptation and self-improvement."
  }
]