[
  {
    "Pattern Name": "LLM as a Planner",
    "Problem": "Existing methods for embodied agents require large amounts of labeled data for planning, hindering versatility and quick task learning. Static plans generated by LLMs may not adapt to dynamic environments.",
    "Context": "Embodied agents (e.g., robots) need to follow natural language instructions to complete complex tasks in visually-perceived, partially-observable environments, often with limited training data.",
    "Solution": "Leverage Large Language Models (LLMs) to directly generate high-level plans (sequences of subgoals) given a natural language instruction, often in a few-shot setting using in-context learning. This approach can be integrated into hierarchical planning models.",
    "Result": "Enables few-shot, sample-efficient planning for embodied agents, reducing data cost and allowing agents to learn new tasks quickly. Generates plausible high-level plans that can be further refined.",
    "Related Patterns": [
      "Hierarchical Planning",
      "Grounded Replanning",
      "LLM as an Auxiliary Helper",
      "LLM as a Skill Ranker"
    ],
    "Uses": [
      "Robotics",
      "Vision-and-language navigation",
      "Embodied AI",
      "Task completion"
    ]
  },
  {
    "Pattern Name": "Hierarchical Planning",
    "Problem": "Complex, long-horizon tasks for embodied agents are difficult to plan directly and require managing different levels of abstraction.",
    "Context": "Embodied agents following multi-step instructions in diverse and partially-observable environments, where tasks involve both high-level goals and low-level primitive actions.",
    "Solution": "Decompose planning into a high-level planner and a low-level planner. The high-level planner (e.g., an LLM) generates a sequence of subgoals (e.g., 'Navigation potato', 'Pickup potato'). The low-level planner then maps each subgoal into a sequence of primitive actions (e.g., 'move forward', 'turn left') to achieve that subgoal in the current environment. Low-level planning becomes conditionally independent of the natural language instruction given the high-level plan.",
    "Result": "Simplifies complex planning by breaking it into manageable levels. Improves effectiveness in complex vision-and-language navigation (VLN) tasks and can be advantageous in low-data regimes.",
    "Related Patterns": [
      "LLM as a Planner",
      "Grounded Replanning"
    ],
    "Uses": [
      "Robotics",
      "Vision-and-language navigation",
      "Embodied AI"
    ]
  },
  {
    "Pattern Name": "Grounded Replanning",
    "Problem": "Static plans generated by LLMs often lack physical grounding to the current environment, leading to incorrect object references, unattainable plans, and agents getting stuck during execution (e.g., taking too long, failed attempts).",
    "Context": "An embodied agent is executing an LLM-generated high-level plan in a dynamic, partially-observable physical environment. The initial plan may not account for real-time environmental conditions or agent perception.",
    "Solution": "Implement a closed-loop mechanism where the LLM can dynamically adapt its high-level plan based on feedback from the environment. When the agent fails to execute an action, takes too long to complete a subgoal, or makes too many failed attempts, replanning is triggered. The list of objects perceived in the environment (from an object detector) and the partially completed plan are injected into the prompt for the LLM, which then generates a new, more grounded continuation of the plan. Logit biases can be used to prioritize observed objects.",
    "Result": "Enables LLMs to produce more grounded and attainable plans, helps the agent overcome difficult situations, and dynamically adapts plans to environmental changes, improving task completion success. Creates a dynamic feedback loop between the environment and the LLM.",
    "Related Patterns": [
      "LLM as a Planner",
      "Hierarchical Planning"
    ],
    "Uses": [
      "Embodied AI",
      "Robotics",
      "Vision-and-language navigation",
      "Dynamic task execution"
    ]
  },
  {
    "Pattern Name": "LLM as an Auxiliary Helper",
    "Problem": "Main models (e.g., vision-language models) may struggle with certain aspects of embodied tasks, such as inferring object locations or generating relevant landmarks, without additional knowledge.",
    "Context": "An embodied agent needs supplementary information to improve the performance of its primary decision-making or perception module, but direct LLM-based planning might be too complex or inefficient for the specific sub-problem.",
    "Solution": "Use an LLM not as the primary planner, but as an auxiliary component to generate relevant, helpful information. Examples include prompting LLMs to provide the most likely location of a goal object or generating a list of landmarks for a vision-language model to use. This information then assists the main model in its task.",
    "Result": "Enhances the capabilities of existing vision-language models or main decision-making systems by providing contextually relevant information, potentially improving accuracy or efficiency for specific sub-tasks.",
    "Related Patterns": [
      "LLM as a Planner",
      "LLM as a Skill Ranker"
    ],
    "Uses": [
      "Vision-and-language navigation",
      "Object localization",
      "Contextual information generation"
    ]
  },
  {
    "Pattern Name": "LLM as a Skill Ranker",
    "Problem": "In environments where all admissible skills (action-object pairs) can be enumerated upfront, selecting the optimal next action is a challenge, especially in complex environments with many objects, leading to high LLM call costs and efficiency deterioration.",
    "Context": "An embodied agent operates in an environment where it can enumerate all possible admissible actions/skills for the current state. The goal is to choose the most appropriate skill to execute next based on a natural language instruction.",
    "Solution": "Instead of generating a plan, use an LLM to rank a predefined list of admissible skills or actions. The LLM evaluates each potential skill based on the natural language instruction and the current environment context, and the highest-ranked skill is chosen for execution. This approach often assumes the agent has sufficient a priori knowledge of the environment to compile the skill list.",
    "Result": "Allows LLMs to contribute to decision-making by evaluating pre-defined options. However, it can lead to high computational costs due to multiple LLM calls per step and may not be feasible in partially-observable or highly diverse environments where admissible actions are hard to enumerate.",
    "Related Patterns": [
      "LLM as a Planner",
      "LLM as an Auxiliary Helper"
    ],
    "Uses": [
      "Embodied AI",
      "Action selection in environments with enumerable skills"
    ]
  }
]