[
  {
    "Pattern Name": "Adaptive Retrieval-Augmented Generation (AdaptiveRAG)",
    "Problem": "Retrieval-Augmented Large Language Models (LLMs) often employ a 'one-size-fits-all' strategy, which leads to inefficiencies for simple queries (unnecessary computational overhead) or inadequacies for complex, multi-step queries (failure to address them effectively). Real-world user requests exhibit a wide range of complexities, making static approaches suboptimal.",
    "Context": "LLM-based applications, particularly in Question Answering (QA), that need to deliver accurate and efficient responses by incorporating external knowledge, where the incoming user queries vary significantly in their complexity (from straightforward to multi-hop reasoning).",
    "Solution": "A novel adaptive framework that dynamically selects the most suitable retrieval-augmented LLM strategy based on the assessed complexity of the incoming query. This selection is operationalized by a 'Query Complexity Classifier' and allows for adaptation between three core strategies: 1) A non-retrieval approach for straightforward queries, 2) A single-step retrieval approach for queries of moderate complexity requiring external knowledge, and 3) A multi-step (iterative) retrieval approach for complex queries necessitating extensive reasoning and information synthesis from multiple sources.",
    "Result": "Enhances the overall efficiency and accuracy of QA systems by balancing computational resources with task requirements. It provides a robust middle ground, preventing unnecessary overhead for simple queries while ensuring comprehensive handling of complex ones.",
    "Related Patterns": "Query Complexity Classifier",
    "Uses": "Open-domain Question Answering, intelligent chatbots, dynamic resource management for LLM services, personalized AI model routing."
  },
  {
    "Pattern Name": "Query Complexity Classifier",
    "Problem": "Dynamically adapting AI system strategies (e.g., choosing between different RAG methods) requires an automated and accurate assessment of the input query's complexity. However, pre-annotated datasets for query-complexity pairs are typically unavailable, making supervised training challenging.",
    "Context": "AI systems, such as adaptive RAG frameworks, that need to adjust their operational behavior or select appropriate sub-models based on the intrinsic difficulty or type of the input query, in scenarios where human-labeled complexity data is scarce.",
    "Solution": "Train a smaller, dedicated Language Model (Classifier) to predict the complexity level of a given query. The training dataset for this classifier is automatically constructed without human labeling by leveraging two main strategies: 1) 'Outcome-based labeling' (silver data) where queries are labeled based on the successful performance of different existing AI strategies (e.g., if a non-retrieval LLM answers correctly, it's 'straightforward'). Simpler models are prioritized in case of ties. 2) 'Inductive bias labeling' where inherent characteristics of existing benchmark datasets (e.g., queries from single-hop QA datasets are labeled 'moderate', multi-hop datasets are labeled 'complex') are used to assign labels to previously unlabeled queries.",
    "Result": "Enables the dynamic selection of optimal AI strategies by providing an automated and resource-efficient mechanism for query complexity assessment. This leads to improved overall performance and efficiency of the adaptive AI system, and reduces the dependency on costly manual data annotation.",
    "Related Patterns": "Adaptive Retrieval-Augmented Generation (AdaptiveRAG)",
    "Uses": "Adaptive RAG, dynamic routing in conversational AI, intelligent workload distribution for AI inference, context-aware model selection."
  }
]