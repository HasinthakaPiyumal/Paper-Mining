[
  {
    "Pattern Name": "Retrieval Augmented Fine Tuning (RAFT)",
    "Problem": "Pretrained Large Language Models (LLMs) struggle to accurately answer questions in specialized, 'open-book' domains when presented with external documents, especially when retrieval is imperfect (i.e., includes irrelevant 'distractor' documents). Existing finetuning or in-context learning methods do not adequately prepare LLMs for this specific challenge.",
    "Context": "Adapting LLMs for domain-specific Retrieval-Augmented Generation (RAG) tasks where the LLM needs to leverage a given set of documents (e.g., legal, medical, enterprise, code repositories) and be robust to noisy retrieval outputs that may contain irrelevant information. The primary goal is to maximize accuracy based on provided documents rather than general knowledge.",
    "Solution": "A novel finetuning recipe that trains the LLM on question-answer pairs (Q-A) alongside a meticulously prepared set of documents (Dk). This set includes 'golden documents' (containing the answer) and 'distractor documents' (irrelevant). The answers (A) are generated in a Chain-of-Thought style, including verbatim citations from the relevant documents. A crucial aspect is that for a portion (1-P fraction) of the training data, the golden document is intentionally omitted, compelling the model to learn robustness against missing context or to memorize when appropriate.",
    "Result": "Significantly improves LLM performance in domain-specific RAG settings, enhances the model's ability to discern and disregard irrelevant information, and boosts factual accuracy and explainability by teaching it to extract and cite relevant information effectively. It consistently outperforms supervised finetuning (with or without RAG) and general-purpose models with RAG.",
    "Related Patterns": [
      "Retrieval-Augmented Generation (RAG)",
      "Training with Distractor Documents",
      "Chain-of-Thought Reasoning"
    ],
    "Uses": "Domain-specific Question Answering, enhancing LLM robustness to imperfect retrieval, improving factual consistency and explainability in RAG systems, adapting LLMs to specialized knowledge domains.",
    "Category": "MLOps, LLM-specific, Prompt Design, Knowledge & Reasoning"
  },
  {
    "Pattern Name": "Training with Distractor Documents",
    "Problem": "Large Language Models (LLMs) are vulnerable to irrelevant text (distractors) within the context provided by retrieval systems, particularly in top-k RAG scenarios where high recall often means including some noise. Training solely with perfectly relevant documents diminishes the model's ability to discern pertinent information from noise.",
    "Context": "Designing robust Retrieval-Augmented Generation (RAG) systems where the LLM needs to process a context window that may contain a mix of relevant and irrelevant information from external sources, and the model must learn to focus only on the pertinent parts.",
    "Solution": "During the LLM finetuning process, intentionally compose training examples by including both 'golden' (relevant) documents and 'distractor' (irrelevant) documents in the input context. Experiment with varying numbers of distractor documents and, for a proportion of the training data, even omit the golden document to further compel the model to handle uncertainty or memorization.",
    "Result": "Enhances the LLM's robustness against irrelevant text, improves its ability to discern and disregard irrelevant content, and makes the model more resilient to fluctuations in the number and quality of documents encountered during testing.",
    "Related Patterns": [
      "Retrieval Augmented Fine Tuning (RAFT)"
    ],
    "Uses": "Improving LLM robustness against noisy retrieval, enhancing context processing capabilities, preventing distraction by irrelevant information in RAG, preparing models for real-world imperfect retrieval scenarios.",
    "Category": "MLOps, LLM-specific"
  },
  {
    "Pattern Name": "Chain-of-Thought Reasoning",
    "Problem": "Large Language Models (LLMs) can struggle with complex, multi-step reasoning tasks, often providing direct answers without showing their derivation. This can lead to a lack of transparency, reduced accuracy for intricate problems, and potential overfitting to concise answer formats.",
    "Context": "Training or prompting LLMs for tasks requiring logical deduction, multi-hop question answering, or improved explainability, where the process of arriving at an answer is as important as the answer itself.",
    "Solution": "Generate training data or craft prompts that explicitly guide the LLM to produce an intermediate, step-by-step reasoning process (a 'chain of thought') that leads to the final answer. In the context of Retrieval Augmented Generation (RAG), this often includes citing specific parts of the provided context verbatim to justify each step.",
    "Result": "Improves the model's ability to perform complex reasoning, enhances overall accuracy, makes training more robust by enriching the model's understanding, and provides greater transparency into the model's decision-making process.",
    "Related Patterns": [
      "Retrieval Augmented Fine Tuning (RAFT)"
    ],
    "Uses": "Complex Question Answering, improving LLM reasoning capabilities, enhancing explainability and trustworthiness, robust model training.",
    "Category": "Prompt Design, Knowledge & Reasoning"
  },
  {
    "Pattern Name": "Retrieval-Augmented Generation (RAG)",
    "Problem": "Large Language Models (LLMs) have a knowledge cutoff date (limited by their training data), may 'hallucinate' incorrect facts, or lack specific, up-to-date, or proprietary domain expertise, making them unsuitable for applications requiring high factual accuracy or current information.",
    "Context": "Deploying LLMs in applications where access to external, dynamic, or specialized knowledge bases is critical for generating accurate, relevant, and reliable responses. The LLM needs to integrate current or domain-specific information beyond its pre-trained knowledge.",
    "Solution": "Integrate a retrieval module with an LLM. Given a user query, the retriever fetches relevant documents, passages, or data snippets from an external knowledge source (e.g., database, document store, web search). These retrieved documents are then appended to the user's prompt as additional context, allowing the LLM to generate an informed response grounded in external evidence.",
    "Result": "Enables LLMs to leverage external, up-to-date, and domain-specific information; significantly reduces factual errors and hallucinations; improves the relevance, accuracy, and trustworthiness of generated content; and allows LLMs to adapt to new information without full retraining.",
    "Related Patterns": [
      "Retrieval Augmented Fine Tuning (RAFT)"
    ],
    "Uses": "Knowledge-intensive Question Answering, factual summarization, enterprise search, chatbots, content generation requiring external validation, reducing LLM hallucinations.",
    "Category": "Generative AI, LLM-specific, Knowledge & Reasoning, Tools Integration"
  }
]