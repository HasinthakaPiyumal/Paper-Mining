[
  {
    "Pattern Name": "Retriever-Aware Training (RAT)",
    "Problem": "Large Language Models (LLMs) struggle to effectively utilize retrieved documentation, especially when it is imperfect, outdated, or irrelevant. This can lead to decreased accuracy and increased hallucination when LLMs are tasked with using external tools like APIs, and they fail to adapt to dynamic changes in tool specifications.",
    "Context": "Training LLMs to interact with external tools or APIs where the model relies on a document retriever to provide relevant context (e.g., API documentation) at inference time. The retrieved information might not always be perfectly accurate or up-to-date.",
    "Solution": "During the instruction-finetuning process, augment the user prompt with the relevant retrieved documentation (which may contain imperfections or be outdated, mimicking a real retriever's output). Crucially, provide the *accurate ground-truth API call* in the LLM's response. This approach teaches the LLM to critically evaluate the retrieved context, utilize it when relevant, and rely on its baked-in domain-specific knowledge (or ignore irrelevant context) when the retrieval is poor.",
    "Result": "The LLM demonstrates improved accuracy, significantly reduced hallucination errors, and enhanced ability to adapt dynamically to test-time changes in API documentation and tool specifications. It learns to 'judge the retriever's' relevance and quality.",
    "Related Patterns": "Instruction-Tuned API Invocation",
    "Uses": "Training LLMs for robust tool use, API invocation, agentic systems requiring dynamic knowledge access, mitigating hallucination stemming from retrieved context, ensuring adaptability to evolving external systems."
  },
  {
    "Pattern Name": "Instruction-Tuned API Invocation",
    "Problem": "General-purpose Large Language Models (LLMs) often lack awareness of the vast and frequently updated landscape of available APIs and struggle with the specific knowledge and syntactic precision required to accurately generate API calls based on natural language instructions. This limits their practical utility in effectively using external tools.",
    "Context": "Developing LLM-powered applications that require the LLM to interact with external systems, services, or libraries by generating precise, functionally correct API calls, often from a large and evolving set of potential tools.",
    "Solution": "Employ a self-instruct paradigm to create a specialized, comprehensive dataset comprising natural language instructions paired with their corresponding ground-truth API calls (including relevant packages and explanations). Then, instruction-finetune a base LLM (e.g., LLaMA) on this dataset, framing the training as a user-agent chat-style conversation. This process imbues the LLM with domain-specific knowledge and the ability to generate syntactically correct and functionally appropriate API calls.",
    "Result": "The finetuned LLM gains a strong capability to accurately select and invoke APIs, reason about user-defined constraints (e.g., performance, parameters, accuracy), and generate actionable code. This approach significantly outperforms un-finetuned or few-shot prompted general LLMs for complex API-related tasks.",
    "Related Patterns": "Retriever-Aware Training (RAT)",
    "Uses": "Building LLM agents for software development, automating tasks requiring external tool use, intelligent code generation, integrating LLMs with complex systems, enabling LLMs to act as 'flexible interfaces' to the digital world."
  },
  {
    "Pattern Name": "AST-based Code/API Evaluation",
    "Problem": "Evaluating the functional correctness and detecting hallucinations in LLM-generated code or API calls is challenging. Semantic equivalence is difficult to verify through traditional unit tests due to multiple valid solutions, and standard Natural Language Processing (NLP) metrics fail to capture the structural and functional accuracy required for code.",
    "Context": "Assessing the quality, correctness, and reliability of code or API calls generated by Large Language Models, particularly in scenarios where the generated output needs to be syntactically correct, functionally equivalent to a reference, and free from imagined (hallucinated) elements.",
    "Solution": "Implement an evaluation framework that utilizes Abstract Syntax Tree (AST) subtree matching. For each generated API call, construct its AST. Compare this AST against a comprehensive database of known, correct API call ASTs. Functional correctness is determined by whether the generated AST (or a relevant part of it) matches a subtree within the reference database. Hallucination is specifically defined and detected when a generated API call's AST does not match any known API in the reference database, indicating an entirely imagined tool or structure.",
    "Result": "Provides a robust, objective, and scalable offline metric for precisely measuring functional correctness and identifying hallucination errors in LLM-generated code. This method demonstrates a strong correlation with human evaluation, making it an efficient alternative to manual validation.",
    "Related Patterns": [],
    "Uses": "Benchmarking LLMs for code generation capabilities, automated testing of API invocation systems, developing hallucination detection mechanisms for LLM outputs, MLOps for code quality assurance in AI systems, and evaluating the adherence of generated code to specific constraints."
  }
]