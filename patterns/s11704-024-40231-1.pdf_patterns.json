[
  {
    "Pattern Name": "Agent Profiling",
    "Problem": "To enable autonomous agents to perform tasks by assuming specific roles (e.g., coders, teachers, domain experts) and to influence their Large Language Model (LLM) behaviors effectively.",
    "Context": "LLM-based autonomous agents operating in various application scenarios where distinct personalities, expertise, or social interactions are required.",
    "Solution": "Indicate the agent's profiles, typically by writing this information into the prompt. Profiles can encompass basic (age, gender, career), psychological (personalities), and social information (relationships between agents). Three common strategies for creating profiles are:\n1.  **Handcrafting:** Manually specifying profiles (e.g., 'you are an outgoing person').\n2.  **LLM-generation:** Automatically generating profiles based on rules and optional seed examples using LLMs.\n3.  **Dataset alignment:** Obtaining profiles from real-world datasets to accurately reflect attributes of a real population.",
    "Result": "Serves as the foundation for agent design, significantly influencing the agent's memorization, planning, and action procedures, allowing for more human-like or role-specific behaviors.",
    "Related Patterns": [
      "Memory-Augmented Agent",
      "LLM as a Planner",
      "Multi-Agent Debate"
    ],
    "Uses": "Generative Agent (defining name, objectives, relationships), MetaGPT (predefining roles and responsibilities in software development), ChatDev, Self-collaboration, PTLLM (exploring personality traits), RecAgent (generating diverse user profiles for recommendations), American National Election Studies (assigning demographic roles to GPT-3 for social simulation)."
  },
  {
    "Pattern Name": "Memory-Augmented Agent",
    "Problem": "Large Language Models (LLMs) have limited context windows, making it challenging for autonomous agents to accumulate experiences, self-evolve, maintain consistent behavior over long interactions, and perform long-range reasoning in dynamic environments.",
    "Context": "LLM-based autonomous agents that need to store information perceived from the environment, recall past behaviors, and use recorded memories to facilitate future actions.",
    "Solution": "Equip the agent with a memory module that stores and manages information. This module often incorporates principles from human memory, such as:\n*   **Hybrid Memory Structure:** Explicitly modeling short-term memory (input information within the context window) for recent perceptions and long-term memory (external vector storage, e.g., vector database) for consolidating important information over extended periods.\n*   **Diverse Memory Formats:** Storing information in natural language, embeddings (for retrieval efficiency), databases (for efficient manipulation via SQL), or structured lists (e.g., hierarchical trees for goals/plans).\n*   **Memory Operations:** Implementing mechanisms for:\n    *   **Memory Reading:** Extracting meaningful information based on criteria like recency, relevance, and importance.\n    *   **Memory Writing:** Storing new information, addressing memory duplication (e.g., condensing similar sequences, count accumulation) and overflow (e.g., explicit deletion, FIFO overwriting).\n    *   **Memory Reflection:** Emulating human ability to independently summarize past experiences into broader, more abstract insights and infer high-level information.",
    "Result": "Enhances the agent's ability to perceive recent contexts, accumulate valuable experiences, perform long-range reasoning, and behave in a more consistent, reasonable, and effective manner in complex environments.",
    "Related Patterns": [
      "Agent Profiling",
      "LLM as a Planner",
      "Planning with Feedback",
      "Self-Driven Evolution",
      "Trial-and-Error Learning Agent",
      "Tool-Augmented LLM Agent"
    ],
    "Uses": "Generative Agent, AgentSims, Reflexion, GITM (Ghost in the Minecraft), SCM, SimplyRetrieve, MemorySandbox, MemoryBank, ChatDB, DBGPT, RETLLM, Voyager."
  },
  {
    "Pattern Name": "LLM as a Planner",
    "Problem": "Autonomous agents need to break down complex, long-horizon tasks into simpler subtasks and generate effective action plans, which is difficult to do directly and often requires human-like strategic thinking.",
    "Context": "LLM-based autonomous agents facing complex tasks that require multi-step reasoning and decomposition to achieve a final goal.",
    "Solution": "Leverage the inherent reasoning capabilities of Large Language Models (LLMs) to generate plans. This can be achieved through:\n*   **Single-path Reasoning:** Decomposing the final task into a sequence of intermediate steps, where each step leads to a single subsequent step (e.g., Chain of Thought prompting).\n*   **Multi-path Reasoning:** Organizing reasoning steps into a tree-like or graph structure, allowing for exploration of multiple choices or 'thoughts' at each intermediate step, with selection based on LLM evaluation (e.g., Tree of Thoughts, Self-consistent CoT).\n*   **External Planner Integration:** Transforming task descriptions into formal planning languages (e.g., PDDL) for external, specialized planners, which then rapidly identify optimal action sequences, with LLMs converting results back to natural language.",
    "Result": "Empowers agents with human-like planning capabilities, enabling them to behave more reasonably, powerfully, and reliably, especially for tasks that benefit from strategic decomposition and exploration of alternatives.",
    "Related Patterns": [
      "Agent Profiling",
      "Memory-Augmented Agent",
      "Planning with Feedback",
      "Tool-Augmented LLM Agent",
      "Grounded Replanning"
    ],
    "Uses": "Chain of Thought (CoT), ZeroshotCoT, RePrompting, ReWOO, HuggingGPT, Self-consistent CoT (CoTSC), Tree of Thoughts (ToT), RecMind, GoT, AoT, RAP, LLMP, LLMDP, COLLM, DEPS, GITM, Voyager."
  },
  {
    "Pattern Name": "Planning with Feedback",
    "Problem": "Generating a flawless plan directly from the beginning is extremely difficult for complex, long-horizon tasks, as it requires considering various complex preconditions and is susceptible to unpredictable environmental dynamics, leading to initial plans becoming non-executable.",
    "Context": "LLM-based autonomous agents operating in dynamic, real-world, or complex simulated environments where plans need to be iteratively made and revised based on real-time information.",
    "Solution": "Design planning modules where the agent can receive and incorporate feedback after taking actions to iteratively revise its plans. Feedback can originate from:\n*   **Environmental Feedback:** Objective signals from the world or virtual environment, such as task completion signals, observations after an action, execution errors, or scene graph updates.\n*   **Human Feedback:** Subjective signals from human users to align agent plans with human values, preferences, and to mitigate hallucination problems.\n*   **Model Feedback:** Internal feedback generated by the agents themselves or other pretrained models (e.g., self-refine mechanisms, evaluation modules, detailed verbal critiques) on the agent's output or reasoning steps.",
    "Result": "Enables agents to create more adaptive, robust, and effective plans, recover from unexpected situations, and address complex tasks involving long-range reasoning by simulating human-like iterative planning.",
    "Related Patterns": [
      "LLM as a Planner",
      "Memory-Augmented Agent",
      "Grounded Replanning",
      "Trial-and-Error Learning Agent",
      "Self-Driven Evolution"
    ],
    "Uses": "ReAct, Voyager, Ghost, SayPlan, DEPS, LLMPlanner, Inner Monologue, Self-refine, SelfCheck, InterAct, ChatCoT, Reflexion."
  },
  {
    "Pattern Name": "Grounded Replanning",
    "Problem": "LLM-generated plans may become invalid or non-executable during execution in real-world or embodied environments due to discrepancies between the planned state and the actual environment (e.g., object mismatches, unattainable plans, unexpected dynamics).",
    "Context": "Embodied agents or agents interacting with physical/simulated environments where plans require dynamic adjustment based on real-time observations and environmental changes.",
    "Solution": "Implement an algorithm that dynamically updates or revises existing LLM-generated plans when environmental discrepancies or execution failures are encountered. This involves perceiving the current environment state, identifying mismatches with the plan, and re-planning from the updated, 'grounded' state.",
    "Result": "Improves the robustness, adaptability, and success rate of agents in dynamic and uncertain environments, allowing them to recover from unexpected situations and complete long-horizon tasks.",
    "Related Patterns": [
      "LLM as a Planner",
      "Planning with Feedback",
      "Trial-and-Error Learning Agent"
    ],
    "Uses": "LLMPlanner, Inner Monologue (by adjusting strategies based on scene descriptions and human feedback)."
  },
  {
    "Pattern Name": "Tool-Augmented LLM Agent",
    "Problem": "Large Language Models (LLMs) may lack comprehensive expert knowledge for specific domains, encounter hallucination problems, or be unable to directly interact with external systems or perform complex computations required for tasks.",
    "Context": "LLM-based autonomous agents needing to expand their action space and capabilities beyond the internal knowledge of the LLM to address domain-specific challenges, access real-time information, or perform precise operations.",
    "Solution": "Empower agents with the capability to call and integrate external tools into their action module. These tools can include:\n*   **APIs:** Leveraging external APIs (e.g., HuggingFace models, Python interpreters, LaTeX compilers, RESTful APIs) to execute sophisticated computations, extract relevant content, or generate specific inputs for other systems.\n*   **Databases/Knowledge Bases:** Integrating external databases or knowledge bases (e.g., using SQL statements) to obtain specific domain information for generating more realistic and accurate actions.\n*   **External Models:** Utilizing other specialized pretrained models (e.g., language models for text retrieval, Codex for code generation, multimodal models for video/image/audio processing) to handle more complex or multimodal tasks.",
    "Result": "Greatly expands the agent's operational scope, mitigates LLM limitations (like hallucination and lack of domain expertise), and enables interaction with diverse real-world systems and data sources.",
    "Related Patterns": [
      "LLM as a Planner",
      "Memory-Augmented Agent",
      "Planning with Feedback"
    ],
    "Uses": "HuggingGPT, TPTU, Gorilla, ToolFormer, APIBank, ToolBench, RestGPT, TaskMatrixAI, MRKL, OpenAGI, MemoryBank, ViperGPT, ChemCrow, MMREACT, ChatDB, DBGPT, DEPS, Voyager, GITM, ChatDev, MetaGPT, Self-collaboration."
  },
  {
    "Pattern Name": "Multi-Agent Debate",
    "Problem": "Individual LLM-based agents may produce inconsistent, incomplete, or suboptimal responses to complex questions or tasks, and a single agent's perspective might be limited.",
    "Context": "Scenarios where multiple LLM-based agents can collaborate to solve a problem or refine a decision, leveraging collective intelligence.",
    "Solution": "Design a mechanism where different agents provide separate responses or solutions to a given question or subtask. If their responses are not consistent, they are prompted to incorporate the solutions or opinions from other agents and provide an updated, refined response. This iterative process continues until a final consensus answer is reached or the quality of the solution is significantly improved.",
    "Result": "Enhances the capability and robustness of each agent by leveraging the 'wisdom of crowds,' leading to more consistent, comprehensive, and accurate outcomes that benefit from diverse perspectives and iterative refinement.",
    "Related Patterns": [
      "Agent Profiling",
      "Self-Driven Evolution",
      "Planning with Feedback"
    ],
    "Uses": "91 (debating mechanism), RoCo (multirobot collaboration where agents discuss and improve plans), ChatEval (LLMs critique and assess model results in a debate format), ChatDev (agents communicate to collectively accomplish tasks), MetaGPT (roles supervise code generation)."
  },
  {
    "Pattern Name": "Self-Driven Evolution",
    "Problem": "Enabling LLM-based agents to autonomously learn, adapt, and improve their capabilities over time in open-ended or long-term interactive environments, without explicit, pre-defined curricula or continuous human intervention.",
    "Context": "Autonomous agents operating in dynamic environments where continuous learning, goal-setting, and adaptation are crucial for long-term effectiveness and acquiring new knowledge/skills.",
    "Solution": "Design agents with mechanisms that allow them to:\n1.  **Autonomously Set Goals:** Define their own objectives based on their current state and environment.\n2.  **Explore and Act:** Interact with the environment to gather information and perform actions.\n3.  **Receive Feedback:** Obtain feedback (e.g., from a reward function, environmental observations, or other agents) on the outcomes of their actions.\n4.  **Gradually Improve:** Leverage this feedback and accumulated experience to refine their strategies, acquire new knowledge, develop new skills, or adapt their roles and relationships. This process can involve internal reflection and communication within multi-agent systems.",
    "Result": "Agents acquire knowledge and develop capabilities according to their own preferences or environmental demands, leading to greater autonomy, adaptability, and generalized performance in complex scenarios.",
    "Related Patterns": [
      "Memory-Augmented Agent",
      "Planning with Feedback",
      "Trial-and-Error Learning Agent",
      "Multi-Agent Debate"
    ],
    "Uses": "LMA3 (autonomously set goals, explore, receive reward feedback), SALLMMS (multi-agent system adapts and performs complex tasks via communication), CLMTWA (teacher LLM improves student LLM's reasoning), NLSOM (agents communicate and collaborate, dynamic role adjustment), Voyager (skill execution code refinement), AppAgent (constructs knowledge base via exploration and observation)."
  },
  {
    "Pattern Name": "Trial-and-Error Learning Agent",
    "Problem": "Agents need to learn from failures and adapt their plans or actions in dynamic environments where a perfect initial plan is often unattainable and unexpected situations arise.",
    "Context": "LLM-based agents performing actions in an environment where the outcomes can be objectively or subjectively judged as satisfactory or unsatisfactory.",
    "Solution": "Implement a cycle where the agent first performs an action or proposes a plan. A predefined critic (which can be the environment, a human, or another model/LLM) then evaluates the outcome. If the action or plan is deemed unsatisfactory (e.g., fails, differs from desired outcome), the critic generates specific feedback (e.g., failure information, detailed reasons for failure). This feedback is then incorporated by the agent to react, redesign its plan, or iteratively refine its subsequent actions or strategies.",
    "Result": "Enables agents to acquire adaptive behaviors, correct planning errors, and improve performance over time by learning directly from the consequences of their interactions and the detailed feedback received.",
    "Related Patterns": [
      "Planning with Feedback",
      "Memory-Augmented Agent",
      "Self-Driven Evolution",
      "Grounded Replanning"
    ],
    "Uses": "RAH (user assistant compares predicted response with human feedback), DEPS (explainer generates details for plan failure), RoCo (plan and waypoints validated by environment checks, feedback appended to prompt for discussion), PREFER (LLMs generate feedback on reasons for failure to refine actions)."
  }
]