[
  {
    "Pattern Name": "InContext Retrieval-Augmented Language Model (InContext RALM)",
    "Problem": "Pretrained Language Models (LMs) inherently lack access to up-to-date external knowledge, often produce factual inaccuracies, and cannot provide source attribution. Existing Retrieval-Augmented Language Modeling (RALM) approaches typically require modifying the LM architecture or extensive retraining, which complicates deployment, especially when using off-the-shelf LMs or LMs accessed via API (black-box models).",
    "Context": "Building or deploying AI systems that rely on large language models for text generation, where factual accuracy, external knowledge grounding, and source attribution are critical, but modification or retraining of the core LM is impractical, undesirable, or impossible. This is common in scenarios using black-box LMs or requiring rapid deployment with existing models.",
    "Solution": "Integrate external knowledge by dynamically retrieving relevant documents from a grounding corpus and prepending them directly to the Language Model's input sequence as a context prefix. This leverages the LM's inherent 'in-context learning' capabilities without altering its architecture or requiring further training. The document selection can initially use off-the-shelf general-purpose retrievers (e.g., BM25). Key parameters include 'retrieval stride' (how often retrieval occurs) and 'retrieval query length' (how much of the prefix is used for the query).",
    "Result": "Substantially improves LM performance (e.g., perplexity), mitigates factual inaccuracies, enables natural source attribution, and significantly simplifies the deployment of retrieval-augmented systems by allowing the use of frozen, off-the-shelf LMs, even those accessible only via API.",
    "Related Patterns": "LM-Oriented Reranking, Nearest Neighbor Language Model (kNNLM)",
    "Uses": "Language modeling, Open-Domain Question Answering (ODQA), improving factuality of generated text, scenarios with black-box LM access."
  },
  {
    "Pattern Name": "LM-Oriented Reranking",
    "Problem": "Initial document retrieval (e.g., using lexical or general-purpose dense retrievers) for Retrieval-Augmented Language Models (RALMs) may not select the most semantically relevant document for the specific LM task, or may not adequately prioritize parts of the query (e.g., recency of tokens). This leaves significant potential for improving the quality of the grounding documents presented to the LM.",
    "Context": "An InContext RALM system where an initial set of candidate documents (e.g., top-k from a general-purpose retriever like BM25) has been identified, but a more refined selection is needed to maximize the LM's performance. The goal is to choose the single best document to prepend to the LM's input.",
    "Solution": "Instead of directly using the top-ranked document from the initial retrieval, a secondary reranking mechanism is employed to select the most relevant document from a pool of top-k candidates. This reranking can be achieved in two primary ways: 1) **Zero-Shot Reranking:** Use an off-the-shelf Language Model (potentially a smaller, faster one) to score each candidate document by evaluating how well it helps predict a subsequent segment of the input prefix or the target generation. The document yielding the highest predictive probability is chosen. 2) **Predictive Reranking (Trained):** Train a specialized reranker (e.g., a fine-tuned bidirectional encoder like RoBERTa) to classify or score candidate documents based on their likelihood of improving the LM's prediction of upcoming text. This reranker is trained using the LM's own signal (e.g., p(next_tokens | document, prefix)) as supervision on domain-specific data.",
    "Result": "Further significant improvements in LM performance (e.g., perplexity) by providing more semantically aligned and contextually relevant grounding documents, especially for black-box LMs where direct LM modification is not possible.",
    "Related Patterns": "InContext Retrieval-Augmented Language Model (InContext RALM)",
    "Uses": "Enhancing InContext RALM, improving document selection for factual grounding, optimizing contextualization for LLMs, improving LM performance in knowledge-intensive tasks."
  },
  {
    "Pattern Name": "Nearest Neighbor Language Model (kNNLM)",
    "Problem": "Language Models (LMs) can struggle with generalization and incorporating specific, fine-grained knowledge, often leading to less accurate next-token predictions, especially for rare or out-of-distribution tokens. Scaling this approach to large corpora is an open challenge due to the expense of storing token representations.",
    "Context": "Enhancing the predictive capabilities of a language model during inference, particularly for next-token prediction, by leveraging a dynamic, external knowledge store. This approach focuses on interpolating distributions rather than prepending documents.",
    "Solution": "Augment the LM's next-token probability distribution by interpolating it with a distribution derived from its k-nearest neighbors found in an external retrieval corpus. The 'neighbors' are typically identified by finding tokens in the corpus whose LM embeddings are closest to the query token's embedding. This is an inference-time model modification.",
    "Result": "Improves language modeling performance and generalization by incorporating specific knowledge from the retrieval corpus.",
    "Related Patterns": "InContext Retrieval-Augmented Language Model (InContext RALM)",
    "Uses": "Language modeling, improving next-token prediction, incorporating dynamic knowledge at inference time."
  }
]