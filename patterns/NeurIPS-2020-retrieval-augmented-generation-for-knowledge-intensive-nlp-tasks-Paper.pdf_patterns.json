[
  {
    "Pattern Name": "Retrieval-Augmented Generation (RAG)",
    "Problem": "Large pretrained language models (PLMs) exhibit limitations in accessing and precisely manipulating factual knowledge, leading to suboptimal performance on knowledge-intensive tasks, difficulty in expanding or revising their internal knowledge, lack of provenance for predictions, and a tendency to hallucinate.",
    "Context": "Knowledge-intensive Natural Language Processing (NLP) tasks (e.g., Open-domain Question Answering, Fact Verification, abstractive text generation) where AI models need to generate factual, specific, and diverse language, or require access to up-to-date and verifiable external knowledge. This pattern is applicable when using pretrained parametric memory models (like seq2seq transformers) that need to be augmented with external, dynamically revisable, and inspectable knowledge.",
    "Solution": "Combine a pretrained parametric memory (a seq2seq model, e.g., BART) with an explicit, non-parametric memory (a dense vector index of text documents, e.g., Wikipedia). A pretrained neural retriever (e.g., DPR) is used to access this non-parametric memory by retrieving top-K relevant documents based on the input query. The retrieved documents are then provided as additional context to the seq2seq generator. The entire system (retriever and generator) is fine-tuned end-to-end, treating the retrieved documents as latent variables and marginalizing over their probabilities during training and inference.",
    "Result": "Achieves state-of-the-art performance on a wide range of knowledge-intensive NLP tasks. Generates more specific, diverse, and factual language, and reduces hallucinations compared to parametric-only baselines. Enables easy expansion and revision of the model's knowledge and provides a form of interpretability by allowing inspection of the accessed knowledge. Facilitates unconstrained generation, outperforming extractive approaches even for extractive tasks.",
    "Related Patterns": [
      "RAGSequence",
      "RAGToken",
      "Index Hotswapping"
    ],
    "Uses": [
      "Open-domain Question Answering (NQ, TriviaQA, WebQuestions, CuratedTrec)",
      "Abstractive Question Answering (MSMARCO NLG)",
      "Jeopardy Question Generation",
      "Fact Verification (FEVER)"
    ],
    "Category": [
      "Generative AI",
      "Knowledge & Reasoning",
      "LLM-specific"
    ]
  },
  {
    "Pattern Name": "RAGSequence",
    "Problem": "Ensuring consistency and coherence in generated sequences when leveraging retrieved knowledge, particularly when the entire output is expected to be derived from a single, dominant source or consistent context.",
    "Context": "Retrieval-Augmented Generation (RAG) tasks where the generated output sequence is best supported by a single, consistent retrieved document. The model needs to make a global decision about which document to use for the entire generation process.",
    "Solution": "Within the Retrieval-Augmented Generation (RAG) framework, the model treats the retrieved document as a single latent variable responsible for generating the complete output sequence. It marginalizes over the top-K retrieved documents, calculating the probability of the entire sequence given each document, and then summing these probabilities weighted by the document retrieval probability.",
    "Result": "Produces coherent and consistent output sequences based on a single, globally chosen document. Effective for tasks where a single document provides sufficient context for the whole output.",
    "Related Patterns": [
      "Retrieval-Augmented Generation (RAG)",
      "RAGToken"
    ],
    "Uses": [
      "Open-domain Question Answering",
      "Abstractive Question Answering",
      "Fact Verification"
    ],
    "Category": [
      "Generative AI",
      "LLM-specific"
    ]
  },
  {
    "Pattern Name": "RAGToken",
    "Problem": "Generating output sequences that require combining information from multiple distinct retrieved documents or dynamically switching context across different parts of the generated text to provide a comprehensive and diverse answer.",
    "Context": "Retrieval-Augmented Generation (RAG) tasks where different tokens or segments of the output sequence might be best supported by different pieces of retrieved knowledge. This is particularly useful for complex generations that synthesize information from various sources.",
    "Solution": "Within the Retrieval-Augmented Generation (RAG) framework, the model allows for drawing a potentially different latent document for each target token to be generated. It marginalizes over the top-K retrieved documents at each generation step, calculating the probability of the next token given each document, and then summing these probabilities. This enables the generator to dynamically choose content from several documents as it produces the answer token by token.",
    "Result": "Generates responses that can combine content from multiple documents, leading to more diverse and factually rich outputs, especially for tasks requiring synthesis of information. Improves performance on tasks that benefit from dynamic context switching.",
    "Related Patterns": [
      "Retrieval-Augmented Generation (RAG)",
      "RAGSequence"
    ],
    "Uses": [
      "Jeopardy Question Generation (where questions often combine two separate pieces of information)",
      "Open-domain Question Answering",
      "Abstractive Question Answering",
      "Fact Verification"
    ],
    "Category": [
      "Generative AI",
      "LLM-specific"
    ]
  },
  {
    "Pattern Name": "Index Hotswapping (Dynamic Knowledge Update)",
    "Problem": "Parametric-only language models suffer from 'stale knowledge' and cannot easily update their world knowledge as new information emerges without computationally expensive and time-consuming full model retraining. This limits their applicability in dynamic, knowledge-intensive environments.",
    "Context": "AI systems, particularly those using large language models, that require up-to-date external knowledge and need to adapt to changing factual landscapes. This pattern is applicable when the core model architecture is designed with a separable, non-parametric memory component.",
    "Solution": "Employ a Retrieval-Augmented Generation (RAG) architecture where factual knowledge is stored in an external, non-parametric memory (a document index). To update the model's world knowledge, simply replace or update this external document index at test time. The parametric components of the model (e.g., the generator and query encoder) remain fixed and do not require retraining.",
    "Result": "Enables dynamic and efficient updating of the model's world knowledge. The model can accurately respond to queries based on the most current information available in the swapped index. Dramatically reduces the need for continuous, costly retraining of large language models to stay current with real-world changes.",
    "Related Patterns": [
      "Retrieval-Augmented Generation (RAG)"
    ],
    "Uses": [
      "Any knowledge-intensive NLP task requiring current information, such as factual question answering, news summarization, or policy compliance checks where external knowledge evolves rapidly."
    ],
    "Category": [
      "MLOps",
      "Knowledge & Reasoning"
    ]
  }
]