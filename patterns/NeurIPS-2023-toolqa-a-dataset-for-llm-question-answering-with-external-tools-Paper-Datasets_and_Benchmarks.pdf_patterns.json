[
  {
    "Pattern Name": "Tool Augmentation",
    "Problem": "Large Language Models (LLMs) suffer from challenges such as hallucination, weak numerical reasoning, and a lack of up-to-date or domain-specific knowledge, as their internal knowledge is limited by their pre-training data.",
    "Context": "LLMs are deployed for tasks requiring factual accuracy, precise numerical computation, or interaction with dynamic, external, or specialized knowledge sources.",
    "Solution": "Enhance LLMs' capabilities by integrating them with external tools. These tools can include retrieval systems for external knowledge, math tools (e.g., calculators, WolframAlpha), code interpreters (e.g., Python, SQL), and database operation tools. The LLM then leverages these tools to obtain information or perform computations that it cannot reliably do internally.",
    "Result": "Mitigates hallucinations by providing verified external facts, improves numerical reasoning, and grants access to current, domain-specific, and dynamic information, expanding the LLM's problem-solving scope.",
    "Related Patterns": [
      "Retrieval Augmentation",
      "LLM as a Planner",
      "Self-Correction / Feedback Loop",
      "LLM as an Orchestrator"
    ],
    "Uses": "Question Answering, Fact Checking, Mathematical Reasoning, Code Generation, Robotics, Information Retrieval."
  },
  {
    "Pattern Name": "Faithful Tool-Use Evaluation Dataset Generation",
    "Problem": "Existing evaluation methodologies for tool-augmented LLMs are often biased. LLMs might answer questions by recalling memorized pre-training information rather than genuinely utilizing external tools, making it difficult to accurately assess their true tool-use reasoning abilities.",
    "Context": "Developing and evaluating LLMs designed to interact with external tools for knowledge acquisition, computation, or complex problem-solving, where an unbiased and precise assessment of tool-use is crucial.",
    "Solution": "Curate a benchmark dataset (like ToolQA) through a multi-phase, automated process: 1) **Reference Data Collection**: Gather diverse public corpora (text, tables, graphs) from various domains, ensuring they have minimal or no overlap with LLM pre-training data and contain context-sensitive facts. 2) **Human-Guided Question Generation**: Use LLMs to generate questions based on pre-defined, human-validated templates. These templates are designed such that the generated questions can *only* be answered by using available tools over the collected reference corpora, preventing LLM internal knowledge recall. 3) **Programmatic Answer Generation**: Implement operators corresponding to the defined tools and construct tool chains. These tool chains are then programmatically executed over the reference data to automatically generate accurate ground-truth answers for the generated questions.",
    "Result": "A high-quality, scalable dataset that enables faithful and precise evaluation of LLMs' ability to leverage external tools for problem-solving, minimizing the influence of pre-trained knowledge and providing verifiable answers.",
    "Related Patterns": [],
    "Uses": "Benchmarking tool-augmented LLMs, developing new methods for LLM tool integration, ensuring unbiased evaluation in LLM research and development."
  },
  {
    "Pattern Name": "LLM as a Planner / Decomposed Planning",
    "Problem": "LLMs struggle with complex, long-horizon tasks that require a sequence of intermediate steps, logical decomposition, or strategic action generation.",
    "Context": "Embodied agents, multi-step question answering, or automated systems where LLMs need to interact with an environment or multiple tools to achieve a goal that cannot be solved directly.",
    "Solution": "Decompose complex tasks by leveraging the LLM's reasoning capabilities to generate a high-level plan or a sequence of intermediate reasoning steps. The LLM acts as a 'planner' that breaks down the main task into smaller, manageable subtasks, each potentially executable by a specific tool or a simpler LLM inference. This can involve techniques like Chain-of-Thought prompting or more advanced planning frameworks (e.g., Tree of Thoughts).",
    "Result": "Improves the LLM's ability to tackle complex, multi-step tasks by providing a structured approach to problem-solving, making the process more efficient and coherent.",
    "Related Patterns": [
      "Tool Augmentation",
      "Self-Correction / Feedback Loop",
      "LLM as an Orchestrator",
      "Chain-of-Thought Prompting"
    ],
    "Uses": "Agentic AI, Robotics, Complex Question Answering, Automated Workflow Generation, Strategic Game Playing."
  },
  {
    "Pattern Name": "Self-Correction / Feedback Loop",
    "Problem": "LLMs, especially in multi-step or tool-augmented tasks, can make errors in their reasoning, generate infeasible actions, or misinterpret observations, leading to incorrect or suboptimal outcomes.",
    "Context": "LLMs operating as agents, interacting with external tools or dynamic environments where real-time feedback on actions is available and crucial for refining performance.",
    "Solution": "Design an iterative process where the LLM generates an action or a reasoning step, receives feedback or an observation from the environment (e.g., tool execution trace, error message, state change), and then uses this observation to reflect on its previous decision, identify errors, and generate a refined or corrected subsequent action or reasoning step. This pattern is exemplified by methods like ReAct (Reasoning and Acting) and Self-Refine.",
    "Result": "Increases the robustness, accuracy, and adaptability of LLM-driven agents by enabling them to learn from and recover from errors, iteratively improving their problem-solving capabilities over time.",
    "Related Patterns": [
      "Tool Augmentation",
      "LLM as a Planner",
      "LLM as an Orchestrator"
    ],
    "Uses": "Agentic AI, Interactive Problem Solving, Debugging LLM outputs, Complex Multi-step Tasks, Robotics."
  },
  {
    "Pattern Name": "LLM as an Orchestrator",
    "Problem": "Combining multiple, diverse external tools to solve complex problems requires intelligent coordination, selection, and management of information flow among them. Manually defining tool chains is often impractical or insufficient.",
    "Context": "LLMs needing to leverage a dynamic pool of specialized tools (e.g., text retrieval, database operations, mathematical calculators, code interpreters, graph tools) where the optimal sequence and interaction of tools are not fixed but depend on the specific task.",
    "Solution": "Employ the LLM as a central controller or orchestrator. The LLM is responsible for understanding the user's query, selecting the most appropriate tools from a given pool based on their descriptions, determining the optimal order of execution, dynamically generating arguments for tool calls, and managing the input/output flow between different tools and the LLM itself. This pattern is exemplified by methods like Chameleon.",
    "Result": "Enables LLMs to synergize various functional tools effectively, expanding their problem-solving capabilities by composing complex workflows that adapt to task requirements, rather than being limited to single-tool usage.",
    "Related Patterns": [
      "Tool Augmentation",
      "LLM as a Planner",
      "Self-Correction / Feedback Loop"
    ],
    "Uses": "Multi-tool agent systems, complex scientific computing, data analysis workflows, domain-specific problem-solving, automated task execution."
  },
  {
    "Pattern Name": "Retrieval Augmentation",
    "Problem": "LLMs are prone to hallucination and may lack up-to-date, specific, or proprietary factual knowledge required for many tasks, as their internal knowledge is static and limited by their pre-training data.",
    "Context": "Information-seeking tasks, open-domain question answering, or fact-checking where factual accuracy, currency, and grounding in verifiable external information are critical.",
    "Solution": "Augment the LLM's generation process with a retrieval mechanism. Before or during generation, a retrieval system queries an external knowledge base (e.g., text corpora, vector databases, knowledge graphs) to fetch relevant documents or facts. This retrieved information is then provided to the LLM as additional context alongside the original prompt, guiding its response generation.",
    "Result": "Mitigates hallucinations, provides LLMs with access to external, up-to-date, and verifiable knowledge, and grounds their responses in factual evidence, leading to more accurate and reliable outputs.",
    "Related Patterns": [
      "Tool Augmentation",
      "Memory Augmentation"
    ],
    "Uses": "Open-domain Question Answering, Fact Checking, Information Extraction, Timely Information Queries, Personalized Recommendations."
  },
  {
    "Pattern Name": "Chain-of-Thought Prompting",
    "Problem": "LLMs often struggle with complex reasoning tasks, providing direct answers that may be incorrect or lack transparency without revealing their intermediate thought processes.",
    "Context": "LLMs solving reasoning-intensive problems such as mathematical word problems, multi-step logic puzzles, or tasks requiring sequential deduction, where the solution involves more than a single inference step.",
    "Solution": "Instruct the LLM, through specific prompting techniques (e.g., adding phrases like 'Let's think step by step', providing few-shot examples that include intermediate thoughts), to generate a series of explicit reasoning steps or 'thoughts' before arriving at the final answer. This encourages the LLM to break down the problem and articulate its internal reasoning process.",
    "Result": "Significantly improves the LLM's reasoning capabilities and accuracy on complex tasks, makes its decision-making process more transparent, and can serve as a scaffold for more advanced planning or self-correction mechanisms.",
    "Related Patterns": [
      "LLM as a Planner"
    ],
    "Uses": "Complex Question Answering, Mathematical Reasoning, Logical Deduction, Problem Solving, Code Generation with reasoning."
  },
  {
    "Pattern Name": "Memory Augmentation",
    "Problem": "LLMs have inherent limitations in their context window size, preventing them from retaining and effectively leveraging information from long documents, past interactions, or extensive knowledge bases for subsequent reasoning or task execution.",
    "Context": "Long-running conversations, agentic systems requiring knowledge of past experiences or learned behaviors, or tasks involving information synthesis from very large corpora that exceed a single prompt's capacity.",
    "Solution": "Integrate external memory systems with LLMs. This external memory (e.g., vector databases, knowledge graphs, specialized episodic memory modules) stores and retrieves relevant past information, extending the LLM's effective context beyond its immediate input window. The LLM can write to or read from this memory, allowing it to maintain state and access a broader knowledge base over time.",
    "Result": "Enables LLMs to maintain coherence over extended interactions, learn and adapt based on past experiences, and access a vast amount of contextual information, improving performance on long-horizon and stateful tasks.",
    "Related Patterns": [
      "Tool Augmentation",
      "Retrieval Augmentation",
      "Self-Correction / Feedback Loop"
    ],
    "Uses": "Conversational AI, Agentic AI, Personalized Assistants, Long-Document Understanding, Continual Learning."
  }
]