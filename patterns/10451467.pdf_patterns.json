[
  {
    "Pattern Name": "ReAct (Reasoning and Acting)",
    "Problem": "Large Language Models (LLMs) struggle with complex, long-horizon tasks that require both dynamic reasoning (e.g., planning, tracking progress, handling exceptions) and grounded interaction with external environments (e.g., gathering information, updating facts). Reasoning-only methods (like Chain-of-Thought) can suffer from hallucination and error propagation due to lack of external grounding, while action-only methods lack abstract reasoning and high-level goal maintenance, leading to issues like repeating actions or failing to comprehend context.",
    "Context": "An agent, powered by a Large Language Model (LLM), interacts with an environment for task solving. The LLM is prompted with few-shot in-context examples to guide its behavior. Environments can include knowledge bases (like Wikipedia API), text-based games (ALFWorld), or web navigation interfaces (WebShop).",
    "Solution": "The agent's action space is augmented to include both 'actions' (domain-specific interactions with the external environment) and 'thoughts' (free-form language reasoning traces). These thoughts and actions are generated in an interleaved manner. Thoughts compose useful information by reasoning over the current context to support future reasoning or acting (reason to act), while actions interface with external sources to incorporate additional information into reasoning (act to reason).",
    "Result": "Synergizes reasoning and acting, leading to superior performance, improved human interpretability, trustworthiness, and diagnosability across diverse tasks. It effectively overcomes hallucination and error propagation issues common in reasoning-only methods and enhances abstract reasoning for action-only methods. The approach is generalizable, robust, and benefits from finetuning.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting",
      "Act-Only",
      "Inner Monologue",
      "LLM as a Planner (SayCan)",
      "Combining Internal and External Knowledge"
    ],
    "Uses": [
      "Multihop Question Answering (HotpotQA)",
      "Fact Verification (FEVER)",
      "Text-based Games (ALFWorld)",
      "Web Navigation (WebShop)"
    ]
  },
  {
    "Pattern Name": "Chain-of-Thought (CoT) Prompting",
    "Problem": "Large Language Models (LLMs) often struggle with complex reasoning tasks (e.g., arithmetic, commonsense, symbolic reasoning) that require multiple steps of inference, often producing direct, unreasoned answers that can be incorrect.",
    "Context": "Large Language Models (LLMs) are used to solve problems requiring multi-step reasoning.",
    "Solution": "The LLM is prompted to generate intermediate reasoning steps, a 'chain of thought,' before producing the final answer. This explicit step-by-step reasoning process is included in the prompt as part of the in-context examples.",
    "Result": "Elicits emergent reasoning capabilities in LLMs, improving their performance on complex tasks. However, it is a static, internal process not grounded in external information, which can lead to issues like fact hallucination and error propagation.",
    "Related Patterns": [
      "Self-Consistency (CoTSC)",
      "ReAct (Reasoning and Acting)",
      "Least-to-Most Prompting",
      "Selection-Inference",
      "Faithful Reasoning"
    ],
    "Uses": [
      "Arithmetic reasoning",
      "Commonsense reasoning",
      "Symbolic reasoning tasks",
      "Question Answering",
      "Fact Verification"
    ]
  },
  {
    "Pattern Name": "Self-Consistency (CoTSC)",
    "Problem": "Chain-of-Thought (CoT) reasoning, while effective, can be brittle, and a single reasoning path might lead to an incorrect answer. The model might generate different, equally plausible reasoning paths, but only one is correct.",
    "Context": "Applying Chain-of-Thought (CoT) prompting with Large Language Models (LLMs) for complex reasoning tasks.",
    "Solution": "Instead of relying on a single CoT trajectory, multiple diverse reasoning paths are sampled from the LLM (e.g., by using a non-zero decoding temperature during inference). The final answer is then determined by aggregating the results, typically by taking the majority vote among the answers derived from these different reasoning paths.",
    "Result": "Consistently boosts performance over vanilla CoT by leveraging the principle that a correct answer is more likely to be consistently derived through multiple distinct reasoning paths. Improves the robustness of CoT reasoning.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting",
      "Combining Internal and External Knowledge"
    ],
    "Uses": [
      "Enhancing Chain-of-Thought reasoning",
      "Question Answering",
      "Fact Verification"
    ]
  },
  {
    "Pattern Name": "Combining Internal and External Knowledge",
    "Problem": "Large Language Models (LLMs) possess strong internal knowledge and reasoning capabilities (e.g., via Chain-of-Thought), but this can lead to hallucinated facts or thoughts. Conversely, external knowledge retrieval (e.g., via ReAct) provides factual grounding but can be inflexible or suffer from non-informative search results, derailing reasoning.",
    "Context": "Tasks that require both robust internal reasoning and accurate, up-to-date factual grounding from external sources.",
    "Solution": "Strategically combine methods that leverage LLM internal knowledge (like Self-Consistency with Chain-of-Thought, CoTSC) with methods that integrate external knowledge and action (like ReAct). Heuristics are used to decide when to switch between methods. For example, if ReAct fails to produce an answer within a set number of steps, back off to CoTSC. Alternatively, if CoTSC's majority answer is not confident (e.g., occurs less than n/2 times), back off to ReAct.",
    "Result": "Achieves superior performance by leveraging the complementary strengths of both internal and external reasoning. This approach leads to more factual and robust problem-solving, mitigating the individual limitations of each method.",
    "Related Patterns": [
      "ReAct (Reasoning and Acting)",
      "Self-Consistency (CoTSC)",
      "Chain-of-Thought (CoT) Prompting"
    ],
    "Uses": [
      "Knowledge-intensive Question Answering (HotpotQA)",
      "Fact Verification (FEVER)"
    ]
  },
  {
    "Pattern Name": "LLM as a Planner (SayCan)",
    "Problem": "Embodied agents need to translate high-level language goals into a sequence of executable actions in a physical environment, which is a complex planning problem.",
    "Context": "Robotic systems and embodied agents operating in physical or simulated environments, receiving high-level language instructions.",
    "Solution": "Large Language Models (LLMs) are used to directly predict a set of possible actions that a robot can take to achieve a given goal. These LLM-generated actions are then reranked by an affordance model, which grounds the linguistic actions in the visual and physical capabilities of the robot and the environment, for final action selection.",
    "Result": "Enables LLMs to effectively guide robotic actions, bridging the gap between high-level language instructions and low-level physical execution, thereby grounding language plans in the real world.",
    "Related Patterns": [
      "Act-Only",
      "Inner Monologue",
      "ReAct (Reasoning and Acting)"
    ],
    "Uses": [
      "Robotic action planning",
      "Embodied AI"
    ]
  },
  {
    "Pattern Name": "Inner Monologue",
    "Problem": "Embodied agents need a mechanism to motivate their actions, track progress, and react to environmental observations to achieve their goals, beyond just predicting raw actions.",
    "Context": "Embodied agents interacting in dynamic environments where actions need to be guided by internal state and external feedback.",
    "Solution": "Actions of an embodied agent are motivated by an 'inner monologue' which is implemented as injected feedback from the environment. This monologue is specifically designed to reiterate observations of the environment state and what needs to be completed by the agent for the goal to be satisfied.",
    "Result": "Enables a closed-loop system for embodied reasoning, providing a form of internal feedback to guide agent behavior. It allows agents to react to the environment and track task completion, though it is noted to be more limited in flexibility and reasoning types compared to 'ReAct's' reasoning traces.",
    "Related Patterns": [
      "ReAct (Reasoning and Acting)",
      "LLM as a Planner (SayCan)"
    ],
    "Uses": [
      "Embodied reasoning and planning"
    ]
  },
  {
    "Pattern Name": "Self-Taught Reasoner (STaR)",
    "Problem": "Improving Large Language Model (LLM) reasoning often requires finetuning on correct rationales, but manually annotating high-quality rationales at scale is expensive and time-consuming.",
    "Context": "Finetuning or training LLMs for tasks that benefit from explicit reasoning traces, such as complex question answering or problem-solving.",
    "Solution": "A bootstrapping approach where the model generates its own rationales. The LLM first generates a rationale and an answer. A verifier (which can be a simple check or another model) then evaluates if the answer is correct. If the answer is correct, the generated rationale is automatically added to a dataset of correct rationales, which is then used to finetune the LLM further.",
    "Result": "Enables LLMs to learn to generate better rationales and improve reasoning performance without extensive manual annotation, by iteratively improving on self-generated high-quality reasoning traces.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting",
      "Scratchpads"
    ],
    "Uses": [
      "Improving LLM reasoning",
      "Generating high-quality rationales for training"
    ]
  },
  {
    "Pattern Name": "Scratchpads",
    "Problem": "Large Language Models (LLMs) can struggle with multi-step computation or reasoning problems where intermediate steps are crucial for correctness but are not explicitly part of the final output, making it difficult for the model to learn and reproduce the full reasoning process.",
    "Context": "Finetuning LLMs for tasks involving multi-step computation or complex reasoning where intermediate calculations or thoughts are important.",
    "Solution": "The LLM is finetuned on data that includes explicit intermediate computation steps, effectively teaching the model to 'show its work' in an internal 'scratchpad' before producing the final answer. This makes the intermediate reasoning process visible and learnable by the model.",
    "Result": "Improves performance on multi-step computation problems by enabling the model to explicitly represent and learn from intermediate steps, leading to more accurate and robust solutions.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting",
      "Self-Taught Reasoner (STaR)"
    ],
    "Uses": [
      "Multi-step computation problems",
      "Complex reasoning tasks"
    ]
  },
  {
    "Pattern Name": "Selection-Inference",
    "Problem": "Complex logical reasoning tasks can be challenging for Large Language Models (LLMs) to handle directly, leading to less interpretable or error-prone reasoning.",
    "Context": "Applying LLMs to tasks requiring interpretable logical reasoning.",
    "Solution": "The overall reasoning process is divided into two distinct steps: a 'selection' step and an 'inference' step. The selection step identifies relevant information or premises, and the inference step then uses this selected information to derive conclusions.",
    "Result": "Provides a more structured and interpretable approach to logical reasoning, allowing LLMs to tackle complex problems by breaking them down into manageable, sequential stages.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting",
      "Faithful Reasoning",
      "Least-to-Most Prompting"
    ],
    "Uses": [
      "Interpretable logical reasoning"
    ]
  },
  {
    "Pattern Name": "Faithful Reasoning",
    "Problem": "Multi-step reasoning tasks can be complex and difficult for a single Large Language Model (LLM) to perform faithfully, potentially leading to errors or ungrounded conclusions.",
    "Context": "Applying LLMs to multi-step reasoning tasks where fidelity and accuracy of each step are critical.",
    "Solution": "The multi-step reasoning process is decomposed into three distinct steps, with each step performed by a dedicated Large Language Model (LM) or a specialized component of a single LM. This modular approach ensures that each stage of reasoning is handled by a component optimized for that specific sub-task.",
    "Result": "Enhances the faithfulness and accuracy of multi-step reasoning by distributing the cognitive load across specialized components, leading to more reliable and verifiable conclusions.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting",
      "Selection-Inference",
      "Least-to-Most Prompting"
    ],
    "Uses": [
      "Multi-step reasoning"
    ]
  },
  {
    "Pattern Name": "Act-Only",
    "Problem": "Agents in interactive environments need to perform domain-specific actions to achieve goals, but without explicit verbal reasoning, they can struggle with abstract goals, maintaining working memory, or recovering from errors (e.g., hallucinating actions).",
    "Context": "Using pretrained language models for planning and acting in interactive environments (e.g., web browsers, text-based games).",
    "Solution": "Multimodal observations from the environment are converted into text, and the language model is then used to generate domain-specific actions or plans. A controller then executes these actions. This approach focuses solely on action prediction without the LLM generating explicit intermediate verbal reasoning traces ('thoughts').",
    "Result": "Enables LLMs to interact with and perform actions in dynamic environments. However, without explicit reasoning, these agents may lack the ability to reason abstractly about high-level goals, track progress, or handle exceptions effectively, often leading to repetitive or ungrounded actions.",
    "Related Patterns": [
      "ReAct (Reasoning and Acting)",
      "LLM as a Planner (SayCan)"
    ],
    "Uses": [
      "Interactive environments",
      "Web browsing for question answering (WebGPT)",
      "Task-oriented dialogue systems"
    ]
  },
  {
    "Pattern Name": "Least-to-Most Prompting",
    "Problem": "Large Language Models (LLMs) can struggle with complex reasoning tasks that inherently require breaking down a problem into smaller, interdependent subproblems, as they might attempt to solve the entire problem in one go.",
    "Context": "Solving complicated reasoning tasks with Large Language Models (LLMs).",
    "Solution": "The LLM is first prompted to decompose a complex problem into a series of simpler, sequential subproblems. Then, each subproblem is solved one by one, with the solution or output of a previous subproblem being explicitly fed as additional context or input to the LLM for solving the next subproblem. This continues until the final answer to the original complex problem is derived.",
    "Result": "Enables LLMs to effectively tackle complex reasoning tasks by mimicking a human-like step-by-step problem-solving approach, where solutions to simpler parts build up to the solution of the whole.",
    "Related Patterns": [
      "Chain-of-Thought (CoT) Prompting",
      "Selection-Inference",
      "Faithful Reasoning"
    ],
    "Uses": [
      "Complex reasoning tasks",
      "Problem decomposition"
    ]
  }
]