[
  {
    "Pattern Name": "Tool-Integrated Reasoning",
    "Problem": "Large language models (LLMs) struggle with complex mathematical problems, precise computation, symbolic manipulation, and algorithmic processing, despite their strong natural language understanding and generation capabilities.",
    "Context": "Designing AI systems for tasks that require both high-level semantic analysis, planning, and abstract reasoning (where natural language excels) and rigorous operations, intricate calculations, and outsourcing to specialized tools (where programs excel).",
    "Solution": "Develop an agent that seamlessly interleaves natural language reasoning with program-based tool utilization. The agent generates natural language rationales for analysis and planning, and when precise computation or symbolic manipulation is needed, it generates and executes a program using external tools (e.g., computation libraries, symbolic solvers). The execution output is then integrated back into the natural language reasoning process.",
    "Result": "Amalgamates the analytical prowess of language and the computational efficiency of tools, significantly outperforming models that rely solely on natural language or program generation. Enhances LLM capabilities for complex quantitative tasks.",
    "Related Patterns": [
      "Program-Aided Language Models (PAL) Prompting",
      "Chain-of-Thought (CoT) Prompting",
      "Multi-Tool Agent",
      "Self-Correction with Tool-Interactive Critiquing"
    ],
    "Uses": "Mathematical problem solving, scientific computation, data analysis, complex reasoning tasks requiring both abstract thought and precise execution."
  },
  {
    "Pattern Name": "Output Space Shaping",
    "Problem": "When training LLMs via imitation learning on limited, curated datasets, the model's output space can become restricted, hindering its flexibility in exploring diverse, plausible reasoning trajectories during inference and leading to improper tool-use behavior.",
    "Context": "Refining the reasoning behavior of LLMs, particularly those trained with imitation learning on interactive tool-use trajectories, to improve diversity, robustness, and reduce errors.",
    "Solution": "1. **Sampling:** Apply a sampling strategy (e.g., nucleus sampling) to an initial imitation-learned model to generate diverse candidate trajectories for a given problem. Retain only the valid trajectories (those with correct answers and no tool-use errors).\n2. **Correction:** For invalid trajectories (e.g., those that are partially correct but fail later), leverage a 'teacher model' to correct the subsequent portions of the trajectory, thereby transforming invalid paths into valid ones.\n3. **Retraining:** Retrain the model on a combined dataset consisting of the initial curated corpus, the newly sampled valid trajectories, and the corrected trajectories.",
    "Result": "Encourages the diversity of plausible reasoning steps, mitigates improper tool-use behavior, and significantly boosts reasoning performance and generalization, especially for smaller models.",
    "Related Patterns": [
      "Knowledge Distillation (KD) for Trajectory Learning",
      "Rejection Sampling Fine-Tuning",
      "Self-Correction"
    ],
    "Uses": "Refining LLM reasoning, expanding output diversity, improving robustness to improper tool use, enhancing generalization, particularly for tool-augmented LLMs."
  },
  {
    "Pattern Name": "Imitation Learning with Tool-Use Trajectories",
    "Problem": "Training LLMs to effectively integrate natural language reasoning with external tool use requires learning complex, interactive, multi-turn behaviors, but existing datasets often lack annotations for such interleaved rationales and tool interactions.",
    "Context": "Developing agentic LLMs that can dynamically use tools, where the desired behavior involves a sequence of natural language thoughts, program generation, tool execution, and response processing.",
    "Solution": "1. **Trajectory Curation:** Utilize a powerful teacher model (e.g., a larger, more capable LLM like GPT-4) to synthesize high-quality, interactive tool-use trajectories on relevant datasets. These trajectories explicitly demonstrate the interleaved format of natural language reasoning, program generation, and tool output processing.\n2. **Supervised Fine-Tuning:** Apply imitation learning (e.g., minimizing negative log-likelihood loss) on this curated corpus of interactive tool-use trajectories to finetune a base LLM.",
    "Result": "Enables the student LLM to learn and reproduce the complex interleaved reasoning and tool-use format, leading to significant performance improvements and the acquisition of agentic capabilities in the target domain.",
    "Related Patterns": [
      "Supervised Fine-Tuning (SFT) with Rationale Data",
      "Knowledge Distillation (KD) for Trajectory Learning",
      "Few-Shot Prompting with Interleaved Format"
    ],
    "Uses": "Training agentic LLMs, acquiring complex interactive behaviors, developing tool-augmented LLMs, transferring advanced reasoning strategies."
  },
  {
    "Pattern Name": "Chain-of-Thought (CoT) Prompting",
    "Problem": "Large language models (LLMs) often struggle with complex reasoning tasks, providing incorrect or superficial answers, especially for multi-step problems, without showing their intermediate thought processes.",
    "Context": "Improving the reasoning ability and transparency of LLMs for tasks that require logical deduction, step-by-step problem-solving, or abstract thinking.",
    "Solution": "Augment the prompt with instructions or few-shot examples that encourage the LLM to generate intermediate, step-by-step natural language rationales before producing the final answer. This makes the LLM 'think aloud' and break down the problem.",
    "Result": "Elicits and enhances the LLM's reasoning capabilities, leading to improved performance on complex tasks, greater interpretability of the model's decision-making process, and better error identification.",
    "Related Patterns": [
      "Program-Aided Language Models (PAL) Prompting",
      "Tool-Integrated Reasoning",
      "Few-Shot Prompting"
    ],
    "Uses": "Mathematical reasoning, common-sense reasoning, logical puzzles, complex question answering, code generation (with natural language planning)."
  },
  {
    "Pattern Name": "Program-Aided Language Models (PAL) Prompting",
    "Problem": "While natural language reasoning (like CoT) improves LLM performance, LLMs still inherently struggle with precise computation, symbolic manipulation, and algorithmic execution, leading to errors in quantitative or rigorous tasks.",
    "Context": "Tasks that demand rigorous operations, intricate calculations, symbolic processing, or the ability to leverage specialized computational tools that LLMs cannot perform reliably on their own.",
    "Solution": "The LLM is prompted to synthesize and execute programs (e.g., Python code) to solve sub-problems or the entire task. The program's execution output (e.g., numerical results, symbolic simplifications) is then used by the LLM to derive or refine the final answer.",
    "Result": "Leverages external computational tools for accuracy and efficiency, overcoming LLM limitations in exact calculations and symbolic processing. It allows LLMs to 'outsource' complex computations to reliable interpreters.",
    "Related Patterns": [
      "Tool-Integrated Reasoning",
      "Chain-of-Thought (CoT) Prompting",
      "Multi-Tool Agent",
      "Tools Integration Patterns"
    ],
    "Uses": "Mathematical problem solving, data analysis, scientific computation, tasks benefiting from external code execution and symbolic manipulation."
  },
  {
    "Pattern Name": "Few-Shot Prompting with Interleaved Format",
    "Problem": "Guiding LLMs to produce outputs that adhere to a specific, complex, multi-modal structure (e.g., alternating between natural language and code/tool calls) can be challenging without explicit demonstrations.",
    "Context": "When training data needs to be curated or when an LLM needs to be guided during inference to perform a task that involves a specific sequence of actions and output formats, such as interactive tool use.",
    "Solution": "Design a detailed prompt that includes clear instructions on the desired interleaved format and provides several diverse, high-quality examples (few-shot examples). These examples showcase the exact sequence of natural language rationales, program generation, and tool output processing that the LLM should follow.",
    "Result": "Effectively guides the LLM to generate outputs that conform to the desired interactive, interleaved format, improving the consistency, structure, and effectiveness of its reasoning and tool-use process. This is crucial for collecting high-quality training data for agentic models.",
    "Related Patterns": [
      "Prompt Engineering",
      "Tool-Integrated Reasoning",
      "Imitation Learning with Tool-Use Trajectories",
      "Chain-of-Thought (CoT) Prompting"
    ],
    "Uses": "Data curation for training tool-augmented LLMs, guiding LLM behavior in interactive agents, ensuring specific output formats in complex reasoning."
  },
  {
    "Pattern Name": "ZeRO (Zero Redundancy Optimizer)",
    "Problem": "Training very large deep learning models (especially LLMs with billions of parameters) is severely constrained by the limited GPU memory, as model parameters, gradients, and optimizer states consume vast amounts of memory.",
    "Context": "Training massive neural networks in a distributed environment where memory efficiency is critical to scale model size and batch size beyond single-GPU limits.",
    "Solution": "A memory optimization technology that partitions model states (optimizer states, gradients, and optionally model parameters themselves) across data-parallel devices. Instead of replicating all states on every GPU, each GPU only stores a portion, drastically reducing memory redundancy. ZeRO Stage 3 partitions all three components.",
    "Result": "Drastically reduces GPU memory consumption, enabling the training of models with hundreds of billions or even trillions of parameters on existing hardware. This improves training efficiency, scalability, and allows for larger batch sizes.",
    "Related Patterns": [
      "Data Parallelism",
      "Model Parallelism",
      "Distributed Training"
    ],
    "Uses": "Training large-scale LLMs and deep learning models with high memory requirements, enabling research and development of frontier AI models."
  },
  {
    "Pattern Name": "Supervised Fine-Tuning (SFT) with Rationale Data",
    "Problem": "General-purpose pre-trained LLMs may not inherently follow specific reasoning styles (like Chain-of-Thought) or perform optimally on particular downstream tasks without explicit task-specific training.",
    "Context": "Adapting a pre-trained LLM to a specific domain or task where a structured reasoning process (e.g., step-by-step rationales) is desired for improved performance and interpretability.",
    "Solution": "Collect or generate a dataset of problem-solution pairs, where the solutions are augmented with explicit, step-by-step natural language rationales. Then, finetune the LLM on this dataset using standard supervised learning techniques (e.g., minimizing negative log-likelihood), training it to generate both the rationale and the final answer.",
    "Result": "Improves the LLM's ability to generate desired reasoning styles, enhances its performance on the target task by guiding its thought process, and makes its decision-making more transparent.",
    "Related Patterns": [
      "Imitation Learning",
      "Chain-of-Thought (CoT) Prompting",
      "Knowledge Distillation"
    ],
    "Uses": "Task-specific adaptation of LLMs, instilling reasoning capabilities, improving instruction following, creating domain-specific reasoning agents."
  },
  {
    "Pattern Name": "Rejection Sampling Fine-Tuning (RFT)",
    "Problem": "LLMs can generate diverse reasoning paths, but many of these paths might be incorrect, suboptimal, or lead to wrong answers. Simple supervised fine-tuning on a fixed dataset might not effectively filter out poor reasoning.",
    "Context": "Improving the quality, reliability, and diversity of reasoning paths generated by LLMs, especially in tasks with multiple valid solution approaches or where correctness can be objectively verified.",
    "Solution": "1. **Generate Diverse Paths:** Prompt the LLM (or multiple LLMs) to generate several diverse reasoning paths or solutions for a given problem.\n2. **Verify and Select:** Implement a verification mechanism (e.g., automatic checking of final answers, rule-based validation, or an external verifier model) to evaluate the correctness or quality of each generated path.\n3. **Fine-Tune:** Only the 'accepted' (correct and high-quality) reasoning paths are used to fine-tune the LLM, effectively 'rejecting' the poor-quality ones.",
    "Result": "Produces models that generate more reliable and diverse reasoning, improving overall performance by focusing training on successful problem-solving strategies and implicitly learning to avoid common errors.",
    "Related Patterns": [
      "Self-Correction",
      "Output Space Shaping",
      "Reward Modeling"
    ],
    "Uses": "Enhancing LLM reasoning capabilities, improving robustness, generating more diverse and correct outputs, reducing hallucinations."
  },
  {
    "Pattern Name": "Reinforcement Learning from Human Feedback (RLHF)",
    "Problem": "Aligning LLMs with complex, subjective human preferences, values, and instructions is difficult using traditional supervised learning, which relies on fixed datasets and cannot capture nuanced human judgments.",
    "Context": "After an initial supervised fine-tuning phase, when LLMs can generate diverse outputs but might still produce undesirable, unhelpful, unsafe, or unaligned responses.",
    "Solution": "1. **Reward Model Training:** Collect human preference data by having annotators rank or score multiple LLM-generated responses for various prompts. Train a separate 'reward model' (RM) to predict these human preferences.\n2. **Reinforcement Learning:** Use the trained reward model to provide a reward signal to the LLM during a reinforcement learning phase (e.g., using Proximal Policy Optimization - PPO). The LLM is optimized to maximize this reward, thereby learning to generate outputs that are highly preferred by humans.",
    "Result": "Significantly improves the LLM's helpfulness, harmlessness, and adherence to complex instructions, making it more aligned with human expectations, safer, and more user-friendly.",
    "Related Patterns": [
      "Supervised Fine-Tuning (SFT)",
      "AI-Human Interaction Patterns",
      "Personalization Patterns",
      "Human-in-the-Loop"
    ],
    "Uses": "Chatbot development, instruction-following models, safety alignment, improving conversational AI, personalization of LLM behavior."
  },
  {
    "Pattern Name": "Multi-Tool Agent",
    "Problem": "Complex, real-world tasks often require diverse capabilities that exceed the scope of any single specialized tool or the inherent abilities of an LLM alone.",
    "Context": "Building LLM-based agents that need to interact with a varied ecosystem of external resources (e.g., calculators, search engines, symbolic solvers, code interpreters, APIs, databases) to achieve a sophisticated goal.",
    "Solution": "Design an agent where an LLM acts as the central orchestrator, capable of:\n1. **Problem Analysis:** Understanding the task and identifying which types of external tools might be relevant.\n2. **Tool Selection:** Dynamically choosing the most appropriate tool(s) for a given sub-problem or step.\n3. **Input Generation:** Formulating correct and effective queries or inputs for the selected tool(s).\n4. **Execution & Parsing:** Executing the tool(s) and parsing their outputs.\n5. **Iterative Refinement:** Using outputs from one tool to inform subsequent reasoning, select other tools, or refine the overall solution in an iterative process.",
    "Result": "Enhances the LLM's ability to tackle a wider range of complex, multi-faceted problems, leverages the strengths of various specialized tools, improves accuracy, efficiency, and expands the agent's overall problem-solving scope.",
    "Related Patterns": [
      "Tool-Integrated Reasoning",
      "Agentic AI Patterns",
      "Tools Integration Patterns",
      "Planning Patterns"
    ],
    "Uses": "Complex problem solving, scientific discovery, data analysis, interactive systems, general-purpose AI assistants, automation of multi-step workflows."
  },
  {
    "Pattern Name": "Knowledge Distillation (KD) for Trajectory Learning",
    "Problem": "Training smaller, more efficient LLMs to achieve complex, interactive behaviors and reasoning performance comparable to larger, more capable teacher models, without requiring the teacher model's computational resources at inference.",
    "Context": "Transferring the knowledge embedded in the detailed reasoning process and tool-use strategies of a high-performing (often larger) LLM to a smaller, more deployment-friendly student model.",
    "Solution": "1. **Teacher Trajectory Generation:** Use a powerful 'teacher' LLM to generate extensive, high-quality, step-by-step reasoning trajectories for a given set of problems. These trajectories capture the teacher's thought process, including interleaved natural language rationales, program calls, and tool interactions.\n2. **Student Fine-Tuning:** Finetune a 'student' LLM on this teacher-generated data. The student model learns to imitate the teacher's behavior by minimizing a loss function (e.g., negative log-likelihood) over the generated sequences, effectively learning the teacher's reasoning and tool-use patterns.",
    "Result": "Enables smaller models to acquire sophisticated reasoning and tool-use capabilities from larger models, leading to significant performance improvements while being more computationally efficient for deployment. It 'compresses' the teacher's expertise into the student.",
    "Related Patterns": [
      "Imitation Learning with Tool-Use Trajectories",
      "Supervised Fine-Tuning (SFT)",
      "Model Compression"
    ],
    "Uses": "Model compression, transferring specialized skills to smaller models, improving efficiency of agentic LLMs, creating domain-specific expert models."
  },
  {
    "Pattern Name": "Self-Correction with Tool-Interactive Critiquing",
    "Problem": "LLMs can make errors in complex reasoning tasks (e.g., logical flaws, computational mistakes, incorrect tool usage), and a single-pass generation often leads to suboptimal or incorrect solutions that lack robustness.",
    "Context": "Improving the reliability, accuracy, and robustness of LLM-generated solutions, particularly in tasks where errors can be identified and corrected through external feedback or internal reflection.",
    "Solution": "1. **Initial Generation:** The LLM generates an initial solution or a reasoning path (e.g., rationale, program).\n2. **Critiquing and Tool Feedback:** A 'critic' component (which can be the LLM itself, or another LLM/module) evaluates the generated output. This evaluation often involves executing generated code with external tools, checking logical consistency, or comparing against expected outcomes. The tool feedback (e.g., runtime errors, incorrect outputs) highlights discrepancies.\n3. **Revision:** Based on the critique and tool feedback, the LLM identifies errors, analyzes the failure, and revises its original solution or reasoning path. This process can be iterative, allowing for multiple rounds of generation, critique, and refinement until a satisfactory result is achieved or a stopping condition is met.",
    "Result": "Enhances the reliability and accuracy of LLM outputs, allows the model to recover from initial errors, and leads to more robust problem-solving, especially in complex and error-prone domains like mathematical reasoning and code generation.",
    "Related Patterns": [
      "Agentic AI Patterns",
      "Rejection Sampling Fine-Tuning",
      "Iterative Refinement",
      "Tools Integration Patterns"
    ],
    "Uses": "Complex problem solving, code generation and debugging, mathematical reasoning, factual consistency checking, improving robustness against errors."
  },
  {
    "Pattern Name": "Backward Reasoning for Verification",
    "Problem": "Verifying the correctness of a complex, multi-step solution or reasoning chain generated by an LLM can be challenging, especially when errors might be subtle or accumulate over steps, and a simple forward check is insufficient.",
    "Context": "Tasks where the desired final state or answer is known or can be objectively checked, and where the correctness of intermediate steps can be rigorously assessed by working backward from the goal.",
    "Solution": "1. **Forward Generation:** An LLM generates a solution or a sequence of reasoning steps from the problem statement to a proposed answer.\n2. **Backward Verification:** A 'verifier' component (which can be the LLM itself, or a specialized module) starts from the proposed final answer (or a later intermediate step) and logically works backward through the generated steps to the initial problem statement.\n3. **Consistency Check:** At each backward step, the verifier checks for logical consistency with the preceding steps (in the backward direction) or known facts. This can involve symbolic checks, tool execution, or formal logical inference. Any inconsistency flags an error.",
    "Result": "Provides a robust method for verifying the validity of LLM-generated solutions, helps in identifying errors earlier in the reasoning chain, and significantly improves the overall reliability and trustworthiness of the system's outputs.",
    "Related Patterns": [
      "Self-Correction",
      "Knowledge & Reasoning Patterns",
      "Planning Patterns"
    ],
    "Uses": "Theorem proving, mathematical reasoning, logical puzzles, formal verification of code, debugging complex reasoning chains."
  },
  {
    "Pattern Name": "Iterative Retrieval-Generation Synergy",
    "Problem": "LLMs can suffer from factual inaccuracies, hallucinations, or lack up-to-date domain-specific information. A single retrieval step might not provide all necessary context for complex or evolving generation tasks.",
    "Context": "Enhancing the factual grounding, comprehensiveness, and dynamism of LLM-generated text, particularly for knowledge-intensive or investigative tasks that benefit from continuous external information gathering.",
    "Solution": "1. **Initial Generation/Query:** The LLM generates an an initial response or formulates a query based on the prompt.\n2. **Retrieval:** This query (or parts of the generated response) is used to retrieve relevant information from an external knowledge base (e.g., search engines, databases, document repositories).\n3. **Contextualized Generation:** The retrieved information is integrated into the prompt or context and fed back to the LLM to refine, expand, or continue its generation.\n4. **Iteration:** This cycle of retrieval and generation is repeated iteratively. The LLM can generate new queries based on prior retrievals and generations, progressively gathering more context and refining its output until a comprehensive and accurate response is formed.",
    "Result": "Improves the factual accuracy, relevance, and depth of LLM-generated content by dynamically incorporating and iteratively refining based on external knowledge. Reduces hallucination and supports more informed, nuanced, and up-to-date responses.",
    "Related Patterns": [
      "Retrieval-Augmented Generation (RAG)",
      "Multi-Tool Agent",
      "Knowledge & Reasoning Patterns",
      "Tools Integration Patterns"
    ],
    "Uses": "Complex question answering, summarization, report generation, knowledge synthesis, domain-specific content creation, research assistance."
  }
]