[
  {
    "Pattern Name": "Tool-Integrated Reasoning Agent (TORA)",
    "Problem": "Large Language Models (LLMs) struggle with complex mathematical reasoning tasks that require both abstract analysis and planning (where natural language excels) and precise computation or symbolic manipulation (where programs and external tools are superior). Existing approaches (rationale-only or program-only) are insufficient on their own.",
    "Context": "An AI system (specifically an LLM) needs to solve problems that demand a combination of nuanced natural language reasoning and the rigorous, efficient application of external computational or symbolic tools. This is particularly relevant for quantitative, logical, or scientific problem-solving.",
    "Solution": "Design an agent that iteratively interleaves natural language rationales with program-based tool use. The agent first generates natural language reasoning for analysis, planning, or problem decomposition. When a sub-task requires precise computation, symbolic manipulation, or algorithmic processing, the agent generates a program (code) for an external tool (e.g., a computation library, symbolic solver). The tool executes the program, and its output is then fed back to the agent. The agent uses this output to continue its natural language reasoning, adjust its approach, solve subsequent sub-tasks, or finalize the answer. This cycle repeats until the problem is solved.",
    "Result": "Synergistically combines the analytical prowess and planning capabilities of natural language with the computational efficiency and rigor of external tools. This significantly improves performance on complex quantitative tasks, surpassing models that rely solely on natural language or program generation.",
    "Related Patterns": [
      "Output Space Shaping",
      "Trajectory Synthesis for Training"
    ],
    "Uses": "Mathematical problem-solving, scientific reasoning, complex quantitative analysis, agentic tasks requiring external computation, any domain where LLMs need to interact dynamically with structured tools for precise operations."
  },
  {
    "Pattern Name": "Output Space Shaping",
    "Problem": "Training Large Language Models (LLMs) via imitation learning on limited, often single-trajectory, high-quality data can restrict the model's output space. This limitation hinders the model's flexibility in exploring diverse, plausible reasoning steps during inference and can lead to improper tool-use behavior, especially in complex, multi-step tasks.",
    "Context": "Fine-tuning LLMs for complex reasoning tasks, particularly those involving tool use, where the initial training data (e.g., from human annotation or an expert model) may not fully capture the diversity of valid reasoning paths or sufficiently address error correction scenarios.",
    "Solution": "Enhance the model's training and output diversity by augmenting the training data beyond initial high-quality examples. This involves a multi-step process: \n1. **Sampling Diverse Valid Trajectories:** Use the current model (e.g., an imitation-learned model) to generate multiple diverse reasoning trajectories for existing training questions. Filter and retain only the trajectories that lead to correct answers and are free of tool-use errors.\n2. **Teacher-Corrected Invalid Trajectories:** For trajectories that lead to incorrect answers or contain errors, identify the point of failure. Leverage a more capable 'teacher model' (e.g., a larger or more advanced LLM) to correct the subsequent portions of these partially incorrect trajectories, thereby generating valid continuations.\n3. **Retraining:** Retrain the LLM on a combined dataset comprising the initial high-quality trajectories, the newly sampled valid trajectories, and the teacher-corrected valid trajectories. This broadens the model's learned output distribution and improves its robustness.",
    "Result": "Significantly boosts the model's reasoning capabilities, encourages the generation of diverse and plausible reasoning steps, mitigates improper tool-use behavior, and improves generalization across different problem types and difficulty levels. It allows smaller models to achieve performance comparable to or exceeding larger baselines.",
    "Related Patterns": [
      "Tool-Integrated Reasoning Agent",
      "Trajectory Synthesis for Training"
    ],
    "Uses": "Improving robustness and flexibility of LLMs in complex reasoning, enhancing tool-use reliability, fine-tuning agentic models, mitigating data scarcity issues in specialized domains, knowledge distillation, and improving the diversity and quality of generated outputs."
  },
  {
    "Pattern Name": "Trajectory Synthesis for Training",
    "Problem": "A significant challenge in training tool-integrated AI agents is the absence of high-quality, interactive tool-use annotations in existing datasets. These datasets typically provide only natural language rationales or final code, lacking the dynamic, interleaved interaction required for effective training.",
    "Context": "Developing and training AI agents or LLMs that need to learn from multi-step, interactive processes involving both natural language reasoning and external tool calls, especially when creating human-annotated data for such complex interactions is prohibitively expensive or time-consuming.",
    "Solution": "Leverage a powerful, pre-trained Large Language Model (e.g., GPT-4) as an 'expert annotator' to generate a corpus of interactive tool-use trajectories. This involves: \n1. **Prompt Curation:** Designing detailed few-shot prompts that instruct the LLM to generate trajectories in a desired interleaved format (e.g., natural language rationale -> program -> tool execution output -> next rationale). These prompts include descriptive examples to guide the generation style and format.\n2. **Interactive Generation:** For each problem, the LLM iteratively generates segments of the trajectory (rationale, then program, then processes tool output, then generates the next rationale) until a solution is reached.\n3. **Sampling and Filtering:** Employ strategies like greedy decoding to obtain initial trajectories. For problems where greedy decoding fails, use nucleus sampling to generate multiple, diverse candidate trajectories. Retain only the trajectories that yield correct answers and are free of tool-use errors, forming a high-quality synthetic dataset.",
    "Result": "Creates a rich, high-quality, and diverse corpus of interactive tool-use trajectories, effectively overcoming the data scarcity problem. This synthetic dataset serves as effective training data (e.g., for imitation learning) to fine-tune other LLMs or smaller models to learn complex interleaved reasoning and tool-use behaviors.",
    "Related Patterns": [
      "Tool-Integrated Reasoning Agent",
      "Output Space Shaping"
    ],
    "Uses": "Bootstrapping training data for agentic LLMs, developing datasets for tool-augmented language models, creating synthetic expert demonstrations for imitation learning, enhancing ML model capabilities in domains requiring complex interactive problem-solving, and accelerating the development of specialized AI agents."
  }
]