[
  {
    "Pattern Name": "Agentic Web Browser Interaction",
    "Problem": "Large Language Models (LLMs) inherently lack access to up-to-date information, struggle with factual accuracy, and are unable to perform complex, multi-step information-seeking tasks that require real-time interaction with external tools like the web.",
    "Context": "An LLM needs to answer open-ended, long-form questions that demand current information retrieval and synthesis from the web, beyond its pre-training data.",
    "Solution": "Design a text-based web-browsing environment that the LLM can interact with. The LLM is finetuned to act as an agent within this environment, receiving contextual prompts (e.g., the question, the current page's text, past actions) and issuing discrete commands (e.g., 'Search query', 'Clicked on link', 'Scrolled down', 'Quote text', 'End Answer') to navigate the web, retrieve information via an external search engine (like the Bing API), and synthesize a comprehensive answer.",
    "Result": "Enables the LLM to perform complex information-seeking tasks, access up-to-date information, significantly improve factual accuracy, and generate comprehensive, referenced long-form answers. This approach allows for end-to-end improvement of both information retrieval and synthesis.",
    "Related Patterns": [
      "Behavior Cloning (BC)",
      "Reward Modeling (RM)",
      "Reinforcement Learning from Human Feedback (RLHF)",
      "Rejection Sampling (Best-of-N)",
      "Reference Collection for Factual Accuracy"
    ],
    "Uses": "Long-form question answering, fact-checking, knowledge acquisition for LLMs, autonomous web agents, interactive information retrieval."
  },
  {
    "Pattern Name": "Behavior Cloning (BC)",
    "Problem": "Training an agent (LLM) to perform complex, multi-step tasks in an environment (like web browsing) where direct reward signals are sparse, difficult to define, or where a foundational policy is needed to mimic human-like interaction.",
    "Context": "An LLM needs to learn how to use a text-based web browsing environment by mimicking human behavior to answer long-form questions, establishing a baseline for complex interactive tasks.",
    "Solution": "Collect human demonstrations of the full task, including sequences of browsing actions (commands issued in the environment) and the final answer. Finetune the LLM using supervised learning, where the human commands and generated text serve as labels for the model's actions.",
    "Result": "The LLM acquires a foundational policy to interact with the environment and perform the task, learning to use the available tools and generate initial answers in a human-like manner. This provides a strong starting point for further optimization.",
    "Related Patterns": [
      "Agentic Web Browser Interaction",
      "Reward Modeling (RM)",
      "Reinforcement Learning from Human Feedback (RLHF)",
      "Rejection Sampling (Best-of-N)"
    ],
    "Uses": "Initial training for agentic LLMs, learning complex human-like behaviors, bootstrapping reinforcement learning agents, acquiring basic tool-use capabilities."
  },
  {
    "Pattern Name": "Reward Modeling (RM)",
    "Problem": "Quantifying the subjective quality of an AI's outputs (e.g., long-form answers, browsing trajectories) in a way that aligns with human preferences, especially when objective metrics are insufficient for criteria like factual accuracy, coherence, and overall usefulness.",
    "Context": "An LLM generates outputs (e.g., answers with references) that need to be evaluated and optimized based on human judgments of quality. Human feedback is available in the form of pairwise comparisons.",
    "Solution": "Collect human comparisons where labelers express preference between two model-generated answers (each with its collected references) to the same question. Train a separate neural network (the reward model) to take a question and an answer with references as input and output a scalar reward (e.g., an Elo score) that predicts human preferences using a cross-entropy loss over the comparison labels.",
    "Result": "A quantifiable proxy for human preferences that can be used for subsequent optimization (e.g., Reinforcement Learning, Rejection Sampling) and evaluation, effectively aligning the AI's quality assessment with human values and subjective criteria.",
    "Related Patterns": [
      "Behavior Cloning (BC)",
      "Reinforcement Learning from Human Feedback (RLHF)",
      "Rejection Sampling (Best-of-N)",
      "Reference Collection for Factual Accuracy"
    ],
    "Uses": "Training for Reinforcement Learning from Human Feedback, evaluating generative models, fine-tuning for subjective quality, aligning LLMs with human values, creating preference-based metrics."
  },
  {
    "Pattern Name": "Reinforcement Learning from Human Feedback (RLHF)",
    "Problem": "Improving an AI's ability to perform complex, sequential tasks in an environment or generate outputs where desired outcomes are subjective, difficult to programmatically define, and simple behavior cloning may not reach optimal performance. A key challenge is mitigating over-optimization of the potentially imperfect reward model and preventing policy divergence.",
    "Context": "An LLM has learned basic interaction with an environment (via Behavior Cloning) and a Reward Model exists to quantify human preferences. The goal is to further improve the agent's policy to maximize human-preferred outcomes while maintaining reasonable behavior and preventing the policy from straying too far from the initial learned policy.",
    "Solution": "Use a reinforcement learning algorithm (such as Proximal Policy Optimization - PPO) to finetune the behavior-cloned model. The reward signal for the RL agent during training is primarily derived from the reward model's score at the end of each episode. To prevent over-optimization and maintain stability, a KL divergence penalty from the original BC model (or a reference policy) is added at each token to the reward, regularizing the policy's updates.",
    "Result": "The AI agent learns to generate outputs and/or perform actions that are highly preferred by humans, often surpassing the performance of models trained solely on demonstrations. This method improves robustness and alignment with human values while controlling for unwanted policy shifts.",
    "Related Patterns": [
      "Agentic Web Browser Interaction",
      "Behavior Cloning (BC)",
      "Reward Modeling (RM)",
      "Rejection Sampling (Best-of-N)"
    ],
    "Uses": "Aligning LLMs with human values, training agents for complex tasks, improving generative model quality, reducing harmful or unhelpful outputs, fine-tuning interactive agents."
  },
  {
    "Pattern Name": "Rejection Sampling (Best-of-N)",
    "Problem": "Improving the quality and consistency of AI-generated outputs, especially when the underlying generative model's single-shot output is not consistently high quality, without requiring additional training of the generative policy. The environment can be unpredictable, making direct policy optimization challenging.",
    "Context": "A generative model (e.g., an LLM trained via Behavior Cloning or Reinforcement Learning) can produce multiple diverse outputs for a given input, and a Reward Model is available to score these outputs based on desired criteria (e.g., factual accuracy, coherence, human preference).",
    "Solution": "Generate 'N' diverse samples (e.g., long-form answers, potentially with their associated browsing trajectories) from the generative model for a given input. Use a pre-trained reward model to score each of these N samples based on human preferences. Select the sample with the highest reward score as the final output. This method trades increased inference-time compute for improved output quality.",
    "Result": "Significantly improves the quality, factual accuracy, and alignment of the final output by leveraging the diversity of multiple samples and the discriminative power of the reward model. This approach can even outperform direct RL-trained policies in some cases by allowing the model to 'try' many more options (e.g., visiting more websites) and evaluate information with hindsight.",
    "Related Patterns": [
      "Reward Modeling (RM)",
      "Behavior Cloning (BC)",
      "Reinforcement Learning from Human Feedback (RLHF)"
    ],
    "Uses": "Enhancing generative model performance, improving factual accuracy, reducing hallucinations, aligning outputs with preferences, post-processing for LLMs, especially when inference-time compute is available."
  },
  {
    "Pattern Name": "Reference Collection for Factual Accuracy",
    "Problem": "Evaluating the factual accuracy of AI-generated long-form answers is difficult, subjective, and requires independent research from human labelers, leading to noisy, expensive, and less transparent feedback. AI models often 'hallucinate' or provide unverifiable information.",
    "Context": "An AI system (e.g., an agentic LLM browsing the web) generates long-form answers that are intended to be factually accurate and need to be evaluated by humans (for training data collection or end-user consumption).",
    "Solution": "Design the AI to actively identify, extract, and 'quote' specific source passages (references) from the web pages it consults while synthesizing its answer. These collected references (including page title, domain name, and the extracted text) are then presented alongside the final answer to human evaluators and/or end-users.",
    "Result": "Enables more accurate, less noisy, and transparent human feedback on factual accuracy, as labelers can directly assess whether claims in the answer are supported by the provided references without independent research. This also increases trust and allows end-users to verify information, and contributes to reducing non-imitative falsehoods (hallucinations) by grounding the answer in retrieved text.",
    "Related Patterns": [
      "Agentic Web Browser Interaction",
      "Reward Modeling (RM)",
      "Reinforcement Learning from Human Feedback (RLHF)"
    ],
    "Uses": "Improving factuality in LLM-generated content, enhancing transparency, facilitating human evaluation of information-seeking agents, reducing hallucinations, building trust in AI-generated information."
  }
]