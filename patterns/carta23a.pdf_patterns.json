[
  {
    "Pattern Name": "LLM as RL Policy with Online Grounding",
    "Problem": "Large Language Models (LLMs), despite possessing abstract knowledge, often lack functional grounding and alignment with interactive environments, which limits their functional competence and ability to solve decision-making problems.",
    "Context": "An agent needs to operate in an interactive textual (or embodied) environment, solving goals specified in natural language, where its internal LLM knowledge must be aligned with external dynamics and relational structures at various levels of abstraction.",
    "Solution": "Use an LLM directly as the agent's policy. The agent's task description, current observation, and the set of possible actions are gathered into a prompt. Action probabilities are computed by leveraging the LLM's language modeling heads to calculate the conditional probabilities of tokens composing each action. This LLM policy is then progressively updated and functionally grounded using online Reinforcement Learning (e.g., PPO) based on real-time interactions, observations, and rewards from the environment. A value head can be added on top of the LLM for the RL algorithm.",
    "Result": "Drastically improves functional grounding, performance, sample efficiency, and generalization abilities (to new objects and some new tasks) compared to zero-shot LLM use or offline pre-finetuning. It enables LLMs to adapt to domain-specific vocabularies and quickly discard useless actions, leveraging their pretrained knowledge for faster skill acquisition.",
    "Related Patterns": [
      "Distributed LLM Policy Inference and Training",
      "Action Head for LLM Policy",
      "Behavioral Cloning for LLM Policy Initialization",
      "Reinforcement Learning from Human Feedback (RLHF) for LLMs",
      "LLM as High-Level Planner"
    ],
    "Uses": "Text-based interactive agents, embodied AI, robotics, learning to solve language-conditioned Reinforcement Learning tasks in complex environments."
  },
  {
    "Pattern Name": "LLM as High-Level Planner",
    "Problem": "Complex, long-horizon tasks for embodied agents or decision-making systems are difficult to plan directly or require strategic guidance beyond low-level actions. LLMs possess extensive prior knowledge that could aid in generating such plans.",
    "Context": "Robotics setups, embodied agents, or textual environments where an agent needs to break down a goal into a sequence of high-level steps or sub-goals. The LLM's role is to provide strategic direction rather than direct low-level control.",
    "Solution": "Leverage an LLM to generate high-level plans, sequences of actions, or sub-goals. The LLM acts as a planner, providing strategic guidance, but does not directly execute low-level actions. This approach often involves a closed-loop feedback mechanism where the LLM's plan is updated based on environmental observations or a 'reporter' providing useful information about the environment's state.",
    "Result": "Provides abstract, strategic guidance for agents, leveraging the LLM's extensive prior knowledge about the world to suggest plans of action to solve goals. However, without direct interaction-based grounding, it may suffer from misalignment for low-level execution or require external affordance functions to rerank proposed actions.",
    "Related Patterns": [
      "LLM as RL Policy with Online Grounding",
      "Behavioral Cloning for LLM Policy Initialization"
    ],
    "Uses": "Robotics, embodied tasks, vision-and-language navigation, textual adventure games, general decision-making requiring multi-step planning."
  },
  {
    "Pattern Name": "Behavioral Cloning for LLM Policy Initialization",
    "Problem": "Training LLMs for decision-making tasks in interactive environments can be sample inefficient when starting from scratch. LLMs may initially lack the specific behaviors or task-relevant knowledge required for effective interaction.",
    "Context": "LLM-based agents in interactive environments where expert demonstrations or a dataset of successful trajectories are available. This pattern is often used as a preparatory step before deploying an agent or engaging in online interaction.",
    "Solution": "Pre-finetune an LLM on a dataset of expert trajectories using Behavioral Cloning (BC) or Offline Reinforcement Learning. This process teaches the LLM to mimic expert actions given observations and goals, effectively initializing its policy with known good behaviors. The pre-trained policy can then be deployed directly or further finetuned with online interaction.",
    "Result": "Provides an initial policy that can mimic expert behavior, potentially improving starting performance and sample efficiency for subsequent online learning. However, models trained purely with BC may perform worse than online RL for complex tasks, especially if expert data is limited or contains suboptimal actions, as BC inherently lacks direct grounding through environmental interaction and active exploration.",
    "Related Patterns": [
      "LLM as RL Policy with Online Grounding",
      "LLM as High-Level Planner"
    ],
    "Uses": "Policy initialization for LLM-based agents, imitation learning, leveraging existing datasets for agent training, pre-training for online Reinforcement Learning."
  },
  {
    "Pattern Name": "Reinforcement Learning from Human Feedback (RLHF) for LLMs",
    "Problem": "Aligning the text generated by LLMs with nuanced human preferences, values, safety guidelines, or specific instructions, which are often difficult to capture with simple, hand-crafted reward functions.",
    "Context": "LLMs are used for natural language generation tasks (e.g., chatbots, content creation, instruction following) where the quality, helpfulness, or safety of the generated text needs to be evaluated and improved based on human judgment.",
    "Solution": "Treat the LLM's text generation as a sequential decision-making problem, where each generated token is an action. Collect human feedback on LLM outputs (e.g., preferences, ratings) to train a reward model. Then, use Reinforcement Learning (e.g., PPO) to finetune the LLM policy directly, using the learned reward model as the reward signal. The 'environment' in this context is typically the LLM itself and its output, with the next state being the previous state plus the newly generated token.",
    "Result": "Produces LLMs that generate text more aligned with human preferences, are more helpful, harmless, and honest, and better follow instructions. This can lead to models with fewer parameters outperforming larger models not fine-tuned with RLHF in terms of human alignment metrics.",
    "Related Patterns": [
      "LLM as RL Policy with Online Grounding"
    ],
    "Uses": "Chatbot alignment, instruction-following models (e.g., InstructGPT), content moderation, improving safety and ethical behavior of generative LLMs."
  },
  {
    "Pattern Name": "Distributed LLM Policy Inference and Training",
    "Problem": "Using large Language Models (LLMs) as real-time policies in online Reinforcement Learning (RL) is computationally intensive. It requires frequent and fast inference for action probability computation across many environments, and efficient distributed training for policy updates, which can lead to significant bottlenecks and make training intractable with a single LLM instance.",
    "Context": "Online RL setups where an LLM acts as the agent's policy, interacting with multiple environments (e.g., 32 BabyAIText environments) in parallel, and requiring frequent gradient updates to the large LLM parameters.",
    "Solution": "To accelerate online RL finetuning, deploy multiple LLM instances (workers) in parallel. For inference, distribute the task of computing action probabilities for different actions or environments across these workers (e.g., each worker scores a subset of actions). For training, leverage Distributed Data Parallelism (DDP) to compute gradients on mini-batches in parallel across LLM instances, then gather and update the models synchronously. A client-server architecture can manage communication and dispatch calls.",
    "Result": "Significantly reduces the computational time for LLM inference and training in online RL, enabling quasi-linear scaling with the number of deployed LLMs. This allows for the use of larger LLMs and more extensive interactions, making previously intractable experiments feasible and improving overall sample efficiency.",
    "Related Patterns": [
      "LLM as RL Policy with Online Grounding"
    ],
    "Uses": "Scaling online RL with large LLMs, high-throughput LLM inference for real-time decision-making, MLOps for LLM-based agents in interactive environments, distributed training of large models for agent policies."
  },
  {
    "Pattern Name": "Action Head for LLM Policy",
    "Problem": "When using an LLM as a policy, directly leveraging its language modeling heads to predict action probabilities can be inefficient, misaligned, or lead to slow learning, especially for non-pretrained LLMs, when the action space is fixed and distinct from the general token vocabulary, or if the LLM's final layer encodings are not directly suitable for language modeling.",
    "Context": "Deploying an LLM as an agent's policy in an RL environment where the agent needs to select an action from a predefined, often small, discrete action space. This is an alternative to using the LLM's original language modeling heads for action probability calculation.",
    "Solution": "Instead of relying on the LLM's inherent language modeling capabilities for action selection, add a dedicated 'action head' (e.g., a Multi-Layer Perceptron - MLP) on top of the LLM's last encoder or decoder layer. This action head directly maps the LLM's internal representation to a probability distribution over the specific, discrete action space.",
    "Result": "Can simplify the learning process for non-pretrained models and provide a more direct and potentially efficient way to derive action probabilities for a fixed, discrete action set. However, for fully pretrained LLMs, it might require more training steps to align the new action head with the LLM's existing knowledge compared to leveraging the language modeling heads directly, which already benefit from extensive pretraining.",
    "Related Patterns": [
      "LLM as RL Policy with Online Grounding"
    ],
    "Uses": "RL agents with LLMs, adapting LLMs to specific discrete action spaces, situations where a direct mapping from LLM embeddings to actions is preferred over token-based probability calculations, finetuning LLMs for specific tasks."
  }
]