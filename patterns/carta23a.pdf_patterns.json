[
  {
    "Pattern Name": "Online Reinforcement Learning for LLM Grounding (GLAM)",
    "Problem": "Large Language Models (LLMs) often suffer from a lack of functional grounding in interactive environments. Their abstract knowledge, derived from text corpora, can be misaligned with the environment's physics and dynamics, limiting their functional competence and ability to solve tasks through direct interaction. This is due to training processes not incentivized for problem-solving in an environment, and a lack of ability to identify causal structures or learn from interaction data.",
    "Context": "Interactive environments, especially textual worlds, where agents perceive observations and execute actions through natural language. The goal is to enable LLMs to act as agent policies, incrementally grounding and updating their knowledge with new observations collected through interaction.",
    "Solution": "Utilize a pre-trained LLM as the agent's policy. Progressively update the LLM's parameters using online Reinforcement Learning (RL), such as Proximal Policy Optimization (PPO), as the agent interacts with the environment. The LLM receives environmental observations and goal descriptions as input, selects actions, and uses the resulting rewards to finetune its policy, thereby functionally grounding its internal symbols to external dynamics.",
    "Result": "Drastically improved performance in solving RL tasks, boosted sample efficiency compared to zero-shot use or supervised finetuning, and enhanced generalization abilities to new objects and certain new tasks. It enables LLMs to escape the 'Tabula Rasa' RL setting by leveraging their prior knowledge.",
    "Related Patterns": [
      "Reinforcement Learning from Human Feedback (RLHF)",
      "LLM as Probabilistic Policy",
      "PPO Finetuning for LLM Policies",
      "LLM as High-Level Planner"
    ],
    "Uses": [
      "Solving decision-making problems in interactive textual environments",
      "Learning spatial and navigation tasks",
      "Object manipulation tasks",
      "Sequential reasoning problems in language-conditioned settings"
    ]
  },
  {
    "Pattern Name": "LLM as High-Level Planner",
    "Problem": "LLMs possess impressive abstract reasoning and planning capabilities, but they are not inherently grounded in the physical world or capable of executing low-level, embodied actions directly. This creates a gap between high-level plans generated by an LLM and the fine-grained actions required in interactive environments, particularly in robotics.",
    "Context": "Embodied AI systems, such as robots, or complex textual environments where an agent needs to perform a sequence of low-level actions to achieve a high-level goal. The LLM's role is to provide strategic guidance rather than direct control.",
    "Solution": "Employ an LLM to generate high-level plans, action sequences, or sub-goals. An external, more grounded component (e.g., a low-level policy, an affordance function, or a dedicated 'actor' agent) then interprets, executes, or reranks these suggestions. Feedback from the environment or a 'reporter' observing the environment can be fed back to the LLM to refine its planning.",
    "Result": "Leverages the LLM's prior knowledge and reasoning for complex tasks, allowing it to guide agent behavior effectively. However, it is often limited by the absence of direct grounding, requiring careful integration with environment-specific mechanisms.",
    "Related Patterns": [
      "Online Reinforcement Learning for LLM Grounding (GLAM)",
      "Prompt Template for Agent State/Goal Representation"
    ],
    "Uses": [
      "Robotics for task planning (e.g., SayCan, Code as Policies, Inner Monologue)",
      "Textual adventure games where LLMs plan actions for an agent",
      "Complex decision-making scenarios requiring abstract strategic thinking"
    ]
  },
  {
    "Pattern Name": "Reinforcement Learning from Human Feedback (RLHF)",
    "Problem": "Aligning the outputs of large language models, particularly in natural language generation tasks, with subjective and nuanced human preferences, values, and instructions. Crafting a precise programmatic reward function for such alignment is often impractical or impossible.",
    "Context": "Natural language generation tasks (e.g., summarization, dialogue, creative writing) where the quality, helpfulness, or safety of the generated text is best judged by humans. Text generation is viewed as a sequential decision-making process.",
    "Solution": "Collect a dataset of human preferences by having humans compare or rate different LLM-generated outputs. Train a separate 'reward model' (RM) on this human preference data to predict human-aligned rewards. Then, use a Reinforcement Learning algorithm (commonly PPO) to finetune the LLM's policy directly, using the learned reward model to provide a dense reward signal for each generated token or sequence, thereby optimizing the LLM to produce outputs that score highly with the RM.",
    "Result": "LLMs generate more human-aligned, helpful, honest, and harmless outputs, often achieving better alignment with fewer parameters compared to models trained solely on next-token prediction.",
    "Related Patterns": [
      "PPO Finetuning for LLM Policies",
      "Online Reinforcement Learning for LLM Grounding (GLAM)"
    ],
    "Uses": [
      "Improving chatbots (e.g., InstructGPT)",
      "Summarization tasks",
      "Dialogue systems",
      "Any text generation task requiring fine-grained alignment with human values and preferences"
    ]
  },
  {
    "Pattern Name": "LLM as Probabilistic Policy",
    "Problem": "How to effectively use a pre-trained Large Language Model, which is primarily designed for next-token prediction, as an agent's policy to select discrete actions from a predefined set in an interactive environment, especially when actions are described textually.",
    "Context": "Interactive environments where actions are represented as sequences of tokens (textual commands). The LLM needs to output a probability distribution over these possible actions to enable decision-making and learning via RL.",
    "Solution": "For each possible action (represented as a sequence of tokens, e.g., 'go forward'), compute its conditional probability given the current prompt (which includes the observation and goal) by multiplying the conditional probabilities of its constituent tokens, as calculated by the LLM's language modeling heads. These raw probabilities (or log probabilities) are then normalized (e.g., using a softmax function) across all possible actions to obtain a valid probability distribution from which an action can be sampled. This method can optionally use a variable temperature to address issues with very small probabilities.",
    "Result": "Leverages the LLM's inherent language modeling capabilities and extensive pre-trained knowledge for action selection. It avoids the need for ad-hoc mappings from generated text to actions or the addition of separate, randomly initialized action heads. It is robust to any action space that can be represented textually.",
    "Related Patterns": [
      "Online Reinforcement Learning for LLM Grounding (GLAM)",
      "PPO Finetuning for LLM Policies",
      "Prompt Template for Agent State/Goal Representation"
    ],
    "Uses": [
      "Textual adventure games",
      "Language-conditioned control tasks",
      "Any interactive environment where actions are natural language commands and LLMs are used as policies"
    ]
  },
  {
    "Pattern Name": "PPO Finetuning for LLM Policies",
    "Problem": "Efficiently adapting large pre-trained language models (LLMs) to serve as effective policies in reinforcement learning tasks, especially when the reward signal is sparse or originates from an external environment, and simultaneously learning a robust value function.",
    "Context": "Reinforcement Learning settings where an LLM is used as an agent's policy. The goal is to optimize the LLM's behavior to maximize cumulative rewards through interaction. LLMs are computationally intensive, making training challenging.",
    "Solution": "Implement the Proximal Policy Optimization (PPO) algorithm to update the LLM's parameters. The LLM's language modeling heads (or a dedicated action head) are used to compute the probability distribution over actions. To learn a value function, an additional value head (e.g., a MultiLayer Perceptron) is added on top of the LLM's final layers. Gradients are backpropagated through both the LLM's policy and its value head, allowing for concurrent optimization of action selection and value estimation.",
    "Result": "Improves the LLM's ability to learn optimal policies and value functions in interactive environments. It leverages the LLM's pre-trained knowledge for better sample efficiency and faster adaptation compared to training from scratch.",
    "Related Patterns": [
      "Online Reinforcement Learning for LLM Grounding (GLAM)",
      "Reinforcement Learning from Human Feedback (RLHF)",
      "LLM as Probabilistic Policy",
      "Distributed Inference and Training for LLM Agents"
    ],
    "Uses": [
      "Training LLM agents for interactive tasks (e.g., navigation, object manipulation)",
      "Language-conditioned control",
      "Policy learning in textual environments",
      "Fine-tuning foundation models for specific RL objectives"
    ]
  },
  {
    "Pattern Name": "Distributed Inference and Training for LLM Agents",
    "Problem": "The computational expense and memory footprint of using very large LLMs as agent policies make online reinforcement learning (RL) intractable. Specifically, frequent forward passes for action probability computation for potentially many actions, combined with the need for numerous environment interactions, create significant bottlenecks for both inference and training.",
    "Context": "Online RL scenarios involving LLM agents, where the LLM is large (hundreds of millions to billions of parameters), the action space might be extensive, and a high volume of environmental interactions is required for learning.",
    "Solution": "Employ a distributed architecture where multiple LLM 'workers' are deployed in parallel. For inference, distribute the task of computing action probabilities across these workers, with each worker handling a subset of actions, achieving quasi-linear speedup. For training, utilize distributed data parallelism, where forward and backward passes for minibatches are dispatched across different LLM instances, and gradients are gathered and synchronized before updating the models. A library (e.g., Lamorel) can abstract these distributed operations.",
    "Result": "Overcomes computational bottlenecks, enabling large-scale online RL finetuning of LLMs. Allows for the use of larger LLMs and more complex environments, making research and practical deployment of LLM-based agents feasible.",
    "Related Patterns": [
      "PPO Finetuning for LLM Policies",
      "MLOps Patterns (specifically distributed training/inference)",
      "Tools Integration Patterns"
    ],
    "Uses": [
      "Scaling online RL finetuning of LLMs (e.g., FlanT5 780M, FlanT5 3B)",
      "Enabling research on the impact of LLM size and action space on learning",
      "Deploying LLM-based agents in environments requiring fast, frequent decisions"
    ]
  },
  {
    "Pattern Name": "Prompt Template for Agent State/Goal Representation",
    "Problem": "Effectively conveying the current state of an interactive environment, the agent's goal, and the available actions in a structured, coherent manner to a Large Language Model so it can accurately interpret the context and generate appropriate actions or responses.",
    "Context": "Using LLMs as agents in textual or language-conditioned environments where all relevant information for decision-making must be encapsulated within a single textual prompt. The LLM's performance is highly sensitive to prompt structure.",
    "Solution": "Design a standardized and structured prompt template that concatenates various pieces of information. This typically includes: 1) A header listing all possible actions, 2) The explicit goal description for the agent, 3) A short-term memory component (e.g., descriptions of the last few observations and actions), 4) A detailed description of the current observation, and 5) A clear indication for where the LLM should 'fill in' the next action. While finetuning can help the LLM adapt, a well-designed template can significantly improve initial performance.",
    "Result": "Provides the LLM with a comprehensive and consistent input, allowing it to better understand the task context, current situation, and available choices. This structured input facilitates more accurate and relevant action selection by the LLM.",
    "Related Patterns": [
      "Prompt Design Patterns",
      "LLM as Probabilistic Policy",
      "Online Reinforcement Learning for LLM Grounding (GLAM)"
    ],
    "Uses": [
      "LLM-based agents in text-based games (e.g., BabyAIText)",
      "Language-conditioned control tasks",
      "Any interactive system where LLMs act as decision-makers based on textual context"
    ]
  },
  {
    "Pattern Name": "Pretraining with Behavioral Cloning (then Finetuning with RL)",
    "Problem": "Training an agent policy efficiently in complex environments. While expert demonstrations can provide a strong initial policy, they might not cover all scenarios, lead to optimal performance, or allow for adaptation to new dynamics. Pure online RL from scratch can be sample-inefficient.",
    "Context": "Reinforcement Learning settings where a dataset of expert trajectories or demonstrations is available, but the ultimate goal is to achieve performance beyond the demonstrations, or to adapt to a dynamic environment through interaction. Applicable to various policy architectures, including Transformer models and LLMs.",
    "Solution": "First, pretrain the agent's policy using Behavioral Cloning (BC) or Offline Reinforcement Learning on a dataset of expert trajectories. This initial phase leverages existing knowledge to provide a strong, stable starting point. Subsequently, the pre-trained model is finetuned using online Reinforcement Learning (RL) by allowing the agent to interact with the environment, collect its own experiences, and optimize its policy based on environmental rewards.",
    "Result": "Combines the benefits of learning from demonstrations (e.g., improved sample efficiency, a good initialization) with the benefits of online RL (e.g., adaptability, potential for super-human performance, exploration). It can lead to better performance than BC alone, as direct interaction allows for crucial grounding.",
    "Related Patterns": [
      "Online Reinforcement Learning for LLM Grounding (GLAM)",
      "PPO Finetuning for LLM Policies",
      "MLOps Patterns (for data collection and management)"
    ],
    "Uses": [
      "Robotics for learning skills from demonstrations and adapting to real-world variations",
      "Embodied AI tasks",
      "Training large language models for decision-making (e.g., Online Decision Transformer)",
      "Any task where a combination of expert knowledge and environmental interaction is beneficial for policy learning"
    ]
  }
]