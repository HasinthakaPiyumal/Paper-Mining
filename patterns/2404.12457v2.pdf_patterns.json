[
  {
    "Pattern Name": "RAG Knowledge Cache",
    "Problem": "High computation and memory costs in Retrieval-Augmented Generation (RAG) systems due to long input sequences from retrieved documents, leading to redundant recomputation of Key-Value (KV) tensors for frequently accessed knowledge. Existing LLM inference optimizations do not fully leverage RAG-specific characteristics.",
    "Context": "RAG systems where Large Language Models (LLMs) are augmented with external knowledge, resulting in long input sequences. Many requests share common retrieved documents, and document access patterns are skewed (a small fraction of documents is frequently accessed). GPU memory is limited for storing KV tensors of long sequences.",
    "Solution": "Implement a multilevel dynamic caching system (RAGCache) that stores the intermediate Key-Value (KV) tensors of retrieved documents. This cache hierarchically spans fast GPU memory (for hot documents) and slower host memory (for less frequent documents). The system reuses these cached KV tensors across multiple requests to avoid redundant computation.",
    "Result": "Significantly reduces Time to First Token (TTFT) by up to 4% and improves throughput by up to 21% by minimizing KV tensor recomputation. Lowers prefill latency, even with host-GPU memory transfer overhead. Outperforms state-of-the-art LLM serving systems by effectively managing KV cache for RAG's long sequences.",
    "Related Patterns": [
      "Prefix-Aware KV Cache Replacement Policy (PGDSF)",
      "RAG Cache-Aware Request Reordering",
      "Dynamic Speculative RAG Pipelining"
    ],
    "Uses": "Accelerating LLM inference in RAG systems, improving efficiency for knowledge-intensive NLP tasks (e.g., question answering, content creation), and optimizing resource utilization in RAG serving."
  },
  {
    "Pattern Name": "Prefix-Aware KV Cache Replacement Policy (PGDSF)",
    "Problem": "Efficiently managing a hierarchical Key-Value (KV) cache for RAG documents is challenging because LLM attention mechanisms are sensitive to document order, making KV tensors context-dependent. Traditional caching policies (LRU, LFU, generic GDSF) do not account for variable document sizes, access costs (recomputation time), or the prefix-dependent nature of KV tensors, leading to suboptimal cache hit rates and evictions.",
    "Context": "A RAG Knowledge Cache (multilevel, spanning GPU and host memory) storing KV tensors of retrieved documents. The order of documents in an LLM's input sequence affects the KV tensor values. Cache capacity is limited, necessitating intelligent eviction.",
    "Solution": "Organize document KV tensors in a 'knowledge tree' structure, where paths represent document sequences and nodes hold KV tensors, enabling sharing of common prefixes. Employ a Prefix-aware GreedyDualSizeFrequency (PGDSF) replacement policy. PGDSF calculates a priority for each node based on access frequency, KV tensor size, recency (last access time), and a prefix-aware recomputation cost. Nodes with lower priority are evicted first, with a 'swap-out-only-once' strategy to minimize GPU-host data transfer.",
    "Result": "Maximizes cache hit rates (10-21% improvement over GDSF, LRU, LFU) and minimizes cache miss rates by making informed eviction decisions that account for LLM's prefix sensitivity and hierarchical memory characteristics. Achieves 10-29% lower average TTFT compared to baseline policies. Ensures that valuable prefixes remain in faster memory.",
    "Related Patterns": [
      "RAG Knowledge Cache"
    ],
    "Uses": "Optimizing memory management and cache performance in RAG systems, particularly for order-sensitive LLM KV caches."
  },
  {
    "Pattern Name": "RAG Cache-Aware Request Reordering",
    "Problem": "Unpredictable arrival patterns of user requests in RAG systems can lead to inefficient cache utilization and 'cache thrashing,' where documents are frequently swapped in and out, resulting in a low cache hit rate and increased recomputation costs.",
    "Context": "A RAG system utilizing a Key-Value (KV) cache (like the RAG Knowledge Cache) to store intermediate states of retrieved documents. Requests arrive asynchronously, and the system needs to process them in an order that maximizes cache reuse.",
    "Solution": "Implement a request scheduling algorithm that uses a priority queue to reorder incoming RAG requests. Requests are prioritized based on a metric, `OrderPriority = Cached Length / Computation Length`, which favors requests with a larger proportion of their required context already in the cache relative to the portion that needs recomputation. This strategy aims to maximize cache hits. A 'window' mechanism is used to ensure fairness and prevent request starvation.",
    "Result": "Improves cache hit rates and reduces total computation time (by 12-21% average TTFT reduction) by strategically processing requests that can benefit most from the existing cache content. Mitigates cache volatility and enhances overall system throughput, especially under high request rates.",
    "Related Patterns": [
      "RAG Knowledge Cache"
    ],
    "Uses": "Enhancing the efficiency and throughput of RAG serving systems by optimizing the order of request processing based on cache state."
  },
  {
    "Pattern Name": "Dynamic Speculative RAG Pipelining",
    "Problem": "Sequential execution of the retrieval (CPU-bound) and LLM generation (GPU-bound) steps in RAG systems leads to significant end-to-end latency and underutilization of GPU resources during the retrieval phase. Additionally, initiating speculative generations without careful consideration can introduce unnecessary LLM computation, degrading performance under high system loads.",
    "Context": "A RAG workflow where relevant documents are first retrieved (e.g., via vector search) and then used to augment an LLM prompt for generation. The retrieval process may produce early, incomplete results before the final set of documents is determined.",
    "Solution": "Dynamically overlap the knowledge retrieval and LLM inference steps. The vector search is broken into stages, continuously sending candidate documents to the LLM engine for speculative generation. If the newly retrieved candidates differ from previous ones, the ongoing speculative generation is terminated, and a new one begins. This speculative generation is dynamically enabled only when retrieved documents change and the number of pending LLM requests is below a predetermined maximum batch size, balancing latency reduction with computational overhead.",
    "Result": "Reduces end-to-end latency and Time to First Token (TTFT) by up to 16% by initiating LLM inference earlier and minimizing the idle time of the GPU. Improves resource utilization by leveraging available GPU capacity during retrieval. Decreases non-overlapping vector search time by 15-43%.",
    "Related Patterns": [
      "RAG Knowledge Cache"
    ],
    "Uses": "Accelerating RAG systems, particularly when retrieval latency is a bottleneck, by intelligently overlapping computationally distinct stages and managing speculative execution."
  }
]