[
  {
    "Pattern Name": "Retrieval-Augmented Generation (RAG)",
    "Problem": "Large Language Models (LLMs) often suffer from factual inaccuracies (hallucinations), lack access to up-to-date or domain-specific information, and their knowledge is limited to their training data, leading to suboptimal generation quality for knowledge-intensive tasks.",
    "Context": "Scenarios requiring LLMs to generate accurate, relevant, and contextually rich responses based on external, dynamic, or specific knowledge, such as question answering, summarization, or content creation.",
    "Solution": "Integrate LLMs with external knowledge databases. A system first retrieves relevant documents or information from a knowledge base (e.g., vector database) based on the user's query. This retrieved information is then appended to the original prompt, creating an 'augmented request,' which is fed to the LLM for generation.",
    "Result": "Significantly improved generation quality, reduced factual errors, access to up-to-date information, and enhanced contextual understanding, often outperforming finetuned LLMs for specific downstream tasks.",
    "Related Patterns": [
      "KV Cache Management",
      "Vector Search"
    ],
    "Uses": [
      "Question Answering",
      "Content Creation",
      "Code Generation",
      "Enhancing NLP tasks that require external knowledge"
    ],
    "Category": "Generative AI Patterns"
  },
  {
    "Pattern Name": "Knowledge Tree for KV Cache",
    "Problem": "In RAG systems, the Key-Value (KV) tensors generated by LLMs are sensitive to the order of preceding tokens. When retrieving and injecting multiple documents, the KV tensors for a document sequence are different from another sequence. Efficiently storing, retrieving, and sharing these order-dependent KV tensors across multiple requests, especially when documents are frequently reused in varying orders, is challenging.",
    "Context": "RAG systems that aim to cache and reuse the intermediate KV states of retrieved documents to reduce recomputation, particularly when LLMs use attention mechanisms where token order matters. The cache needs to support hierarchical memory (GPU/host).",
    "Solution": "Organize the intermediate KV tensors of retrieved documents in a tree structure, specifically a prefix tree (trie), based on document IDs. Each path from the root to a node represents a specific sequence of documents, with nodes holding the KV tensors for a referred document in that sequence. This allows different request paths to share common prefixes (nodes) and their associated KV tensors.",
    "Result": "Facilitates fast and order-aware retrieval of KV tensors, enables sharing of intermediate states for common document prefixes across multiple requests, and aligns with hierarchical memory structures (GPU/host). Reduces redundant computation and improves efficiency.",
    "Related Patterns": [
      "KV Cache Management",
      "Retrieval-Augmented Generation (RAG)",
      "Prefix-aware GreedyDualSizeFrequency (PGDSF) Replacement Policy"
    ],
    "Uses": [
      "Managing and sharing KV cache for retrieved documents in RAG systems to optimize LLM inference"
    ],
    "Category": "LLM-specific Patterns"
  },
  {
    "Pattern Name": "Prefix-aware GreedyDualSizeFrequency (PGDSF) Replacement Policy",
    "Problem": "In a hierarchical caching system for RAG's KV tensors (e.g., GPU and host memory), deciding which cached document KV tensors to evict is complex. Traditional policies (LRU, LFU, GDSF) do not adequately account for the variable sizes of document KV tensors, their access frequency, recency, and critically, the 'prefix-aware recomputation cost' which varies based on the document's position in a sequence and whether preceding documents are also cached. This leads to suboptimal cache hit rates and inefficient resource utilization.",
    "Context": "Multilevel caching systems (e.g., GPU/host memory) for LLM KV caches in RAG, where documents have variable sizes, access patterns are skewed, and the cost of recomputing KV tensors is dependent on the cached prefix context.",
    "Solution": "Extend the classic GDSF policy by calculating a priority for each cached node (document KV tensor) based on its access frequency, size, last access time (recency), and a 'prefix-aware recomputation cost.' The cost estimation is sophisticated, amortizing the cost across non-cached tokens and using offline profiling with bilinear interpolation to estimate compute time for varying cached/non-cached token lengths. Nodes with lower priority are evicted first.",
    "Result": "Minimizes cache miss rates, ensures that the most valuable and frequently used KV tensors (especially those forming critical prefixes) are retained in faster memory tiers. This leads to higher cache hit rates and significant reductions in Time-to-First-Token (TTFT) and improved throughput for RAG systems.",
    "Related Patterns": [
      "Knowledge Tree (for KV Cache)",
      "KV Cache Management"
    ],
    "Uses": [
      "Cache eviction and placement decisions in hierarchical KV cache systems for RAG"
    ],
    "Category": "LLM-specific Patterns"
  },
  {
    "Pattern Name": "Cache-aware Reordering (Request Scheduling)",
    "Problem": "In LLM serving systems, particularly RAG, unpredictable arrival patterns of user requests can lead to cache thrashing. Requests that could benefit from shared KV cache might arrive in an order that forces frequent evictions and recomputations, resulting in a low cache hit rate and reduced overall efficiency, especially under high request loads.",
    "Context": "LLM serving systems for RAG applications with a shared KV cache and a queue of incoming requests, where cache hit rate is critical for performance.",
    "Solution": "Implement a request scheduling algorithm that reorders incoming requests in a priority queue. The priority metric, `OrderPriority = Cached Length / Computation Length`, prioritizes requests that have a larger portion of their context already cached relative to the amount of new computation required. A window size is used to ensure fairness and prevent starvation.",
    "Result": "Improves the cache hit rate, reduces total computation time, and optimizes resource utilization. This strategy effectively mitigates cache volatility and enhances overall system throughput and Time-to-First-Token (TTFT), particularly under high request rates.",
    "Related Patterns": [
      "MLOps Serving Patterns",
      "KV Cache Management"
    ],
    "Uses": [
      "Optimizing request scheduling in RAG serving to maximize KV cache utilization"
    ],
    "Category": "MLOps Patterns"
  },
  {
    "Pattern Name": "Dynamic Speculative Pipelining (for RAG)",
    "Problem": "In the RAG workflow, vector retrieval (often CPU-bound) and LLM inference (GPU-bound) are typically executed sequentially. This leads to significant idle time for the GPU during retrieval and long end-to-end latency, especially when retrieval itself can be time-consuming or require high accuracy.",
    "Context": "RAG systems where retrieval and generation are distinct steps, and retrieval can produce intermediate candidate results before completion. Minimizing end-to-end latency is a critical performance goal.",
    "Solution": "Overlap the knowledge retrieval and LLM inference steps. The vector search process is split into multiple stages, continuously providing candidate documents. The LLM engine can then initiate 'speculative generation' using these early candidate documents. If later stages of retrieval produce different (more accurate) documents, the current speculative generation is terminated, and a new one starts. If the documents remain the same, generation continues. This pipelining is dynamically enabled based on system load (e.g., only when the LLM request pool is empty or below a certain threshold) to avoid unnecessary computation under high load.",
    "Result": "Significantly reduces end-to-end latency by minimizing idle GPU time, decreases non-overlapping vector search time, and improves Time-to-First-Token (TTFT). Balances latency reduction with computational overhead by dynamic activation.",
    "Related Patterns": [
      "Tools Integration Patterns",
      "MLOps Serving Patterns"
    ],
    "Uses": [
      "Accelerating the RAG workflow by concurrently executing retrieval and generation, particularly beneficial when retrieval latency is substantial"
    ],
    "Category": "MLOps Patterns"
  },
  {
    "Pattern Name": "Swap-out-Only-Once for Hierarchical KV Cache",
    "Problem": "In hierarchical caching systems (e.g., GPU HBM and host memory connected via PCIe), frequent data transfers of large Key-Value (KV) tensors between different memory tiers can become a significant performance bottleneck due to the lower bandwidth of interconnects like PCIe compared to GPU HBM.",
    "Context": "Multilevel caching systems for LLM KV caches, where data needs to be moved between fast (GPU) and slower (host) memory, and minimizing transfer overhead is crucial.",
    "Solution": "When a KV tensor (node in the Knowledge Tree) is first evicted from GPU memory, it is swapped out and copied to the host memory. For all subsequent evictions of that same KV tensor from GPU memory, it is simply freed without copying it again, as a copy already exists in the host memory. The host memory retains its copy until the node is evicted from the entire cache system.",
    "Result": "Minimizes redundant data transfers between GPU and host memory, significantly reducing the overhead associated with cache evictions and improving overall cache performance, especially when host memory capacity is much larger than GPU memory.",
    "Related Patterns": [
      "KV Cache Management",
      "Knowledge Tree (for KV Cache)"
    ],
    "Uses": [
      "Optimizing memory transfer efficiency in hierarchical KV cache management for LLMs"
    ],
    "Category": "LLM-specific Patterns"
  },
  {
    "Pattern Name": "Fault-Tolerant Knowledge Cache",
    "Problem": "In RAG systems employing a hierarchical KV cache (e.g., Knowledge Tree with GPU and host memory), hardware failures (like GPU failure) can lead to the invalidation of critical cached intermediate states (KV tensors). Given the prefix sensitivity of LLM inference, a GPU failure might invalidate an entire subtree of dependent KV caches, leading to significant data loss and recovery overhead. Request processing failures also need robust handling.",
    "Context": "RAG systems with complex, stateful caching mechanisms (like Knowledge Tree) that are susceptible to hardware or processing failures, where quick recovery and continued operation are important for system reliability and user experience.",
    "Solution": "Implement mechanisms to ensure the resilience of the KV cache. This includes replicating a portion of the most frequently accessed and critical upper-level nodes (e.g., system prompts or common document prefixes) in the more resilient host memory. Additionally, a timeout mechanism is used to detect and retry failed requests. If a request fails before its first iteration, it's recomputed; otherwise, it can continue using its stored KV cache.",
    "Result": "Enhances system reliability and availability by providing mechanisms for fast recovery from GPU failures and robust handling of request processing errors. Minimizes data loss and reduces the impact of transient failures on RAG service continuity.",
    "Related Patterns": [
      "Knowledge Tree (for KV Cache)",
      "MLOps Reliability Patterns"
    ],
    "Uses": [
      "Ensuring robustness and availability of RAG serving systems"
    ],
    "Category": "MLOps Patterns"
  }
]