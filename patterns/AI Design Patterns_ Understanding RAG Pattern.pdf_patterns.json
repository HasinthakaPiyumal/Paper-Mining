[
  {
    "Pattern Name": "Interpretability-Constrained Learning",
    "Problem": "Achieving high accuracy with black-box models often sacrifices interpretability, leading to a lack of understanding and trust in critical applications.",
    "Context": "Designing and training AI/ML models, especially for high-risk applications (e.g., healthcare, finance, criminal justice), where both high performance and inherent interpretability are non-negotiable requirements.",
    "Solution": "Integrate interpretability criteria (e.g., model size, number of nodes/rules, number of non-zero weights) directly into the model's optimization problem during training, typically as a penalty term in the loss function or as explicit constraints.",
    "Result": "Develops models that are inherently transparent and understandable by design, while still striving for high predictive performance, overcoming the traditional accuracy-interpretability tradeoff.",
    "Related Patterns": [],
    "Uses": "Healthcare, criminal justice, finance, ethical AI, applications requiring regulatory compliance (e.g., GDPR 'right to explanation')."
  },
  {
    "Pattern Name": "Global Surrogate Model",
    "Problem": "Understanding the overall logic and global behavior of a complex, black-box AI model that cannot be directly inspected.",
    "Context": "When a high-performing black-box model is deployed, and a holistic, high-level understanding of its decision-making process across the entire dataset is required, for purposes like auditing or general comprehension.",
    "Solution": "Train a simpler, inherently interpretable model (e.g., decision tree, rule-based model) on the predictions generated by the original black-box model across the entire dataset. This interpretable model acts as a 'surrogate' that mimics the black-box model's global behavior.",
    "Result": "Provides a global, understandable approximation of the black-box model's logic, offering insights into its overall decision-making patterns.",
    "Related Patterns": ["Local Surrogate Model", "Local Rule-Based Explanation"],
    "Uses": "Model auditing, general model understanding, compliance, comparing overall model behaviors."
  },
  {
    "Pattern Name": "Local Surrogate Model",
    "Problem": "Explaining why a specific individual prediction was made by a complex, black-box AI model, as its global behavior may be too complex to understand or faithfully mimic.",
    "Context": "When a detailed, instance-level explanation is needed for a black-box model's output, especially for critical decisions, without needing to understand the entire model's logic.",
    "Solution": "In the vicinity of the specific instance to be explained, generate a local dataset (e.g., through perturbed samples or nearest neighbors). Then, train a simple, interpretable model (e.g., linear model, rule-based model) on this local dataset, using the black-box model's predictions as labels, to approximate the black-box model's behavior *only in that locality*.",
    "Result": "Provides instance-specific, understandable explanations (e.g., feature importances, local rules) that are locally faithful to the black-box model's decision for that particular instance.",
    "Related Patterns": ["Global Surrogate Model", "Local Rule-Based Explanation", "Shapley Value Explanation", "Human-in-the-Loop Explanation"],
    "Uses": "Debugging individual predictions, building trust, human-in-the-loop decision support, understanding specific decision boundaries."
  },
  {
    "Pattern Name": "Shapley Value Explanation",
    "Problem": "Quantifying the fair and consistent contribution of each feature (or group of features) to a specific individual prediction from a black-box AI model, accurately reflecting feature interactions.",
    "Context": "When a precise, fair, and axiomatic allocation of responsibility for an individual prediction across its input features is required, especially when feature interactions are significant.",
    "Solution": "Apply the Shapley value concept from cooperative game theory, treating input features as players in a game and the prediction (or prediction probability) as the game's payout. The Shapley value for a feature is its average marginal contribution across all possible permutations (coalitions) of features.",
    "Result": "Provides a unique, fair, and consistent attribution of an individual prediction to its input features, inherently capturing interaction effects, which can be summarized in a feature importance vector.",
    "Related Patterns": ["Local Surrogate Model", "Local Rule-Based Explanation", "Divergent Subgroup Analysis"],
    "Uses": "Explaining individual predictions, fairness analysis (e.g., for bias detection), understanding feature impact, model auditing."
  },
  {
    "Pattern Name": "Local Rule-Based Explanation",
    "Problem": "Providing an easily understandable, qualitative explanation for an individual prediction of a black-box AI model, highlighting specific combinations of feature values that led to the decision.",
    "Context": "When human users (e.g., domain experts, end-users) need clear, logical 'if-then' rules to understand the local reasons for an AI model's decision, often for debugging, trust-building, or compliance.",
    "Solution": "Derive a set of simple, human-interpretable 'if-then' rules that accurately describe the black-box model's behavior in the local neighborhood of a specific instance. These rules capture the relevant associations of attribute values with the predicted class. The rules can be extracted from a local surrogate model (e.g., an associative classifier or decision tree) trained on local data.",
    "Result": "Offers qualitative insights into individual predictions through interpretable rules, making the decision-making process transparent at an instance level and revealing specific feature value configurations that drive a decision.",
    "Related Patterns": ["Local Surrogate Model", "Human-in-the-Loop Explanation"],
    "Uses": "Human-in-the-loop inspection, debugging specific predictions, trust-building, compliance (e.g., GDPR 'right to explanation')."
  },
  {
    "Pattern Name": "Counterfactual Explanation",
    "Problem": "Explaining why a specific prediction was made by an AI model and, more importantly, identifying the minimal changes to the input features that would alter that prediction to a desired alternative outcome.",
    "Context": "When users need actionable guidance on how to achieve a different AI outcome (e.g., 'What if I had done X instead of Y?'), or for auditing models for potential biases by seeing what minimal changes would flip a discriminatory decision.",
    "Solution": "Given an instance and its prediction, search for a new, synthetic instance that is as close as possible to the original instance (e.g., in feature space) but results in a different, desired prediction from the black-box model. The differences between the original and the counterfactual instance form the explanation.",
    "Result": "Provides intuitive and actionable explanations by showing the 'what-if' scenarios, enabling users to understand decision boundaries and potentially influence future AI outcomes.",
    "Related Patterns": ["Human-in-the-Loop Explanation"],
    "Uses": "User guidance for decision-making, fairness auditing, understanding decision boundaries, policy making, debugging."
  },
  {
    "Pattern Name": "Divergent Subgroup Analysis",
    "Problem": "Identifying and characterizing specific data subgroups where an AI model exhibits a significantly different or anomalous behavior (e.g., higher error rates, lower accuracy, specific biases) compared to its overall performance.",
    "Context": "When auditing AI models for fairness, validating performance across diverse populations, debugging unexpected behaviors in specific data segments, or trying to uncover hidden biases not tied to predefined protected attributes.",
    "Solution": "Systematically explore a large number of data subgroups (often defined by combinations of attribute values, i.e., patterns or itemsets). For each subgroup, quantify its 'divergence' by comparing a chosen performance metric (e.g., false positive rate, false negative rate, accuracy) within the subgroup to the same metric calculated over the entire dataset. This often involves leveraging efficient data mining techniques (like frequent pattern mining) to find sufficiently represented subgroups and statistical tests to assess significance.",
    "Result": "Reveals critical data segments where the model performs poorly or exhibits bias, enabling targeted debugging, fairness interventions, and model improvement. It can also identify 'corrective items' that reduce divergence when added to a pattern.",
    "Related Patterns": ["Shapley Value Explanation", "Human-in-the-Loop Explanation"],
    "Uses": "Model validation, fairness auditing, bias detection, error analysis, targeted interventions, understanding model generalization."
  },
  {
    "Pattern Name": "Human-in-the-Loop Explanation",
    "Problem": "AI systems, especially black-box models, often lack transparency, hindering human understanding, trust, and the ability of domain experts to debug or improve them.",
    "Context": "Designing and deploying AI systems in critical applications where continuous human oversight, validation, debugging, and collaboration with AI are essential, and where users need to actively investigate and query AI decisions.",
    "Solution": "Implement interactive interfaces and tools that allow users to: Actively query and inspect AI explanations (e.g., local rules, feature importances) for individual predictions or specific data subgroups; Perform 'what-if' analyses by interactively changing input features and observing the impact on predictions and explanations; Test their own hypotheses or user-defined rules against the model's behavior; Compare explanations across different models or for different target classes; Aggregate local explanations to gain global insights.",
    "Result": "Fosters trust, facilitates model debugging, allows for active hypothesis testing, supports continuous learning and improvement of AI systems based on human feedback, and enables expert-AI collaboration.",
    "Related Patterns": ["Local Rule-Based Explanation", "Counterfactual Explanation", "Divergent Subgroup Analysis"],
    "Uses": "Model validation, debugging, trust-building, compliance, expert-AI collaboration, active learning, policy making."
  }
]