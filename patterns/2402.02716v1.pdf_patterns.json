[
  {
    "Pattern Name": "Task Decomposition",
    "Problem": "Complex, multi-step tasks are difficult for LLM-based agents to plan directly or address through a one-step planning process.",
    "Context": "LLM-based agents need to accomplish intricate, long-horizon tasks in real-world environments.",
    "Solution": "Decompose a complex task into a sequence of simpler subtasks. This can be done either 'decomposition-first' (all subtasks defined upfront) or 'interleaved' (subtasks are dynamically generated and planned one by one with environmental feedback).",
    "Result": "Simplifies complex planning, reduces the cognitive load on the LLM, and enhances the agent's ability to solve challenging tasks.",
    "Related Patterns": [
      "Multiplan Selection",
      "External Planner-Aided Planning",
      "Reflection and Refinement",
      "Memory-Augmented Planning"
    ],
    "Uses": "Multimodal tasks (e.g., image generation, object recognition), mathematical reasoning, commonsense reasoning, symbolic reasoning, robotics, vision-and-language navigation."
  },
  {
    "Pattern Name": "Multiplan Selection",
    "Problem": "A single plan generated by an LLM for a complex task is often suboptimal or infeasible due to the LLM's inherent uncertainty and task complexity.",
    "Context": "LLM-based agents require robust and optimal plans, especially in scenarios with large search spaces or high stakes where a single-shot generation might be insufficient or unreliable.",
    "Solution": "Generate multiple diverse candidate plans (e.g., by sampling decoding strategies or explicit prompting) and then employ a task-related search algorithm (e.g., majority vote, tree search, A* algorithm) to evaluate and select the optimal plan among them.",
    "Result": "Provides a broader exploration of potential solutions, leading to more robust and optimal plan selection, and improved task success rates.",
    "Related Patterns": [
      "Task Decomposition",
      "External Planner-Aided Planning",
      "Reflection and Refinement",
      "Memory-Augmented Planning"
    ],
    "Uses": "Complex problem-solving, reasoning tasks, scenarios requiring high-quality plan reliability."
  },
  {
    "Pattern Name": "External Planner-Aided Planning",
    "Problem": "LLMs struggle with planning in environments with intricate constraints, ensuring plan feasibility, or achieving high planning efficiency.",
    "Context": "LLM-based agents operate in domains requiring strict adherence to rules, mathematical precision, or optimal resource utilization, where LLMs alone may hallucinate or generate invalid actions.",
    "Solution": "Integrate LLMs with specialized external planners. The LLM's role often involves formalizing tasks or providing high-level reasoning, while the external planner (either symbolic like PDDL/ASP solvers or neural like RL/IL models) handles the constrained plan generation or optimization.",
    "Result": "Improves plan feasibility, ensures adherence to complex constraints, enhances planning efficiency, and leverages the theoretical guarantees and interpretability of traditional planning systems.",
    "Related Patterns": [
      "Task Decomposition",
      "Multiplan Selection",
      "Reflection and Refinement",
      "Memory-Augmented Planning"
    ],
    "Uses": "Mathematical problem-solving, generating admissible actions, dynamic interactive environments, general tasks for symbolic AI, text-based games, complex interactive tasks."
  },
  {
    "Pattern Name": "Reflection and Refinement",
    "Problem": "LLM-generated plans can contain errors, lead to dead ends (thought loops), or suffer from hallucinations due to the LLM's limitations, insufficient reasoning abilities, or limited feedback.",
    "Context": "LLM-based agents need to improve their fault tolerance and error correction capabilities, especially in multi-step or interactive tasks.",
    "Solution": "Implement a feedback loop where the LLM reflects on its past actions, detected failures, or external feedback. Based on this reflection, it identifies errors and refines its current or subsequent plans iteratively. This process can be supported by evaluators or external tools for validation.",
    "Result": "Enhances the agent's ability to self-correct, improves plan quality and robustness, reduces factual errors, and increases overall task success rates by breaking out of suboptimal paths.",
    "Related Patterns": [
      "Task Decomposition",
      "Multiplan Selection",
      "External Planner-Aided Planning",
      "Memory-Augmented Planning"
    ],
    "Uses": "Complex planning, interactive recommendation systems, tasks requiring high reliability and adaptability."
  },
  {
    "Pattern Name": "Memory-Augmented Planning",
    "Problem": "LLMs have limited context windows, leading to 'forgetting' past experiences, or lacking access to crucial commonsense, domain-specific, or long-term knowledge necessary for effective planning.",
    "Context": "LLM-based agents need to leverage long-term, diverse knowledge and past experiences to inform their planning, enabling growth and handling knowledge-intensive tasks.",
    "Solution": "Equip the LLM agent with an external memory module or embed knowledge directly into its parameters. This memory stores valuable information (e.g., past experiences, facts, domain knowledge) which can be retrieved (RAG-based) or directly accessed/utilized (finetuning-based) to aid in plan generation.",
    "Result": "Extends the effective context of the LLM, provides access to up-to-date and domain-specific knowledge, enhances planning capabilities, and improves the agent's ability to learn and adapt over time.",
    "Related Patterns": [
      "Task Decomposition",
      "Multiplan Selection",
      "External Planner-Aided Planning",
      "Reflection and Refinement"
    ],
    "Uses": "Human-like agent simulation, interactive recommendation, text-based games, generalized agent abilities, tasks requiring extensive or dynamic knowledge."
  }
]