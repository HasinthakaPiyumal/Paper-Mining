Pattern Name,Problem,Context,Solution,Result,Related Patterns,Uses,Cluster Number,Category
LLM as a Knowledge Base Augmenter,"Traditional knowledge graphs (KGs) for recommender systems are often sparse, limited, and expensive to construct or complete, leading to ignored user preferences and reduced recommendation performance. They also lack cross-domain information.","Recommender systems that rely on knowledge graphs as side information to enhance semantic understanding, user-item relations, and explainability.","Leverage Large Language Models' (LLMs) ability to retrieve factual knowledge to construct more comprehensive knowledge graphs. LLMs are used for knowledge graph completion (filling missing facts) and knowledge graph construction (entity discovery, coreference resolution, relation extraction from text). They can also distill common sense facts and provide cross-domain information.","More extensive and up-to-date knowledge bases, enhanced recommendation accuracy, relevance, and personalization, and improved cross-domain recommendations.","['LLM as a Content Interpreter', 'LLM as a Tool Orchestrator']","Recommender systems, CTR prediction models.",8,
LLM as a Content Interpreter,"Content features in content-based recommendation systems can be sparse, and conventional methods (statistical models, basic neural networks) struggle to fully exploit the potential of textual content or capture deep semantic representations and extensive world knowledge effectively.","Content-based recommender systems or any system requiring deep processing and understanding of textual features (e.g., news articles, reviews, item descriptions) to match user preferences.","Employ (pretrained) large language models (e.g., BERT, GPT, T5) as advanced content interpreters. This involves fine-tuning LLMs with task-specific pretraining or instruction tuning to align them with recommendation objectives, or directly leveraging their emergent abilities in reasoning and generalization.","Enhanced understanding and interpretation of textual content, improved recommendations, alleviation of cold-start problems, facilitation of cross-domain recommendations, and better reasoning of user personalized intent and interest.","['LLM as a Knowledge Base Augmenter', 'LLM for In-Context Learning Recommendations']","Content-based recommenders, news recommendation, tag recommendation, tweet representation, code example recommendation, zero-shot/few-shot recommendation, sequential recommendation, rating prediction.",8,
LLM as an Explainer,"Traditional recommender systems often act as 'black boxes,' diminishing user trust due to inscrutable internal workings. Conventional template-based or natural language generation methods for explanations lack adaptability, personalization, diversity, and coherence, and are often tightly coupled to specific recommendation models.","Recommender systems where user trust, understanding, and acceptance of recommendations are crucial.","Leverage LLMs' remarkable generative ability in language tasks and in-context learning capabilities (zero-shot, few-shot, Chain-of-Thought prompting) to craft customized, precise, natural, and adaptable explanations for recommendations. This approach is model-agnostic and can incorporate real-time user feedback to foster human-machine alignment.","Improved model transparency, persuasiveness, and reliability; enhanced user trust and satisfaction; adaptable and personalized explanations; and a versatile, scalable interpretational framework.","['Chain-of-Thought Prompting', 'LLM as a Conversational Recommender Agent']","Explainable recommendation, interpreting deep learning models.",8,
LLM for In-Context Learning Recommendations,"Adapting LLMs for specific recommendation tasks typically requires extensive fine-tuning, which is computationally expensive and data-intensive. There is a need for quick adaptation to new cases or domains, especially in cold-start scenarios.","Recommender systems, particularly in scenarios requiring rapid deployment or handling cold-start problems, where limited or no explicit training data is available for a new task or domain.",Utilize LLMs' in-context learning ability by providing natural language instructions and a few input-output demonstrations (few-shot learning) or just instructions (zero-shot learning) directly in the prompt. LLMs then generate recommendations without explicit fine-tuning.,"Ability to make recommendations (rating, ranking) on new cases without explicit tuning, reduced data dependency, and potential for addressing cold-start problems.","['Chain-of-Thought Prompting', 'LLM as a Content Interpreter']","Rating prediction, ranking prediction, sequential recommendation, direct recommendation, explanation generation, review summarization (in a recommendation context).",8,
Chain-of-Thought Prompting,"Large Language Models (LLMs) may struggle with complex reasoning tasks, leading to incorrect or less accurate conclusions, and their decision processes remain opaque.","Tasks requiring multi-step reasoning, problem-solving, or complex decision-making within LLM-based systems, such as mathematical word problems or complex recommendation reranking.",Guide the LLM to break down complex tasks into sub-problems and explicitly generate intermediate reasoning steps (a 'chain of thought') before arriving at a final answer. This is achieved through specific prompting strategies like 'let us think about it step by step'.,"Improved reasoning abilities, enhanced accuracy for complex tasks (e.g., mathematical word problems, reranking items in recommendations), and better interpretability of the LLM's decision process.","['LLM as an Explainer', 'LLM for In-Context Learning Recommendations', 'LLM as a Tool Orchestrator']","Complex reasoning tasks, mathematical word problems, reranking items in recommendation systems, enhancing tool-using capabilities.",9,
LLM for Automated ML (AutoML) Search,"Automated Machine Learning (AutoML), including Neural Architecture Search (NAS) and feature selection, is computationally expensive and requires iterative trials or complex differentiable searching. Identifying optimal configurations in complex search spaces (e.g., recommender systems with diverse feature interactions) is challenging.","Designing and optimizing ML models (e.g., recommender systems) where manual configuration and hyperparameter tuning are costly and time-consuming.","Leverage LLMs' generative, memorization, and reasoning capabilities to assist or automate the AutoML process. This includes generating network architectures, suggesting better-performing configurations based on previous trials, or acting as mutation and crossover operators in genetic algorithms for search. LLMs can help reduce the search space or guide the search process.","Reduced search space, generation of reasonable or better-performing architectures, and improved efficiency and interpretability when integrated into existing search algorithms (e.g., genetic algorithms).",['LLM as a Tool Orchestrator'],"Neural Architecture Search (NAS), feature selection, optimizing recommender system architectures.",35,
LLM as a Conversational Recommender Agent,"Traditional recommender systems are often passive and struggle to adapt in real-time or understand nuanced user intents expressed in natural language. Creating engaging, adaptive, and personalized conversational interfaces for recommendations is challenging, especially in handling user memory and domain-specific knowledge gaps in long dialogues.","Building conversational recommender systems (CRS) that interact with users via natural language to uncover preferences and provide real-time, adaptive recommendations.","Employ LLMs as the core dialogue module of a CRS. Leverage their inherent conversational abilities for understanding user intent, generating natural language responses, and providing recommendations. Address domain-specific knowledge gaps by fine-tuning with private data or by integrating 'tool learning' (treating traditional recommendation models as external tools). For long conversations, employ memory modules or user profile extraction techniques to retain context and personal information.","Enhanced user engagement, real-time understanding of user intents, adaptive recommendation strategies, more natural and seamless communication, personalized assistance, and the ability to provide recommendations in open domains.","['LLM as a Tool Orchestrator', 'LLM as an Explainer', 'LLM as a Personalized Content Creator (AIGC for Personalization)']","Conversational recommender systems, personalized assistance, chatbots for product recommendations.",8,
LLM as a Tool Orchestrator,"Large Language Models (LLMs), despite their power, have inherent limitations: restricted memory for specific private/specialized domain knowledge, temporal generalization issues (external knowledge evolves rapidly), potential for hallucinations, and inability to perform precise external computations or access real-world information. Complex, long-horizon tasks for personalization systems are difficult for LLMs alone.","LLM-based personalization systems or general AI systems needing to extend capabilities beyond the LLM's internal knowledge, involving multi-step tasks, external data access, or specialized computations.","Design LLMs to act as intelligent controllers or orchestrators. They interpret user requests, break down complex tasks into subtasks, plan sequences of actions, select and invoke external specialized tools (e.g., search engines, recommendation engines, calculators, service APIs, databases, other AI/ML models, physics engines) to execute subtasks, and then integrate the tools' outputs to complete end-to-end tasks. This often involves advanced prompt engineering (e.g., Chain-of-Thought, ReAct) or fine-tuning the LLM for better tool-use strategies. They can also maintain user memory for personalized planning.","Enhanced task-solving capabilities for complex, long-horizon tasks; access to up-to-date, domain-specific, and factual knowledge; reduced memory burden and mitigation of hallucinations; more accurate, efficient, and personalized solutions; and active user engagement and service provision.","['Chain-of-Thought Prompting', 'LLM as a Conversational Recommender Agent', 'LLM for Automated ML (AutoML) Search']","End-to-end personalization tasks, conversational recommender systems, automated ML (e.g., NAS), long story generation, knowledge-intensive language tasks, dialog, question answering, mathematical/symbolic/algorithmic reasoning, robotic tasks, image classification/captioning/object detection, web browsing, search, translation, calendar, personalized recommendation (retrieval/reranking, cold-start for new items, user profiles).",35,
LLM as a Personalized Content Creator (AIGC for Personalization),"Traditional content generation for personalization (e.g., online advertising text, product descriptions) often relies on predefined templates or data-driven methods that may not fully capture nuanced user preferences or offer sufficient customization and realism. Sparse feedback also limits the learning process for content generation models.","Personalization systems requiring dynamic, customized content creation to match individual user interests and preferences (e.g., online advertising, e-commerce, customer service chatbots).","Employ Large Language Models (and other Generative AI models, AIGC) to generate customized, appealing digital content (e.g., text, images, music, multi-modal content) based on user personalized intent and interest extracted from instructions. Techniques like Reinforcement Learning from Human Feedback (RLHF) can be applied to fine-tune models to better capture user intent and address the problem of extremely sparse feedback.","More appealing and customized content, better reasoning of user intent, realistic and high-quality content creation, ability to understand explicit user preferences, enhanced user experiences, and accelerated business growth.","['LLM for In-Context Learning Recommendations', 'LLM as a Conversational Recommender Agent']","Online advertising (ad titles, descriptions), e-commerce (product descriptions, chatbots), customer service (automated responses, personalized assistance), general content creation (images, music, text).",6,
Agent-Computer Interface (ACI),"Language Model (LM) agents struggle to reliably interact with general-purpose computer environments (like Linux shell) for complex tasks due to interfaces designed for human users, leading to inefficient actions, poor feedback interpretation, error propagation, and limited performance.","LM agents tasked with performing complex, multi-step operations in digital environments, such as software engineering tasks (e.g., creating, editing, navigating code, executing tests).","Introduce an abstraction layer, the Agent-Computer Interface (ACI), between the LM agent and the computer. This interface is tailored to the LM's needs and abilities, providing a curated set of commands, structured feedback, and built-in guardrails. Key principles include: simple and easy-to-understand actions, compact and efficient actions, informative but concise environment feedback, and mechanisms to manage context and mitigate errors.","Substantially enhances the LM agent's ability to autonomously use computers, leading to significantly improved performance on complex tasks (e.g., state-of-the-art on SWEbench), increased reliability, and more efficient interaction cycles compared to using human-designed interfaces or non-interactive systems.","['LM-Friendly Search and Navigation', 'LM-Centric File Editing with Guardrails', 'Context Management for Agents']","Automated Software Engineering, Digital Environment Control for LM Agents, Interactive Code Generation, Agentic AI System Design.",26,
LM-Friendly Search and Navigation,"General-purpose search and navigation tools (e.g., `grep`, `find`, `cd`, `ls`, `cat`) are often too verbose, highly configurable, or require inefficient multi-step compositions for LM agents. This can lead to context window overflow, 'lost in the middle' issues, and inefficient exhaustive search behaviors.","LM agents needing to efficiently locate relevant files, specific code snippets, or definitions within large codebases to understand issues or plan edits.","Provide a set of specialized search and navigation commands (`findfile`, `searchfile`, `searchdir`, `open`, `goto`, `scrolldown`, `scrollup`) that are tailored for LM agents. These commands offer: summarized search results (e.g., max 50 results) with guidance for refining queries, efficient in-file navigation (e.g., jumping to specific lines, scrolling within a limited window), and contextual information (e.g., line numbers, lines omitted) in a simplified, consistent format.","Improves the agent's ability to perform efficient code localization, reduces the burden on the LM's context window, and enables faster progress towards task goals by providing targeted information.","['Agent-Computer Interface (ACI)', 'LM-Centric File Editing with Guardrails', 'Context Management for Agents']","Bug localization, code exploration, understanding codebase structure, automated refactoring.",44,
LM-Centric File Editing with Guardrails,"LM agents struggle with direct file modification using traditional tools (e.g., `sed`, redirection) due to their granular, error-prone nature, lack of immediate feedback, and the difficulty of performing multi-line, context-aware edits in a single step. Self-introduced errors (e.g., syntax errors) are common, and recovery is challenging.","LM agents requiring reliable, efficient, and robust methods to modify code files as part of software engineering tasks (e.g., applying bug fixes, adding features, writing reproduction scripts).","Implement a specialized `edit` command, integrated with a file viewer, that: allows replacing a specific range of lines with new content in a single, compact action; automatically displays the updated file content immediately after an edit; and incorporates guardrails like a code linter or syntax checker. Invalid edits are discarded, and the agent receives specific error messages (including before/after snippets) to guide recovery.","Streamlines the code modification process, significantly reduces the occurrence and propagation of errors, improves the agent's ability to recover from mistakes, and enhances overall performance on code-editing tasks.","['Agent-Computer Interface (ACI)', 'LM-Friendly Search and Navigation', 'Context Management for Agents']","Automated program repair, interactive code generation, refactoring, writing test scripts.",44,
Context Management for Agents,"Large Language Models have limited context windows and are sensitive to the quantity and relevance of input. In multi-turn agentic interactions, a growing history can lead to 'lost in the middle' performance degradation, increased computational cost, and the presence of outdated or irrelevant information.","LM agents engaging in long-running, iterative interactions with an environment, where each turn involves generating thoughts and actions based on current instructions and accumulated history.","Implement intelligent context management strategies to optimize the information presented to the LM at each turn. This includes: history processing (selectively collapsing or summarizing older observations), informative error messages (specific and concise feedback for malformed generations or silent commands), error denoising (removing malformed generation attempts from history), and structured prompts (system, demonstration, and instance templates to convey task settings and tips).","Keeps the agent's input concise and relevant, reduces token consumption and inference cost, mitigates the 'lost in the middle' effect, and improves the agent's ability to maintain focus, learn from feedback, and successfully complete long-horizon tasks.","['Agent-Computer Interface (ACI)', 'LM-Friendly Search and Navigation', 'LM-Centric File Editing with Guardrails']","Long-horizon agentic tasks, interactive environments, reducing LLM inference cost, improving prompt effectiveness, Agentic AI System Design.",26,
LLM-Powered Symbolic World Model Engineering,"Large Language Models (LLMs) often generate incorrect or implausible plans when used directly for complex planning tasks due to limitations in reasoning, handling long-term dependencies, and combinatorial search. Simultaneously, acquiring robust, symbolic world models (e.g., in PDDL) is traditionally labor-intensive and requires specialized knowledge, making it inaccessible to non-experts. Initial LLM-generated symbolic models may also contain errors (syntax or factual) that are difficult for non-PDDL experts to correct directly.","Developing AI agents for sequential decision-making in domains like robotics or household automation, where plan correctness and reliability are paramount. Users (e.g., robot engineers) possess natural language descriptions of actions and domain constraints but may lack expertise in formal planning languages. The goal is to leverage LLMs' common-world knowledge and natural language understanding to build reliable symbolic models, without relying on their unreliable planning capabilities.","The process involves two main phases: 
1.  **Acquisition:** Prompt an LLM (e.g., GPT-4) with detailed instructions, illustrative examples, domain context, natural language descriptions of actions, and a dynamically updated list of predicates. The LLM then generates symbolic representations of actions (e.g., PDDL action models) including their preconditions and effects. This process can be iterative (e.g., generating models action-by-action and then re-running with a full predicate list) to refine the model and leverage LLMs for knowledge acquisition from minimal descriptions.
2.  **Correction:** Employ the LLM as an intelligent interface between the generated symbolic model and various feedback sources (human experts, PDDL validators). The LLM translates PDDL models and validator error messages into natural language for user comprehension. It then incorporates natural language corrective feedback (e.g., missing effects, syntax errors) back into the PDDL model by continuing the dialogue, effectively concealing the complexity of PDDL from end-users.","Produces high-quality, correct, and intuitively named symbolic world models (e.g., PDDL domain models) by effectively separating the modeling of causal dependencies (LLM's strength) from the combinatorial search (external planner's strength). This significantly reduces the human involvement and specialized expertise required for symbolic model acquisition and correction, making the process more efficient and accessible. The generated models can then be seamlessly used by sound, domain-independent classical planners, offering correctness guarantees.","['LLM as Goal Translator', 'Classical Planner with LLM-Acquired World Model', 'LLM Planner with Symbolic Validation and Feedback']","Robotics (e.g., household robots), general AI agent development requiring robust planning capabilities, knowledge engineering for symbolic AI systems, enabling non-experts to define agent behaviors and causal models.",47,
LLM as Goal Translator,"Classical symbolic planners require precise, formal goal specifications (e.g., PDDL goals), but end-users typically provide high-level instructions in natural language. This disparity creates a barrier to accessibility and usability for symbolic planning systems.","An AI system that utilizes a classical symbolic planner (which demands formal goal input) but is designed to interact with users through natural language commands. A symbolic world model (e.g., PDDL) is already available, defining the predicates and objects relevant to the domain.","An LLM is employed to parse and interpret natural language user instructions and translate them into a formal, symbolic goal specification (e.g., a set of PDDL predicates to be achieved). The LLM leverages its natural language understanding capabilities and its knowledge of the defined predicates to accurately represent the user's intent in a machine-readable format suitable for the symbolic planner.","Bridges the semantic gap between human natural language and formal planning systems, thereby making classical planners more accessible and user-friendly. This allows users to specify complex tasks without needing to learn or understand a formal planning language.","['LLM-Powered Symbolic World Model Engineering', 'Classical Planner with LLM-Acquired World Model']","Natural language interfaces for robotic control, intelligent assistants, task automation systems, and any application where symbolic planning is used, and user interaction is primarily in natural language.",47,
Classical Planner with LLM-Acquired World Model,"Large Language Models (LLMs) struggle with the combinatorial search and correctness guarantees essential for reliable planning in complex, real-world scenarios. They often produce plans that are logically inconsistent, physically impossible, or fail to achieve goals, making them unsuitable for critical applications.","An AI agent needs to execute plans for complex, long-horizon tasks in a structured environment where reliability is paramount. A robust symbolic world model (e.g., PDDL domain model) has been successfully acquired and refined using LLM-Powered Symbolic World Model Engineering, and user goals are translated into formal specifications by an LLM (LLM as Goal Translator).","A standard, robust, and domain-independent classical symbolic planner (e.g., Fast Downward, STRIPS planner) is integrated with the LLM-generated and corrected symbolic world model and the LLM-translated formal goal specifications. The classical planner is exclusively responsible for performing the search through the state space to find a sequence of actions that achieves the goal, leveraging its proven algorithms for optimality or satisficing.","Generates highly reliable, correct, and executable plans with strong correctness guarantees, effectively overcoming the limitations of direct LLM planning regarding combinatorial search and logical consistency. This pattern successfully combines the natural language understanding and knowledge acquisition strengths of LLMs with the robust, provable planning capabilities of classical AI.","['LLM-Powered Symbolic World Model Engineering', 'LLM as Goal Translator']","Robotics, automated task execution, complex system control, and any application where plan correctness, logical consistency, and provable execution are critical requirements.",47,
LLM Planner with Symbolic Validation and Feedback,"While LLM planners can sometimes capture implicit human preferences or ordering constraints not easily formalized, they frequently generate incorrect, incomplete, or non-executable plans due to a lack of deep causal reasoning and logical consistency. Manually correcting these plans is resource-intensive and impractical for complex tasks.","An AI system employs an LLM directly as a planner (e.g., using a ReAct-like approach) to leverage its flexibility and ability to incorporate commonsense or implicit user constraints. However, the resulting plans require rigorous validation to ensure their correctness and executability. A reliable symbolic world model (e.g., PDDL) is available, typically acquired through LLM-Powered Symbolic World Model Engineering.","The LLM-acquired symbolic world model (e.g., PDDL) is utilized as an inexpensive, high-level 'symbolic simulator' or 'human proxy' to validate plans proposed by the LLM planner. A PDDL validator tool (e.g., VAL) checks the LLM-generated plan against the symbolic model for unmet preconditions or goal conditions. Any validation feedback (e.g., error messages, unmet conditions) is then translated by an LLM into natural language and provided back to the LLM planner through a reprompting mechanism. The LLM planner iteratively refines its plan based on this corrective feedback.","Significantly improves the correctness and executability of LLM-generated plans by introducing a robust, automated feedback loop. This approach allows the LLM planner to retain its capacity for incorporating implicit preferences and commonsense knowledge while benefiting from the logical consistency checks and correctness guarantees of symbolic AI. It reduces reliance on costly physical simulators or extensive human oversight during plan execution.",['LLM-Powered Symbolic World Model Engineering'],"Agentic AI systems, interactive planning where LLMs generate initial plans, scenarios where implicit user preferences are valuable but plan correctness must be ensured, and enhancing the reliability of LLM-driven agents in dynamic environments.",47,
LLM as a Planner,Complex user instructions or long-horizon tasks for AI systems are difficult to execute directly without a structured approach.,Foundation models (LLMs) serving as the controller in agentic systems that need to interact with tools or environments.,"Leverage the LLM's inherent reasoning and decision-making capabilities to decompose complex tasks into multiple subtasks and sequence them logically, forming an executable plan.","Enables AI systems to tackle complex problems by breaking them down into manageable steps, leading to more coherent and effective task execution.","['Introspective Reasoning', 'Extrospective Reasoning', 'Multi-Agent Collaboration', 'Parallel Tool Execution']","Robotics, multi-step Question Answering (QA), embodied learning, autonomous agents, general tool-learning systems.",0,
Feedback Integration,AI systems need to adapt their behavior and plans based on the outcomes of their actions and external input from users or the environment.,Interactive AI systems and agentic frameworks where actions produce observable effects and user input is dynamic.,"Implement a 'perceiver' component that processes feedback from the user and the environment (e.g., execution results, state changes, human preferences) and summarizes it for the controller (foundation model). This feedback then informs subsequent decision-making and plan adjustments.","Enables adaptive planning, error correction, and iterative refinement of actions, making the AI system more robust and responsive to dynamic conditions.","['Extrospective Reasoning', 'Reinforcement Learning from Human Feedback (RLHF) for Tool Learning', 'Proactive AI Agent']","Embodied agents, interactive Question Answering (QA), tool execution monitoring, conversational AI.",20,
Tool Selection,"Given a user's intent and a diverse set of available tools with specific functionalities, the AI system must accurately choose the most appropriate tool(s) for a given subtask.","Foundation models interacting with a collection of specialized tools (APIs, software, etc.) in a tool-learning framework.","The controller (foundation model) infers the user's underlying intent and comprehends the functionalities of available tools. It then develops a plan to select the most suitable tool(s) for tackling each subtask. For large tool sets, an intermediate retrieval stage can pre-select a relevant subset of tools.","Ensures efficient and effective utilization of specialized tools, bridging the gap between user intent and tool capabilities.","['Prompt-based Tool Understanding', 'LLM as a Planner']","General tool-learning systems, multi-tool scenarios, API invocation.",19,
Prompt-based Tool Understanding,"Foundation models need to quickly comprehend the functionalities, usage, and parameters of various tools without requiring extensive re-training or large labeled datasets.","Foundation models with strong few-shot and zero-shot learning capabilities interacting with diverse tools (e.g., APIs).","Construct suitable task-specific prompts (either manually designed or retrieved) that describe API functionalities, their input/output formats, and possible parameters (zero-shot prompting), or provide concrete tool-use demonstrations (few-shot prompting).",Enables models to effectively unravel tool functionalities and comprehend how to use them proficiently with minimal human effort and high adaptability to tool changes.,"['Self-supervised Tool Learning', 'Tool Selection']","Integrating new APIs, adapting to modified tools, teaching models about tool capabilities.",49,
Chain of Thought (CoT) Prompting,"Foundation models often struggle with complex reasoning tasks, even when provided with few-shot examples, leading to suboptimal performance.",Large language models (LLMs) used for problem-solving that require multi-step reasoning.,Augment few-shot prompts by additionally inserting the reasoning trace (the intermediate steps required to derive the final answer) for each example. This encourages the model to generate its own explicit thoughts and intermediate steps before arriving at the final answer.,Significantly boosts performance on a wide range of complex reasoning tasks by eliciting more structured and transparent reasoning processes.,"['Introspective Reasoning', 'Extrospective Reasoning']","Arithmetic reasoning, commonsense reasoning, symbolic reasoning, complex problem-solving in LLMs.",9,
Introspective Reasoning,"Generating multi-step plans for complex tasks where immediate environmental feedback is either unavailable or costly during the initial planning phase, potentially leading to unrealistic plans.","Foundation models acting as controllers in tool-learning frameworks, often in simulated or abstract environments.","The foundation model directly generates a static, multi-step plan for tool use based on its internal knowledge and reasoning capabilities. This can involve generating executable programs or anticipating possible anomalies in the plan execution without direct interaction with the environment.","Enables upfront planning for complex tasks, suitable for scenarios where environmental interaction is delayed or for generating initial, high-level strategies. Helps in creating executable programs for agents.","['LLM as a Planner', 'Chain of Thought (CoT) Prompting']","Program-Aided Language Models (PAL), embodied agents (ProgPrompt, Code-as-Policies), Visual ChatGPT, planning in physically grounded agents (SayCan).",0,
Extrospective Reasoning,"Static plans generated by introspective reasoning cannot adapt to intermediate execution results or unexpected situations in dynamic environments, leading to failures.",Interactive environments where an AI agent needs to generate plans incrementally and continuously adapt to real-time feedback from the user and environment.,"The foundation model generates plans incrementally, typically one step at a time, with subsequent plans dynamically dependent on previous execution results and feedback. This establishes a closed-loop interaction among the controller, perceiver, environment, and user.","More rational, robust, and adaptable planning, better suited for complex and dynamic tasks by enabling real-time adjustment to anomalies and intermediate outcomes.","['Feedback Integration', 'LLM as a Planner', 'Chain of Thought (CoT) Prompting', 'Conflict Resolution for Augmented Knowledge']","Multi-step Question Answering (Self-Ask, ReAct, ToolFormer), embodied learning (Inner Monologue, LLMPlanner), autonomous agents, interactive problem-solving.",0,
Formalism-Enhanced Reasoning,"Relying solely on plain natural language for reasoning and planning can limit the performance of LLM-based agents in complex, logically demanding tasks.","LLM-based agents operating in domains requiring high precision, structured knowledge, or complex procedural execution.","Incorporate external formalisms, such as mathematical tools, probabilistic graph models (PGMs), or integrate with structured automation frameworks like Robotic Process Automation (RPA), to represent and process information beyond natural text. This enhances the agent's reasoning capabilities.","Significantly enhances agents' performance in complex reasoning tasks, improves decision-making capabilities, and maintains controllability by leveraging structured representations and external computational power.","['Extrospective Reasoning', 'Conflict Resolution for Augmented Knowledge']","Multi-agent reasoning, agentic process automation (APA), scientific discovery, tasks requiring symbolic manipulation.",47,
Parallel Tool Execution,"Executing all subtasks sequentially can be inefficient, especially when certain subtasks are independent and do not rely on the output of others.","Multi-step, multi-tool scenarios where a complex task can be decomposed into several independent subtasks.","The AI system analyzes task dependencies to identify subtasks that can be executed concurrently. These independent subtasks are then assigned for simultaneous execution, potentially by different agents or parallel processes.",Improves overall execution efficiency and reduces the total time required to complete complex tasks by leveraging parallelism.,"['LLM as a Planner', 'Multi-Agent Collaboration']","Complex task decomposition, multi-agent systems, code generation for independent components.",42,
Multi-Agent Collaboration,"Complex tasks often demand a wider range of abilities and expertise than a single AI agent can possess, or require simulating human-like social interactions.","Scenarios where a complex problem benefits from diverse perspectives, specialized skills, or distributed problem-solving.","Design a system where multiple AI agents, each potentially possessing unique abilities or specialized tools, collaborate to solve a task. This requires implementing mechanisms for communication, coordination, and negotiation among agents.","Unlocks more effective and efficient problem-solving approaches for complex tasks, and can simulate realistic human behaviors in interactive scenarios.","['LLM as a Planner', 'Parallel Tool Execution']","Interactive scenarios, complex task solving, simulating human behavior, distributed control systems.",42,
Self-supervised Tool Learning,"The traditional reliance on extensive human-annotated tool-use demonstrations for training models is time-consuming, labor-intensive, and limits scalability.","Foundation models with strong in-context learning abilities, aiming to learn tool use with minimal human supervision.","Leverage the foundation model's in-context learning capabilities to iteratively bootstrap tool-use examples based on a small handful of human-written examples. These autogenerated examples are then filtered to reduce noise, creating a large, self-supervised dataset.","Significantly reduces the dependency on extensive human annotation, improving tool-use performance and enabling models to enhance their capabilities in a scalable and efficient manner.",['Prompt-based Tool Understanding'],"Bootstrapping tool-use datasets, enhancing tool-use capabilities with minimal supervision, few-shot tool integration.",49,
Reinforcement Learning from Human Feedback (RLHF) for Tool Learning,"Aligning AI agent behavior with nuanced human preferences and values in tool-use scenarios is challenging, especially when explicit reward signals are sparse or difficult to define programmatically.","Training AI agents to perform tool-oriented tasks, where the goal is to achieve human-aligned outcomes and improve user experience.","Utilize human feedback (explicit ratings or implicit behaviors) to train a reward model that imitates human preferences. This reward model then guides reinforcement learning algorithms (e.g., PPO) to optimize the agent's policy for tool selection and action, aligning its behavior with desired human expectations.","Improves tool-use capabilities, ensures agent behavior aligns with human preferences, and helps manipulate tools to achieve desired long-form or subjective outcomes.","['Feedback Integration', 'Extrospective Reasoning']","Web search (WebGPT), conversational agents, general tool learning where human alignment and preferences are critical.",20,
Unified Tool Interface,"Models struggle to transfer learned tool-use knowledge and skills to new tools due to varying interfaces, communication protocols, and action spaces across different tools.","AI systems interacting with a diverse and rapidly expanding array of tools (APIs, GUIs, physical devices).","Design and implement a standardized interface for tool interaction. This can be a semantic interface (natural language action triggers), a GUI interface (mapping predicted tokens to human-like mouse/keyboard actions), or a programming interface (code-based function calls with standardized syntax).","Facilitates knowledge transfer and generalization among tools, enabling models to identify and abstract essential features more easily, and allows for quicker adaptation to new scenarios and tools.","['Meta Tool Learning', 'Curriculum Tool Learning']","Robotics, web-based agents, code generation, general tool-learning systems.",19,
Meta Tool Learning,"AI models need to generalize tool-use knowledge to new tasks or domains efficiently, beyond merely memorizing specific tool applications, by understanding underlying principles.",Developing adaptable and intelligent ML models that can effectively use new or unfamiliar tools.,"Train the model not just to use a specific tool, but to learn the optimal strategy or common underlying principles/patterns in tool-use. This allows it to transfer these high-level strategies to new tasks or similar tools in different domains.","Enables models to identify commonalities in tool-use strategies and adapt their behaviors when faced with unfamiliar situations, significantly improving generalization and adaptability to novel tools and tasks.",['Unified Tool Interface'],"Transferring knowledge between similar tools (e.g., different search engines), applying a calculator for different types of mathematical problems, adapting to new software with similar functionalities.",19,
Curriculum Tool Learning,"Introducing models directly to complex tools or tasks from the outset can be overwhelming, hindering effective learning and generalization.",Training models to use complex tools or perform intricate multi-step tasks.,"Implement a pedagogical strategy that starts with simple tools or basic operations, gradually introducing the model to more complex tools and advanced concepts. This allows the model to build upon its prior knowledge and develop a deeper, more structured understanding of the tool's capabilities.","Ensures a manageable and effective learning process, enabling the model to identify similarities and differences between situations, adjust its approach accordingly, and generalize across different tools and tasks more effectively.","['Unified Tool Interface', 'LLM as a Planner']","Teaching models to use complex software (e.g., Mathematica from basic arithmetic to calculus), mastering progressively difficult tasks in robotics or simulation.",7,
AI-Driven Tool Creation/Encapsulation,"Existing tools are often designed for human use and may not be optimal for AI's information processing, or AI needs to adapt/extend tools for new, specialized purposes.",Advancing AI capabilities from merely tool users to autonomous tool makers and enhancers.,"Leverage large code models or foundation models to autonomously generate executable programs (new tools) based on natural language descriptions, or to encapsulate/extend existing APIs into more advanced, specialized functions tailored for specific tasks or AI-centric workflows.","Creates tools that are better suited for AI's information processing, enables autonomous development of sophisticated solutions, and extends tool functionalities beyond their original design, promoting AI creativity and self-improvement.",['Unified Tool Interface'],"Generating code for specific tasks, extending weather forecast APIs to compute average temperature, integrating stock market data for investment recommendations, creating automated medical diagnosis systems.",19,
Personalized Tool Learning,"Foundation models, typically trained on generic data, struggle to process personal information and provide tailored assistance, adapting tool manipulation to individual user needs and preferences.","AI systems interacting with diverse users, requiring customized responses and tool usage based on individual profiles.","Develop methods to model heterogeneous user information (e.g., language style, historical interactions, social network data) into a unified semantic space. This information is then used to develop personalized tool execution plans and adapt tool calls based on specific user preferences, often leveraging user feedback.","Provides more personalized and effective tool assistance, aligning tool manipulation with individual user preferences, leading to a better user experience.","['Feedback Integration', 'Proactive AI Agent']","Personalized email tools, online shopping platforms, user-specific dialogue generation, tailored AI assistants.",19,
Proactive AI Agent,"Most foundation models operate as reactive systems, only responding to direct user queries, which limits their ability to anticipate user needs or initiate helpful actions autonomously.","AI systems aiming for a more seamless, natural, and personalized user experience by anticipating needs and acting on behalf of the user.","Design AI agents that can initiate actions proactively, without explicit user prompts, by leveraging the history of user interactions and understanding potential future needs. This involves a paradigm shift from purely reactive to anticipatory behavior.","Offers a more personalized and seamless user experience, but necessitates careful design with safety mechanisms and ethical considerations to prevent unintended consequences.","['Personalized Tool Learning', 'Feedback Integration']","Intelligent assistants, automated task completion, personalized recommendations, smart home systems.",20,
Conflict Resolution for Augmented Knowledge,"When augmenting foundation models with external tools, knowledge conflicts inevitably arise between the model's internalized knowledge and the real-time augmented knowledge, or among different augmented knowledge sources (e.g., from multiple tools).","Foundation models enhanced by various external knowledge sources (e.g., real-time APIs, curated databases, web search).","Develop mechanisms for conflict detection (identifying discrepancies between knowledge sources) and conflict resolution (verifying the reliability of sources, choosing the most trustworthy information, and providing explanations for the decisions made). Models should be guided to distinguish and verify reliability.","Improves the accuracy, reliability, and explainability of model generation and planning, especially critical in domains requiring high factual correctness and trustworthiness.","['Extrospective Reasoning', 'Formalism-Enhanced Reasoning']","Medical assistance, legal advice, financial transactions, fact-checking, open-domain Question Answering (QA).",18,
Task Decomposition,"Complex, multi-step tasks are difficult for LLM-based agents to plan directly or address through a one-step planning process.","LLM-based agents need to accomplish intricate, long-horizon tasks in real-world environments.",Decompose a complex task into a sequence of simpler subtasks. This can be done either 'decomposition-first' (all subtasks defined upfront) or 'interleaved' (subtasks are dynamically generated and planned one by one with environmental feedback).,"Simplifies complex planning, reduces the cognitive load on the LLM, and enhances the agent's ability to solve challenging tasks.","['Multiplan Selection', 'External Planner-Aided Planning', 'Reflection and Refinement', 'Memory-Augmented Planning']","Multimodal tasks (e.g., image generation, object recognition), mathematical reasoning, commonsense reasoning, symbolic reasoning, robotics, vision-and-language navigation.",0,
Multiplan Selection,A single plan generated by an LLM for a complex task is often suboptimal or infeasible due to the LLM's inherent uncertainty and task complexity.,"LLM-based agents require robust and optimal plans, especially in scenarios with large search spaces or high stakes where a single-shot generation might be insufficient or unreliable.","Generate multiple diverse candidate plans (e.g., by sampling decoding strategies or explicit prompting) and then employ a task-related search algorithm (e.g., majority vote, tree search, A* algorithm) to evaluate and select the optimal plan among them.","Provides a broader exploration of potential solutions, leading to more robust and optimal plan selection, and improved task success rates.","['Task Decomposition', 'External Planner-Aided Planning', 'Reflection and Refinement', 'Memory-Augmented Planning']","Complex problem-solving, reasoning tasks, scenarios requiring high-quality plan reliability.",0,
External Planner-Aided Planning,"LLMs struggle with planning in environments with intricate constraints, ensuring plan feasibility, or achieving high planning efficiency.","LLM-based agents operate in domains requiring strict adherence to rules, mathematical precision, or optimal resource utilization, where LLMs alone may hallucinate or generate invalid actions.","Integrate LLMs with specialized external planners. The LLM's role often involves formalizing tasks or providing high-level reasoning, while the external planner (either symbolic like PDDL/ASP solvers or neural like RL/IL models) handles the constrained plan generation or optimization.","Improves plan feasibility, ensures adherence to complex constraints, enhances planning efficiency, and leverages the theoretical guarantees and interpretability of traditional planning systems.","['Task Decomposition', 'Multiplan Selection', 'Reflection and Refinement', 'Memory-Augmented Planning']","Mathematical problem-solving, generating admissible actions, dynamic interactive environments, general tasks for symbolic AI, text-based games, complex interactive tasks.",0,
Reflection and Refinement,"LLM-generated plans can contain errors, lead to dead ends (thought loops), or suffer from hallucinations due to the LLM's limitations, insufficient reasoning abilities, or limited feedback.","LLM-based agents need to improve their fault tolerance and error correction capabilities, especially in multi-step or interactive tasks.","Implement a feedback loop where the LLM reflects on its past actions, detected failures, or external feedback. Based on this reflection, it identifies errors and refines its current or subsequent plans iteratively. This process can be supported by evaluators or external tools for validation.","Enhances the agent's ability to self-correct, improves plan quality and robustness, reduces factual errors, and increases overall task success rates by breaking out of suboptimal paths.","['Task Decomposition', 'Multiplan Selection', 'External Planner-Aided Planning', 'Memory-Augmented Planning']","Complex planning, interactive recommendation systems, tasks requiring high reliability and adaptability.",16,
Memory-Augmented Planning,"LLMs have limited context windows, leading to 'forgetting' past experiences, or lacking access to crucial commonsense, domain-specific, or long-term knowledge necessary for effective planning.","LLM-based agents need to leverage long-term, diverse knowledge and past experiences to inform their planning, enabling growth and handling knowledge-intensive tasks.","Equip the LLM agent with an external memory module or embed knowledge directly into its parameters. This memory stores valuable information (e.g., past experiences, facts, domain knowledge) which can be retrieved (RAG-based) or directly accessed/utilized (finetuning-based) to aid in plan generation.","Extends the effective context of the LLM, provides access to up-to-date and domain-specific knowledge, enhances planning capabilities, and improves the agent's ability to learn and adapt over time.","['Task Decomposition', 'Multiplan Selection', 'External Planner-Aided Planning', 'Reflection and Refinement']","Human-like agent simulation, interactive recommendation, text-based games, generalized agent abilities, tasks requiring extensive or dynamic knowledge.",0,
LLMPlanner (Embodied Agent Few-Shot Grounded Planning),"Building versatile, sample-efficient embodied agents that can follow natural language instructions for complex, long-horizon tasks in diverse, partially observable environments. Existing methods require large amounts of labeled data, and static plans lack environmental grounding, leading to failures.","Embodied agents (e.g., robots, virtual agents) operating in dynamic, visually-perceived environments, needing to execute multi-step instructions from natural language with minimal task-specific training data.","Implement a hierarchical planning framework where a Large Language Model (LLM) serves as a few-shot high-level planner, directly generating sequences of subgoals. This planning is continuously refined by a dynamic 'grounded replanning' algorithm that updates the plan based on real-time environmental observations (e.g., perceived objects) and execution failures, feeding this context back into the LLM prompt. The solution leverages in-context learning with dynamic example retrieval and logit biases for robust plan generation.","Dramatically reduces the need for human annotations and training data, enabling versatile and sample-efficient embodied agents that can dynamically adapt their plans to the current environment, overcome unforeseen challenges, and achieve competitive performance with significantly less data.","['Hierarchical Planning for Embodied Agents', 'LLM as a Planner', 'Grounded Replanning (with LLMs)', 'In-Context Learning for Agent Planning', 'Dynamic In-Context Example Retrieval', 'Logit Biases for LLM Output Constraint']","Embodied AI, Robotics, Vision-and-Language Navigation, Autonomous Agents, Task Automation, Few-Shot Learning for Robotics",32,"Agentic AI, LLM-specific, Planning, Knowledge & Reasoning, Prompt Design"
Hierarchical Planning for Embodied Agents,"Directly planning complex, long-horizon tasks for embodied agents from high-level natural language instructions to low-level primitive actions is computationally intractable, difficult to generalize, or requires extensive domain knowledge.","Embodied agents (e.g., robots, virtual agents) needing to perform multi-step, complex tasks in dynamic environments.","Decompose the overall planning process into two distinct layers: a High-Level Planner that interprets natural language instructions and generates a sequence of abstract subgoals (e.g., 'Navigate to object X', 'Pickup object Y'), and a Low-Level Planner that translates each subgoal into a sequence of primitive, executable actions within the current environment state. The high-level plan makes low-level planning conditionally independent of the initial natural language instruction.","Simplifies the complex planning problem, allows for modular development and different specialized models for high-level (e.g., LLMs, symbolic AI) and low-level (e.g., classical planners, trained policies) tasks, and improves scalability for long-horizon tasks.","['LLMPlanner (Embodied Agent Few-Shot Grounded Planning)', 'LLM as a Planner', 'Grounded Replanning (with LLMs)']","Vision-and-Language Navigation, Robotics, Autonomous Agents, Complex Task Execution",47,"Planning, Agentic AI, Classical AI"
LLM as a Planner,"Generating flexible, human-interpretable, and executable plans for agents from natural language instructions often requires extensive domain-specific rule engineering, large datasets of plan-action pairs, or pre-defined admissible action lists, which are difficult to obtain or manage in complex, partially observable environments.","Agentic systems where natural language instructions need to be translated into a sequence of executable steps (plans or subgoals), often in dynamic or open-ended environments.","Leverage a Large Language Model (LLM) to directly generate a sequence of high-level actions, subgoals, or a full plan in response to a natural language instruction. This is typically achieved through careful prompt engineering, allowing the LLM to use its vast pre-trained knowledge and in-context examples to infer plausible and executable plans.","Enables few-shot or zero-shot planning, reduces reliance on explicit domain models or large training datasets, and allows for more natural language interaction with the agent. The LLM's commonsense knowledge can produce contextually plausible plans.","['LLMPlanner (Embodied Agent Few-Shot Grounded Planning)', 'Hierarchical Planning for Embodied Agents', 'Grounded Replanning (with LLMs)', 'In-Context Learning for Agent Planning']","Embodied AI, Robotics, Conversational Agents, Task Automation, Agentic AI",32,"LLM-specific, Planning, Agentic AI"
Grounded Replanning (with LLMs),"Static plans generated by high-level planners (especially LLMs) may lack physical grounding, failing when confronted with the actual, dynamic, or partially observed environment (e.g., an object is not found, an obstacle blocks a path, or the environment state differs from initial assumptions). This leads to execution failures and an inability to complete tasks.",Embodied or autonomous agents executing pre-generated plans in real-world or simulated environments where unforeseen circumstances or discrepancies between the plan's assumptions and reality can occur.,"Implement a feedback loop where the agent continuously monitors its execution progress and the environment state. When an execution failure or significant deviation occurs (e.g., taking too long to reach a subgoal, failed action, or a critical object is not found), the high-level planner (an LLM) is dynamically re-prompted. This re-prompt includes the original instruction, the partially completed plan, and crucially, real-time observations from the environment (e.g., a list of currently visible objects). The LLM then generates a new, grounded continuation of the plan, adapting to the current reality.","Enables agents to dynamically adapt to environmental changes, recover from execution failures, resolve ambiguities (e.g., finding an object in an alternative location), and produce more robust and physically grounded plans, leading to higher task completion rates and a closed-loop interaction between the agent and its environment.","['LLMPlanner (Embodied Agent Few-Shot Grounded Planning)', 'LLM as a Planner', 'Hierarchical Planning for Embodied Agents', 'Logit Biases for LLM Output Constraint']","Embodied AI, Robotics, Autonomous Navigation, Adaptive Planning, Error Recovery in Agent Systems",16,"Agentic AI, LLM-specific, Planning, Knowledge & Reasoning"
In-Context Learning for Agent Planning,"Effectively leveraging the capabilities of Large Language Models (LLMs) for specific, structured tasks like agent planning without resource-intensive fine-tuning or requiring large task-specific datasets, especially in few-shot settings.","Utilizing pre-trained LLMs for downstream tasks, particularly in scenarios with limited labeled data (few-shot setting) where the desired output is a structured plan or sequence of actions for an agent.","Craft a prompt that explicitly guides the LLM to generate the desired plan. This prompt typically includes: 1) An explicit instruction explaining the task to the LLM, 2) A clear definition of allowed actions or the expected output format, and 3) A small set of high-quality, relevant input-output examples (e.g., natural language instruction-plan pairs) placed directly within the prompt, demonstrating the desired planning behavior.","Enables LLMs to perform few-shot planning, significantly reducing the need for task-specific training data and development time. This allows for rapid prototyping and adaptation of agent behaviors by simply modifying prompts and examples.","['LLMPlanner (Embodied Agent Few-Shot Grounded Planning)', 'Dynamic In-Context Example Retrieval', 'Logit Biases for LLM Output Constraint']","Few-shot learning, Prompt Engineering, Rapid Development of LLM-powered Agents, Task-Specific LLM Adaptation",32,"Prompt Design, LLM-specific, Classical AI"
Dynamic In-Context Example Retrieval,"The effectiveness of in-context learning with LLMs is highly dependent on the quality and relevance of the provided examples. A fixed set of examples may not be optimal for diverse inputs, leading to suboptimal or inconsistent performance across different tasks or instructions.","Implementing in-context learning with LLMs for various tasks, especially when dealing with a varied set of user instructions or tasks, and a larger pool of potential demonstration examples is available.","Instead of using a static set of in-context examples, dynamically select the most relevant examples for each specific test input. This typically involves: 1) Encoding the test input (e.g., the natural language instruction) into an embedding, 2) Comparing this embedding to pre-computed embeddings of a pool of available training examples, and 3) Retrieving the top-K most similar examples (e.g., using k-nearest neighbors based on cosine or Euclidean distance) to include in the LLM's prompt.","Improves the overall performance, robustness, and consistency of in-context learning by ensuring that the LLM receives demonstrations that are highly pertinent to the current task, leading to more accurate and appropriate outputs.","['LLMPlanner (Embodied Agent Few-Shot Grounded Planning)', 'In-Context Learning for Agent Planning']","Few-shot learning, Prompt Engineering, LLM-based systems, Contextual AI",32,"Prompt Design, LLM-specific, Classical AI"
Logit Biases for LLM Output Constraint,"When LLMs are used for structured tasks like agent planning, their unconstrained natural language generation can produce outputs that are syntactically incorrect, semantically irrelevant, or contain tokens (e.g., actions, objects) not valid in the current context or environment. This leads to unexecutable or nonsensical results.","Utilizing LLMs for tasks requiring specific output formats, adherence to a defined vocabulary (e.g., a set of allowed actions, recognized objects), or where certain tokens should be prioritized based on external, real-time information.","Apply 'logit biases' during the LLM's generation process. This involves programmatically adjusting the probability scores (logits) of specific tokens before sampling, increasing the likelihood of desired tokens (e.g., known high-level actions, currently observed objects) and decreasing the likelihood of undesired ones. This bias can be based on a dynamically compiled list of admissible tokens from the environment or a predefined action space.","Guides the LLM to produce outputs that are more aligned with the desired structure and available context, improving the executability and correctness of generated plans. It also aids in disambiguation (e.g., selecting 'TableLamp' over 'FloorLamp' if 'TableLamp' is the only observed lamp) and grounding the LLM's output to the physical environment.","['LLMPlanner (Embodied Agent Few-Shot Grounded Planning)', 'Grounded Replanning (with LLMs)', 'In-Context Learning for Agent Planning']","Prompt Engineering, Constrained Text Generation, Structured Output from LLMs, Agent Planning, Object Disambiguation",6,"Prompt Design, LLM-specific, Tools Integration"
Retrieval-Augmented Generation (RAG),"Large pretrained language models (PLMs) exhibit limitations in accessing and precisely manipulating factual knowledge, leading to suboptimal performance on knowledge-intensive tasks, difficulty in expanding or revising their internal knowledge, lack of provenance for predictions, and a tendency to hallucinate.","Knowledge-intensive Natural Language Processing (NLP) tasks (e.g., Open-domain Question Answering, Fact Verification, abstractive text generation) where AI models need to generate factual, specific, and diverse language, or require access to up-to-date and verifiable external knowledge. This pattern is applicable when using pretrained parametric memory models (like seq2seq transformers) that need to be augmented with external, dynamically revisable, and inspectable knowledge.","Combine a pretrained parametric memory (a seq2seq model, e.g., BART) with an explicit, non-parametric memory (a dense vector index of text documents, e.g., Wikipedia). A pretrained neural retriever (e.g., DPR) is used to access this non-parametric memory by retrieving top-K relevant documents based on the input query. The retrieved documents are then provided as additional context to the seq2seq generator. The entire system (retriever and generator) is fine-tuned end-to-end, treating the retrieved documents as latent variables and marginalizing over their probabilities during training and inference.","Achieves state-of-the-art performance on a wide range of knowledge-intensive NLP tasks. Generates more specific, diverse, and factual language, and reduces hallucinations compared to parametric-only baselines. Enables easy expansion and revision of the model's knowledge and provides a form of interpretability by allowing inspection of the accessed knowledge. Facilitates unconstrained generation, outperforming extractive approaches even for extractive tasks.","['RAGSequence', 'RAGToken', 'Index Hotswapping']","['Open-domain Question Answering (NQ, TriviaQA, WebQuestions, CuratedTrec)', 'Abstractive Question Answering (MSMARCO NLG)', 'Jeopardy Question Generation', 'Fact Verification (FEVER)']",21,"['Generative AI', 'Knowledge & Reasoning', 'LLM-specific']"
RAGSequence,"Ensuring consistency and coherence in generated sequences when leveraging retrieved knowledge, particularly when the entire output is expected to be derived from a single, dominant source or consistent context.","Retrieval-Augmented Generation (RAG) tasks where the generated output sequence is best supported by a single, consistent retrieved document. The model needs to make a global decision about which document to use for the entire generation process.","Within the Retrieval-Augmented Generation (RAG) framework, the model treats the retrieved document as a single latent variable responsible for generating the complete output sequence. It marginalizes over the top-K retrieved documents, calculating the probability of the entire sequence given each document, and then summing these probabilities weighted by the document retrieval probability.","Produces coherent and consistent output sequences based on a single, globally chosen document. Effective for tasks where a single document provides sufficient context for the whole output.","['Retrieval-Augmented Generation (RAG)', 'RAGToken']","['Open-domain Question Answering', 'Abstractive Question Answering', 'Fact Verification']",30,"['Generative AI', 'LLM-specific']"
RAGToken,Generating output sequences that require combining information from multiple distinct retrieved documents or dynamically switching context across different parts of the generated text to provide a comprehensive and diverse answer.,Retrieval-Augmented Generation (RAG) tasks where different tokens or segments of the output sequence might be best supported by different pieces of retrieved knowledge. This is particularly useful for complex generations that synthesize information from various sources.,"Within the Retrieval-Augmented Generation (RAG) framework, the model allows for drawing a potentially different latent document for each target token to be generated. It marginalizes over the top-K retrieved documents at each generation step, calculating the probability of the next token given each document, and then summing these probabilities. This enables the generator to dynamically choose content from several documents as it produces the answer token by token.","Generates responses that can combine content from multiple documents, leading to more diverse and factually rich outputs, especially for tasks requiring synthesis of information. Improves performance on tasks that benefit from dynamic context switching.","['Retrieval-Augmented Generation (RAG)', 'RAGSequence']","['Jeopardy Question Generation (where questions often combine two separate pieces of information)', 'Open-domain Question Answering', 'Abstractive Question Answering', 'Fact Verification']",30,"['Generative AI', 'LLM-specific']"
Index Hotswapping (Dynamic Knowledge Update),"Parametric-only language models suffer from 'stale knowledge' and cannot easily update their world knowledge as new information emerges without computationally expensive and time-consuming full model retraining. This limits their applicability in dynamic, knowledge-intensive environments.","AI systems, particularly those using large language models, that require up-to-date external knowledge and need to adapt to changing factual landscapes. This pattern is applicable when the core model architecture is designed with a separable, non-parametric memory component.","Employ a Retrieval-Augmented Generation (RAG) architecture where factual knowledge is stored in an external, non-parametric memory (a document index). To update the model's world knowledge, simply replace or update this external document index at test time. The parametric components of the model (e.g., the generator and query encoder) remain fixed and do not require retraining.","Enables dynamic and efficient updating of the model's world knowledge. The model can accurately respond to queries based on the most current information available in the swapped index. Dramatically reduces the need for continuous, costly retraining of large language models to stay current with real-world changes.",['Retrieval-Augmented Generation (RAG)'],"['Any knowledge-intensive NLP task requiring current information, such as factual question answering, news summarization, or policy compliance checks where external knowledge evolves rapidly.']",21,"['MLOps', 'Knowledge & Reasoning']"
Prompt Chaining / Multi-Step Prompting,"Single LLM calls are often insufficient for complex tasks requiring iterative refinement, multi-step reasoning, or interaction.",Tasks that can be broken down into a sequence of dependent sub-tasks or require intermediate steps of reasoning and/or action.,"Decompose a complex task into a sequence of smaller steps, where the output of one LLM call (or external action) informs the next prompt. This can involve using the LLM itself for preprocessing, eliciting reasoning, or generating intermediate steps.","Enables more complex algorithms and behaviors from LLMs, allowing for structured, multi-stage problem-solving.","['Self-Critique / Self-Refinement', 'ReAct (Reasoning and Acting)', 'Reasoning Actions', 'Decision Making (Planning & Execution)', 'Tree of Thoughts (ToT) / Deliberative Search-Based Planning']","Complex question answering, interactive agents, code generation, planning",41,
Retrieval Augmented Generation (RAG),"LLMs have limited knowledge (knowledge cutoff, factual inaccuracies) and struggle with domain-specific, private, or real-time information.","Tasks requiring access to up-to-date, specific, or external knowledge not present in the LLM's training data, or to ground responses in verifiable sources.","Augment the LLM's input prompt with relevant information retrieved from an external knowledge source (e.g., vector database, document store, web search, API). This retrieved context guides the LLM's generation.","Improves factual accuracy, reduces hallucinations, provides transparency, and grounds the LLM to specific, external information.","['Modular Memory System', 'Reasoning Actions']","Knowledge-intensive NLP tasks, question answering, data-driven intelligence, agents requiring external information",18,
Self-Critique / Self-Refinement,"LLM outputs may contain errors, biases, or suboptimal solutions, and a single generation might not be sufficient.","Tasks where quality control, error detection, and iterative improvement of LLM-generated content are crucial, or where multiple attempts can lead to a better solution.","Use the LLM itself (or another model) to evaluate its own previous output, identify flaws, and generate revised outputs or critiques. This often involves feeding the initial output and a critique/feedback back into the LLM as part of a new prompt, or generating multiple options and selecting the best.","Improves the quality, accuracy, and safety of LLM outputs by allowing for iterative correction and optimization.","['Prompt Chaining / Multi-Step Prompting', 'Reasoning Actions', 'Learning Actions', 'Memory-Augmented Reflection']","Code generation, complex problem-solving, dialogue systems, safety alignment, creative tasks",23,
Agentic Loop / Cognitive Loop / Decision-Making Loop,"LLMs are inherently stateless and passive, making them unsuitable for autonomous, adaptive behavior in dynamic, interactive environments.","Building intelligent agents that need to operate continuously, interact with external environments (physical, digital, human), maintain internal state, and pursue long-term goals.","Implement a continuous feedback loop where the agent iteratively performs the following steps: 1. Observe (perceive the environment, often grounded to text). 2. Plan (use internal processes like reasoning and retrieval from memory to decide on the next action). 3. Act (execute the chosen action, internal or external). 4. Learn (update internal memory or parameters based on the action's outcome or new experiences).","Transforms stateless LLMs into stateful, adaptive, and autonomous agents capable of sustained interaction and goal-directed behavior.","['Modular Memory System', 'Grounding Actions / Tool Use', 'Reasoning Actions', 'Learning Actions', 'Decision Making (Planning & Execution)', 'ReAct (Reasoning and Acting)']","Robotics, web agents, game AI, social simulations, long-running conversational agents",16,
Modular Memory System,"LLMs have limited context windows and are stateless, making it challenging to retain long-term information, track dialogue history, or store agent-specific knowledge and experiences.","Agents requiring persistence of information across interactions, access to past experiences, domain-specific knowledge, or the ability to learn and update internal state.","Design the agent with distinct, specialized memory modules, each optimized for different types of information and access patterns. These include: Working Memory (active, short-term), Episodic Memory (past experiences), Semantic Memory (general knowledge/facts), and Procedural Memory (agent's code, LLM weights, skills). Information is dynamically moved between these memories and working memory via retrieval and learning actions.","Overcomes LLM statelessness and context window limitations, enabling long-term memory, learning, more sophisticated reasoning, and a richer internal state for the agent.","['Retrieval Augmented Generation (RAG)', 'Learning Actions', 'Memory-Augmented Reflection']","Long-running dialogue agents, embodied agents, lifelong learning systems, personalized agents",26,
Grounding Actions / Tool Use,"LLMs operate on text and are isolated from the real world or specific digital tools, limiting their ability to perceive non-textual inputs or perform concrete actions.","Agents designed to operate in physical robots, interact with digital interfaces (APIs, websites, games), or engage in human dialogue, requiring interaction beyond pure text generation.","Implement specific procedures (tools, APIs, or direct code execution) that translate non-textual perceptual inputs (vision, audio, sensor data) into textual observations for the LLM, and translate LLM-generated textual commands or structured function calls into executable actions in the environment (e.g., motor commands, API calls, web interactions, natural language responses).","Enables LLMs to perceive and act in the external world, connecting their linguistic and reasoning abilities to real-world effects, expanding their capabilities beyond text generation.","['Agentic Loop / Cognitive Loop / Decision-Making Loop', 'Function Calling / Structured Output Parsing', 'ReAct (Reasoning and Acting)', 'LLM-Code Hybrid / Complementary Capabilities']","Robotics, web automation, API integration, game playing, conversational AI, data querying",24,
Reasoning Actions,"LLMs often generate direct answers or actions without explicit intermediate thought processes, which can lead to suboptimal or incorrect outputs for complex tasks.","Tasks requiring deeper analysis, summarization, distillation of observations, inference from retrieved knowledge, or the generation of intermediate steps before a final action or answer.","Design specific internal actions where the LLM is prompted to process and synthesize information within its working memory (e.g., recent observations, retrieved facts) to generate new, temporary insights, analyses, or intermediate steps. This output is then written back into working memory, informing subsequent decisions.","Enables more deliberate, robust, and interpretable thought processes, leading to better-informed actions and improved problem-solving.","['Prompt Chaining / Multi-Step Prompting', 'Self-Critique / Self-Refinement', 'Decision Making (Planning & Execution)', 'Tree of Thoughts (ToT) / Deliberative Search-Based Planning', 'Memory-Augmented Reflection', 'ReAct (Reasoning and Acting)']","Planning, self-reflection, summarization, complex problem-solving, generating Chain-of-Thought",41,
Learning Actions,"Agents need to continuously acquire and store new information, update their knowledge, or refine their behaviors based on experience or feedback to improve performance over time, beyond just in-context learning.","Agents operating in dynamic environments, requiring adaptation, skill acquisition, long-term knowledge retention, or self-improvement.","Implement explicit internal actions that allow the agent to write new information to its long-term memory modules or update its internal parameters. This includes: updating episodic memory with experiences, updating semantic memory with knowledge/inferences, finetuning LLM parameters (implicit procedural memory), and modifying agent code (explicit procedural memory).","Enables agents to improve their capabilities, adapt to new situations, and achieve lifelong learning and self-improvement, reducing dependence on static knowledge.","['Modular Memory System', 'Self-Critique / Self-Refinement', 'Skill Learning / Procedural Knowledge Acquisition', 'Memory-Augmented Reflection']","Lifelong learning, skill acquisition, adaptation to new tasks/environments, self-improving agents",10,
Decision Making (Planning & Execution),"With a diverse action space and memory, agents need a structured and deliberate way to choose the most appropriate action at any given time, especially for complex, multi-step tasks.","Agents operating in environments requiring deliberation, exploration of alternatives, anticipation of consequences, or complex goal-directed behavior.","Structure the agent's internal decision process into a planning phase that precedes execution. This involves: 1. Proposal (generating one or more candidate actions or intermediate thoughts). 2. Evaluation (assessing the potential value, outcomes, or feasibility of proposed actions/thoughts, often using LLM reasoning or internal simulations). 3. Selection (choosing the best action or thought, or deciding to backtrack/re-propose). The selected action is then executed.","Enables deliberate, goal-oriented behavior, allowing agents to explore options, anticipate consequences, and make more informed decisions than direct, single-pass action generation.","['Agentic Loop / Cognitive Loop / Decision-Making Loop', 'Reasoning Actions', 'Tree of Thoughts (ToT) / Deliberative Search-Based Planning', 'LLM-Code Hybrid / Complementary Capabilities']","Complex task solving, strategic games, robotics, problem-solving, general agent control flow",0,
Skill Learning / Procedural Knowledge Acquisition,"Agents often require domain-specific skills or complex action sequences that are difficult to hardcode or learn from scratch with a single LLM, and need to expand their capabilities over time.","Embodied agents or agents in complex digital environments (e.g., games, software development) where new, reusable capabilities (code-based skills, prompt templates) are needed.","Enable the agent to autonomously generate, test, refine, and store new code-based skills (grounding procedures) or prompt templates (reasoning procedures) in its procedural memory. These learned skills can then be retrieved and reused for future tasks.","Allows agents to autonomously expand their action space, master complex tasks, and generalize to unseen tasks, reducing reliance on human pre-specification and enabling curriculum learning.","['Learning Actions', 'Modular Memory System', 'Grounding Actions / Tool Use', 'Self-Critique / Self-Refinement', 'LLM-Code Hybrid / Complementary Capabilities']","Robotics, game AI (e.g., Minecraft's tech tree), interactive code generation, automating complex workflows",10,
Memory-Augmented Reflection,"Agents accumulate raw episodic experiences, but need to distill these into higher-level, generalized knowledge or insights to improve future decision-making and planning, beyond just direct recall.","Agents with a growing episodic memory that contains many past interactions or trajectories, where abstracting lessons learned is beneficial.","Periodically use the LLM to perform 'reasoning actions' over a collection of past 'episodic memories' to generate 'reflections' or 'inferences.' These distilled insights are then stored in 'semantic memory' as new, generalized knowledge, which can be retrieved later to inform planning or decision-making.","Enables agents to learn higher-level, abstract knowledge from their experiences, leading to more robust planning, better self-correction, and improved generalization.","['Modular Memory System', 'Reasoning Actions', 'Learning Actions', 'Retrieval Augmented Generation (RAG)', 'Self-Critique / Self-Refinement']","Social simulation, long-term planning, self-improvement, debugging agent behavior",10,
Tree of Thoughts (ToT) / Deliberative Search-Based Planning,"Complex reasoning problems (e.g., creative writing, puzzle solving, multi-step planning) require exploring multiple reasoning paths, evaluating intermediate steps, and backtracking from dead ends, which single-pass LLM generation struggles with.","Tasks where the optimal solution requires exploring a combinatorial space of thoughts or actions, similar to classical search problems, and where intermediate thoughts can be evaluated.","Structure the decision-making process as a tree search (e.g., BFS, DFS, MCTS) where the LLM generates 'thoughts' (intermediate reasoning steps or potential actions) as nodes, evaluates their quality or promise, and the agent selects the most promising path to explore, allowing for backtracking and global exploration.","Enables more deliberate, robust, and potentially optimal problem-solving by leveraging the LLM's reasoning capabilities within a structured search framework, overcoming the 'myopia' of autoregressive generation.","['Decision Making (Planning & Execution)', 'Reasoning Actions', 'Prompt Chaining / Multi-Step Prompting']","Creative writing, puzzle solving (e.g., Game of 24), complex reasoning tasks, strategic planning",41,
LLM-Code Hybrid / Complementary Capabilities,"LLMs excel at flexible, commonsense reasoning and text generation but can be brittle, opaque, and poor at deterministic logic, complex algorithms, or maintaining precise state. Traditional code is robust for logic but lacks generalization and natural language understanding.","Building agents that require both flexible, open-ended reasoning and reliable, deterministic execution of complex logic, algorithms, or precise interactions.","Design agents with a hybrid architecture where the LLM handles flexible reasoning, natural language understanding, and generation of high-level plans/intentions, while deterministic code (e.g., Python functions, classical algorithms, state machines) handles structured logic, precise calculations, state management, and reliable interactions with tools/APIs.","Combines the strengths of LLMs (flexibility, commonsense) with the reliability of traditional code (logic, control), leading to more robust, controllable, and capable agents. Reduces LLM hallucination in structured tasks.","['Grounding Actions / Tool Use', 'Function Calling / Structured Output Parsing', 'Decision Making (Planning & Execution)', 'Skill Learning / Procedural Knowledge Acquisition']","Planning, robotics, data analysis, complex workflow automation, any agent requiring both high-level reasoning and precise execution",24,
ReAct (Reasoning and Acting),LLMs struggle with tasks requiring iterative interaction with the environment and intermediate reasoning steps to effectively plan and execute actions. Direct action generation can be myopic or prone to errors.,"Interactive tasks in digital environments (e.g., web, APIs, text games) where the agent needs to observe, think, and act sequentially to achieve a goal.","Integrate reasoning and acting in a tight, interleaved loop. The LLM generates a 'Thought' (reasoning step) to analyze the current situation, plan, or reflect, followed by an 'Action' (grounding step) to interact with the environment or call a tool. The environment's 'Observation' (feedback) then feeds back into the next 'Thought', creating a continuous thought-action-observation cycle.","Enables agents to perform complex, multi-step tasks by breaking them down into manageable, self-correcting cycles, improving task completion, robustness, and interpretability.","['Agentic Loop / Cognitive Loop / Decision-Making Loop', 'Prompt Chaining / Multi-Step Prompting', 'Reasoning Actions', 'Grounding Actions / Tool Use', 'Decision Making (Planning & Execution)']","Web navigation, API interaction, text-based games, general problem-solving, tool-use agents",11,
Function Calling / Structured Output Parsing,"LLMs naturally generate free-form text, which is difficult to reliably parse into structured data or executable function calls needed for tools, APIs, or internal actions, leading to brittle integration.","Agents needing to use external tools, APIs, or internal functions where inputs must conform to a specific schema, function signature, or data structure.","Design prompts that explicitly instruct the LLM to generate output in a structured, machine-readable format (e.g., JSON, YAML, or a specific function call syntax). Use a robust parser (or built-in LLM capabilities like OpenAI's function calling) to extract the structured data or arguments for executing a predefined function or tool. Techniques like constrained generation can enforce this.","Enables reliable and robust integration of LLMs with external tools and internal code, making agent actions more predictable, controllable, and less prone to parsing errors.","['Grounding Actions / Tool Use', 'LLM-Code Hybrid / Complementary Capabilities']","API integration, database querying, code generation, structured data extraction, controlling external systems",27,
End-to-End Domain-Adaptive RAG Training,"Standard Retrieval Augmented Generation (RAG) models, pre-trained on general knowledge bases (e.g., Wikipedia), struggle to adapt to specialized domains (e.g., healthcare, news) because their retriever components and external knowledge base encodings are typically fixed during finetuning, preventing effective domain-specific learning.",Developing or deploying RAG systems for Open-Domain Question Answering (ODQA) or other knowledge-intensive NLP tasks in specialized domains where the target knowledge base significantly differs from the general-purpose data used for initial pre-training. The system needs to learn deep domain-specific representations for both retrieval and generation.,"Jointly finetune all RAG components: the retriever's question encoder, the retriever's passage encoder, and the generator (e.g., BART) on domain-specific data. This is enabled by:
1.  **Asynchronous Knowledge Base Re-encoding and Re-indexing:** To overcome the computational bottleneck of updating a large external knowledge base, implement an asynchronous process. This involves dedicated computational resources (e.g., separate GPUs for re-encoding, CPUs for FAISS re-indexing) that run independently of the main training loop. The main loop continues training with a periodically updated index, while the asynchronous processes prepare the next updated knowledge base and index.
2.  **Auxiliary Statement Reconstruction Task:** Introduce a secondary, auxiliary training signal where the model is tasked with reconstructing a given domain-specific statement (e.g., a sentence from an abstract or summary) by retrieving relevant passages from the external knowledge base and generating the statement. This explicitly forces the model to acquire and utilize domain-specific knowledge, enhancing the retriever's understanding and the generator's factual grounding in the new domain.","Significantly improved performance (higher Exact Match, F1 scores for QA, and Top-K retrieval accuracy) in domain-specific ODQA tasks. The approach leads to better domain adaptation of the retriever component compared to standalone finetuning, and the auxiliary task further boosts overall accuracy by injecting more domain-specific knowledge.",['Domain-Specific Retriever Finetuning'],"Domain adaptation for RAG models, building specialized knowledge-intensive AI systems (e.g., medical QA, legal search, enterprise chatbots), improving factual consistency and reducing hallucinations in generative models for new domains.",21,
Domain-Specific Retriever Finetuning,"A general-purpose neural retriever (e.g., DPR) trained on broad datasets (e.g., Wikipedia) performs poorly when deployed in specialized, domain-specific contexts due to a mismatch in data distribution and knowledge.","Improving the retrieval accuracy of a neural retriever for a specific domain when used as a standalone component or as part of a larger system (e.g., a RAG model initialized with a domain-adapted retriever). This approach requires access to domain-specific gold-standard QA pairs and carefully selected negative examples.","Generate a dataset of domain-specific gold-standard passages (containing answers for given questions) and carefully selected hard-negative passages (lexically similar to the question but not containing the answer, often identified using methods like BM25 lexical matching). Then, finetune the neural retriever (e.g., DPR, consisting of a question encoder and a passage encoder) on this domain-specific data using a similarity-based loss function (e.g., dot-product similarity) that maximizes similarity between relevant question-passage pairs and minimizes it for negative pairs.","Improved retrieval accuracy (Top-K scores) for the neural retriever within the target domain. This can be used to initialize RAG models with a better-performing retriever for the specific domain, although it is shown to be less effective for overall RAG domain adaptation than the End-to-End Domain-Adaptive RAG Training pattern.",['End-to-End Domain-Adaptive RAG Training'],"Improving information retrieval in specialized domains, pre-training/initializing retrievers for domain-specific RAG systems, standalone search engines for niche knowledge bases.",21,
LLM as RL Policy with Online Grounding,"Large Language Models (LLMs), despite possessing abstract knowledge, often lack functional grounding and alignment with interactive environments, which limits their functional competence and ability to solve decision-making problems.","An agent needs to operate in an interactive textual (or embodied) environment, solving goals specified in natural language, where its internal LLM knowledge must be aligned with external dynamics and relational structures at various levels of abstraction.","Use an LLM directly as the agent's policy. The agent's task description, current observation, and the set of possible actions are gathered into a prompt. Action probabilities are computed by leveraging the LLM's language modeling heads to calculate the conditional probabilities of tokens composing each action. This LLM policy is then progressively updated and functionally grounded using online Reinforcement Learning (e.g., PPO) based on real-time interactions, observations, and rewards from the environment. A value head can be added on top of the LLM for the RL algorithm.","Drastically improves functional grounding, performance, sample efficiency, and generalization abilities (to new objects and some new tasks) compared to zero-shot LLM use or offline pre-finetuning. It enables LLMs to adapt to domain-specific vocabularies and quickly discard useless actions, leveraging their pretrained knowledge for faster skill acquisition.","['Distributed LLM Policy Inference and Training', 'Action Head for LLM Policy', 'Behavioral Cloning for LLM Policy Initialization', 'Reinforcement Learning from Human Feedback (RLHF) for LLMs', 'LLM as High-Level Planner']","Text-based interactive agents, embodied AI, robotics, learning to solve language-conditioned Reinforcement Learning tasks in complex environments.",25,
LLM as High-Level Planner,"Complex, long-horizon tasks for embodied agents or decision-making systems are difficult to plan directly or require strategic guidance beyond low-level actions. LLMs possess extensive prior knowledge that could aid in generating such plans.","Robotics setups, embodied agents, or textual environments where an agent needs to break down a goal into a sequence of high-level steps or sub-goals. The LLM's role is to provide strategic direction rather than direct low-level control.","Leverage an LLM to generate high-level plans, sequences of actions, or sub-goals. The LLM acts as a planner, providing strategic guidance, but does not directly execute low-level actions. This approach often involves a closed-loop feedback mechanism where the LLM's plan is updated based on environmental observations or a 'reporter' providing useful information about the environment's state.","Provides abstract, strategic guidance for agents, leveraging the LLM's extensive prior knowledge about the world to suggest plans of action to solve goals. However, without direct interaction-based grounding, it may suffer from misalignment for low-level execution or require external affordance functions to rerank proposed actions.","['LLM as RL Policy with Online Grounding', 'Behavioral Cloning for LLM Policy Initialization']","Robotics, embodied tasks, vision-and-language navigation, textual adventure games, general decision-making requiring multi-step planning.",0,
Behavioral Cloning for LLM Policy Initialization,Training LLMs for decision-making tasks in interactive environments can be sample inefficient when starting from scratch. LLMs may initially lack the specific behaviors or task-relevant knowledge required for effective interaction.,LLM-based agents in interactive environments where expert demonstrations or a dataset of successful trajectories are available. This pattern is often used as a preparatory step before deploying an agent or engaging in online interaction.,"Pre-finetune an LLM on a dataset of expert trajectories using Behavioral Cloning (BC) or Offline Reinforcement Learning. This process teaches the LLM to mimic expert actions given observations and goals, effectively initializing its policy with known good behaviors. The pre-trained policy can then be deployed directly or further finetuned with online interaction.","Provides an initial policy that can mimic expert behavior, potentially improving starting performance and sample efficiency for subsequent online learning. However, models trained purely with BC may perform worse than online RL for complex tasks, especially if expert data is limited or contains suboptimal actions, as BC inherently lacks direct grounding through environmental interaction and active exploration.","['LLM as RL Policy with Online Grounding', 'LLM as High-Level Planner']","Policy initialization for LLM-based agents, imitation learning, leveraging existing datasets for agent training, pre-training for online Reinforcement Learning.",17,
Reinforcement Learning from Human Feedback (RLHF) for LLMs,"Aligning the text generated by LLMs with nuanced human preferences, values, safety guidelines, or specific instructions, which are often difficult to capture with simple, hand-crafted reward functions.","LLMs are used for natural language generation tasks (e.g., chatbots, content creation, instruction following) where the quality, helpfulness, or safety of the generated text needs to be evaluated and improved based on human judgment.","Treat the LLM's text generation as a sequential decision-making problem, where each generated token is an action. Collect human feedback on LLM outputs (e.g., preferences, ratings) to train a reward model. Then, use Reinforcement Learning (e.g., PPO) to finetune the LLM policy directly, using the learned reward model as the reward signal. The 'environment' in this context is typically the LLM itself and its output, with the next state being the previous state plus the newly generated token.","Produces LLMs that generate text more aligned with human preferences, are more helpful, harmless, and honest, and better follow instructions. This can lead to models with fewer parameters outperforming larger models not fine-tuned with RLHF in terms of human alignment metrics.",['LLM as RL Policy with Online Grounding'],"Chatbot alignment, instruction-following models (e.g., InstructGPT), content moderation, improving safety and ethical behavior of generative LLMs.",6,
Distributed LLM Policy Inference and Training,"Using large Language Models (LLMs) as real-time policies in online Reinforcement Learning (RL) is computationally intensive. It requires frequent and fast inference for action probability computation across many environments, and efficient distributed training for policy updates, which can lead to significant bottlenecks and make training intractable with a single LLM instance.","Online RL setups where an LLM acts as the agent's policy, interacting with multiple environments (e.g., 32 BabyAIText environments) in parallel, and requiring frequent gradient updates to the large LLM parameters.","To accelerate online RL finetuning, deploy multiple LLM instances (workers) in parallel. For inference, distribute the task of computing action probabilities for different actions or environments across these workers (e.g., each worker scores a subset of actions). For training, leverage Distributed Data Parallelism (DDP) to compute gradients on mini-batches in parallel across LLM instances, then gather and update the models synchronously. A client-server architecture can manage communication and dispatch calls.","Significantly reduces the computational time for LLM inference and training in online RL, enabling quasi-linear scaling with the number of deployed LLMs. This allows for the use of larger LLMs and more extensive interactions, making previously intractable experiments feasible and improving overall sample efficiency.",['LLM as RL Policy with Online Grounding'],"Scaling online RL with large LLMs, high-throughput LLM inference for real-time decision-making, MLOps for LLM-based agents in interactive environments, distributed training of large models for agent policies.",25,
Action Head for LLM Policy,"When using an LLM as a policy, directly leveraging its language modeling heads to predict action probabilities can be inefficient, misaligned, or lead to slow learning, especially for non-pretrained LLMs, when the action space is fixed and distinct from the general token vocabulary, or if the LLM's final layer encodings are not directly suitable for language modeling.","Deploying an LLM as an agent's policy in an RL environment where the agent needs to select an action from a predefined, often small, discrete action space. This is an alternative to using the LLM's original language modeling heads for action probability calculation.","Instead of relying on the LLM's inherent language modeling capabilities for action selection, add a dedicated 'action head' (e.g., a Multi-Layer Perceptron - MLP) on top of the LLM's last encoder or decoder layer. This action head directly maps the LLM's internal representation to a probability distribution over the specific, discrete action space.","Can simplify the learning process for non-pretrained models and provide a more direct and potentially efficient way to derive action probabilities for a fixed, discrete action set. However, for fully pretrained LLMs, it might require more training steps to align the new action head with the LLM's existing knowledge compared to leveraging the language modeling heads directly, which already benefit from extensive pretraining.",['LLM as RL Policy with Online Grounding'],"RL agents with LLMs, adapting LLMs to specific discrete action spaces, situations where a direct mapping from LLM embeddings to actions is preferred over token-based probability calculations, finetuning LLMs for specific tasks.",25,
InContext Retrieval-Augmented Language Model (InContext RALM),"Pretrained Language Models (LMs) inherently lack access to up-to-date external knowledge, often produce factual inaccuracies, and cannot provide source attribution. Existing Retrieval-Augmented Language Modeling (RALM) approaches typically require modifying the LM architecture or extensive retraining, which complicates deployment, especially when using off-the-shelf LMs or LMs accessed via API (black-box models).","Building or deploying AI systems that rely on large language models for text generation, where factual accuracy, external knowledge grounding, and source attribution are critical, but modification or retraining of the core LM is impractical, undesirable, or impossible. This is common in scenarios using black-box LMs or requiring rapid deployment with existing models.","Integrate external knowledge by dynamically retrieving relevant documents from a grounding corpus and prepending them directly to the Language Model's input sequence as a context prefix. This leverages the LM's inherent 'in-context learning' capabilities without altering its architecture or requiring further training. The document selection can initially use off-the-shelf general-purpose retrievers (e.g., BM25). Key parameters include 'retrieval stride' (how often retrieval occurs) and 'retrieval query length' (how much of the prefix is used for the query).","Substantially improves LM performance (e.g., perplexity), mitigates factual inaccuracies, enables natural source attribution, and significantly simplifies the deployment of retrieval-augmented systems by allowing the use of frozen, off-the-shelf LMs, even those accessible only via API.","LM-Oriented Reranking, Nearest Neighbor Language Model (kNNLM)","Language modeling, Open-Domain Question Answering (ODQA), improving factuality of generated text, scenarios with black-box LM access.",21,
LM-Oriented Reranking,"Initial document retrieval (e.g., using lexical or general-purpose dense retrievers) for Retrieval-Augmented Language Models (RALMs) may not select the most semantically relevant document for the specific LM task, or may not adequately prioritize parts of the query (e.g., recency of tokens). This leaves significant potential for improving the quality of the grounding documents presented to the LM.","An InContext RALM system where an initial set of candidate documents (e.g., top-k from a general-purpose retriever like BM25) has been identified, but a more refined selection is needed to maximize the LM's performance. The goal is to choose the single best document to prepend to the LM's input.","Instead of directly using the top-ranked document from the initial retrieval, a secondary reranking mechanism is employed to select the most relevant document from a pool of top-k candidates. This reranking can be achieved in two primary ways: 1) **Zero-Shot Reranking:** Use an off-the-shelf Language Model (potentially a smaller, faster one) to score each candidate document by evaluating how well it helps predict a subsequent segment of the input prefix or the target generation. The document yielding the highest predictive probability is chosen. 2) **Predictive Reranking (Trained):** Train a specialized reranker (e.g., a fine-tuned bidirectional encoder like RoBERTa) to classify or score candidate documents based on their likelihood of improving the LM's prediction of upcoming text. This reranker is trained using the LM's own signal (e.g., p(next_tokens | document, prefix)) as supervision on domain-specific data.","Further significant improvements in LM performance (e.g., perplexity) by providing more semantically aligned and contextually relevant grounding documents, especially for black-box LMs where direct LM modification is not possible.",InContext Retrieval-Augmented Language Model (InContext RALM),"Enhancing InContext RALM, improving document selection for factual grounding, optimizing contextualization for LLMs, improving LM performance in knowledge-intensive tasks.",30,
Nearest Neighbor Language Model (kNNLM),"Language Models (LMs) can struggle with generalization and incorporating specific, fine-grained knowledge, often leading to less accurate next-token predictions, especially for rare or out-of-distribution tokens. Scaling this approach to large corpora is an open challenge due to the expense of storing token representations.","Enhancing the predictive capabilities of a language model during inference, particularly for next-token prediction, by leveraging a dynamic, external knowledge store. This approach focuses on interpolating distributions rather than prepending documents.",Augment the LM's next-token probability distribution by interpolating it with a distribution derived from its k-nearest neighbors found in an external retrieval corpus. The 'neighbors' are typically identified by finding tokens in the corpus whose LM embeddings are closest to the query token's embedding. This is an inference-time model modification.,Improves language modeling performance and generalization by incorporating specific knowledge from the retrieval corpus.,InContext Retrieval-Augmented Language Model (InContext RALM),"Language modeling, improving next-token prediction, incorporating dynamic knowledge at inference time.",21,
Retriever-Aware Training (RAT),"Large Language Models (LLMs) struggle to effectively utilize retrieved documentation, especially when it is imperfect, outdated, or irrelevant. This can lead to decreased accuracy and increased hallucination when LLMs are tasked with using external tools like APIs, and they fail to adapt to dynamic changes in tool specifications.","Training LLMs to interact with external tools or APIs where the model relies on a document retriever to provide relevant context (e.g., API documentation) at inference time. The retrieved information might not always be perfectly accurate or up-to-date.","During the instruction-finetuning process, augment the user prompt with the relevant retrieved documentation (which may contain imperfections or be outdated, mimicking a real retriever's output). Crucially, provide the *accurate ground-truth API call* in the LLM's response. This approach teaches the LLM to critically evaluate the retrieved context, utilize it when relevant, and rely on its baked-in domain-specific knowledge (or ignore irrelevant context) when the retrieval is poor.","The LLM demonstrates improved accuracy, significantly reduced hallucination errors, and enhanced ability to adapt dynamically to test-time changes in API documentation and tool specifications. It learns to 'judge the retriever's' relevance and quality.",Instruction-Tuned API Invocation,"Training LLMs for robust tool use, API invocation, agentic systems requiring dynamic knowledge access, mitigating hallucination stemming from retrieved context, ensuring adaptability to evolving external systems.",5,
Instruction-Tuned API Invocation,General-purpose Large Language Models (LLMs) often lack awareness of the vast and frequently updated landscape of available APIs and struggle with the specific knowledge and syntactic precision required to accurately generate API calls based on natural language instructions. This limits their practical utility in effectively using external tools.,"Developing LLM-powered applications that require the LLM to interact with external systems, services, or libraries by generating precise, functionally correct API calls, often from a large and evolving set of potential tools.","Employ a self-instruct paradigm to create a specialized, comprehensive dataset comprising natural language instructions paired with their corresponding ground-truth API calls (including relevant packages and explanations). Then, instruction-finetune a base LLM (e.g., LLaMA) on this dataset, framing the training as a user-agent chat-style conversation. This process imbues the LLM with domain-specific knowledge and the ability to generate syntactically correct and functionally appropriate API calls.","The finetuned LLM gains a strong capability to accurately select and invoke APIs, reason about user-defined constraints (e.g., performance, parameters, accuracy), and generate actionable code. This approach significantly outperforms un-finetuned or few-shot prompted general LLMs for complex API-related tasks.",Retriever-Aware Training (RAT),"Building LLM agents for software development, automating tasks requiring external tool use, intelligent code generation, integrating LLMs with complex systems, enabling LLMs to act as 'flexible interfaces' to the digital world.",5,
AST-based Code/API Evaluation,"Evaluating the functional correctness and detecting hallucinations in LLM-generated code or API calls is challenging. Semantic equivalence is difficult to verify through traditional unit tests due to multiple valid solutions, and standard Natural Language Processing (NLP) metrics fail to capture the structural and functional accuracy required for code.","Assessing the quality, correctness, and reliability of code or API calls generated by Large Language Models, particularly in scenarios where the generated output needs to be syntactically correct, functionally equivalent to a reference, and free from imagined (hallucinated) elements.","Implement an evaluation framework that utilizes Abstract Syntax Tree (AST) subtree matching. For each generated API call, construct its AST. Compare this AST against a comprehensive database of known, correct API call ASTs. Functional correctness is determined by whether the generated AST (or a relevant part of it) matches a subtree within the reference database. Hallucination is specifically defined and detected when a generated API call's AST does not match any known API in the reference database, indicating an entirely imagined tool or structure.","Provides a robust, objective, and scalable offline metric for precisely measuring functional correctness and identifying hallucination errors in LLM-generated code. This method demonstrates a strong correlation with human evaluation, making it an efficient alternative to manual validation.",[],"Benchmarking LLMs for code generation capabilities, automated testing of API invocation systems, developing hallucination detection mechanisms for LLM outputs, MLOps for code quality assurance in AI systems, and evaluating the adherence of generated code to specific constraints.",5,
LLM as Knowledge Graph Agent,"Large Language Models (LLMs) frequently suffer from hallucinations, struggle with specialized or outdated knowledge, lack transparency, and perform poorly in complex multi-hop reasoning tasks. Existing LLM-KG integration methods often treat LLMs as mere translators or augmenters, rather than active reasoners, which limits their ability to fully leverage structured knowledge and provide explainable decisions.","AI systems that demand deep, responsible, and verifiable reasoning over factual knowledge, where the inherent limitations of LLMs (knowledge gaps, hallucination) and the structured nature of Knowledge Graphs (KGs) need to be synergistically combined.","Design the LLM to function as an intelligent agent that actively and interactively explores, reasons, and makes dynamic decisions on a Knowledge Graph. Instead of simply receiving pre-retrieved facts or translating queries, the LLM directly guides the search and reasoning process step-by-step on the KG, dynamically discovering relevant entities and relations.","Significantly enhances LLMs' deep reasoning capabilities, mitigates hallucination by grounding responses in verifiable facts, improves knowledge traceability and explainability, and allows for more flexible and efficient knowledge updates. This approach can also enable smaller LLMs to achieve performance competitive with larger models in knowledge-intensive tasks.","['LLM-Guided Iterative Knowledge Graph Exploration (ThinkonGraph)', 'Knowledge Traceability and Correctability via Explicit Reasoning Paths']","Multi-hop Knowledge Base Question Answering (KBQA), complex open-domain question answering, fact-checking, and any application requiring LLMs to perform grounded, verifiable reasoning.",14,
LLM-Guided Iterative Knowledge Graph Exploration (ThinkonGraph),"Effectively navigating and extracting multi-hop reasoning paths from large and complex Knowledge Graphs (KGs) for LLM-based reasoning is challenging, as direct query formulation is often insufficient and brute-force search is inefficient. A key challenge is to ensure that the most relevant information is prioritized in the exploration process.",AI systems that utilize LLMs to answer complex questions or execute reasoning tasks requiring the synthesis of information from multiple entities and relations within a Knowledge Graph. The system needs to dynamically discover and refine relevant reasoning paths.,"Implement an iterative beam search mechanism on the Knowledge Graph, where a Large Language Model (LLM) serves as the intelligent decision-making agent at each step. This process involves:
1.  **Initialization:** The LLM identifies initial topic entities from the input question to begin the search.
2.  **Iterative Exploration:** In each iteration, the LLM performs a two-step 'Search and Prune' process:
    *   **Relation Exploration:** Search for all neighboring relations from the current tail entities of the top-N paths. The LLM then 'prunes' these candidates by scoring and selecting the most relevant relations to extend the paths.
    *   **Entity Exploration:** Based on the selected relations, search for candidate tail entities. The LLM then 'prunes' these entities by scoring and selecting the most relevant ones to form new top-N reasoning paths.
3.  **Reasoning/Evaluation:** After each exploration step, the LLM evaluates whether the current set of top-N reasoning paths is sufficient to answer the question. If so, it generates the answer; otherwise, it continues iterating or falls back to its inherent knowledge if the maximum search depth is reached.
**Variant (ToGR):** For increased efficiency, the entity pruning step can be replaced with random sampling, thereby focusing more on relation chains and reducing LLM calls.","Enables LLMs to dynamically extract diverse and multi-hop reasoning paths, significantly enhancing deep reasoning capabilities for knowledge-intensive tasks. Improves efficiency by guiding the search with LLM intelligence and allows for flexible integration of different LLMs and KGs.","['LLM as Knowledge Graph Agent', 'Knowledge Traceability and Correctability via Explicit Reasoning Paths']","Multi-hop Knowledge Base Question Answering (KBQA), complex question answering, information retrieval from structured knowledge bases, and general knowledge-intensive reasoning tasks requiring iterative refinement.",14,
Knowledge Traceability and Correctability via Explicit Reasoning Paths,"Large Language Models often produce opaque answers, making it difficult to understand the provenance of information, diagnose errors (e.g., hallucinations or outdated facts), and build user trust. Furthermore, correcting internal LLM knowledge is an expensive and time-consuming process.","AI systems where transparency, explainability, verifiability, and the ability to diagnose and correct knowledge-related errors are critical. This pattern is particularly relevant when combining LLMs with external knowledge sources like Knowledge Graphs.","The AI system is designed to generate and expose the explicit reasoning paths (e.g., sequences of triples from a Knowledge Graph) that were used by the LLM to arrive at its answer. These paths serve as verifiable evidence. If an answer is questioned or found incorrect by users, experts, or other AI systems, the explicit path enables:
*   **Tracing:** Pinpointing the exact knowledge triples or inference steps that led to the output.
*   **Diagnosis:** Identifying erroneous, outdated, or missing information within the KG, or issues in the LLM's interpretation/reasoning.
*   **Correction:** Facilitating human or automated correction of the identified faulty knowledge (e.g., updating triples in the KG, a process referred to as 'knowledge infusion'), thereby improving the system and its underlying knowledge base.","Enhances the explainability and transparency of LLM reasoning, builds user trust, enables efficient debugging and correction of knowledge errors, and contributes to the continuous improvement and quality assurance of underlying Knowledge Graphs.","['LLM as Knowledge Graph Agent', 'LLM-Guided Iterative Knowledge Graph Exploration (ThinkonGraph)']","Responsible AI development, human-in-the-loop AI systems, fact-checking applications, knowledge base curation, and any domain where verifiable and auditable AI decisions are required.",23,
Retrieval Augmented Fine Tuning (RAFT),"Pretrained Large Language Models (LLMs) struggle to accurately answer questions in specialized, 'open-book' domains when presented with external documents, especially when retrieval is imperfect (i.e., includes irrelevant 'distractor' documents). Existing finetuning or in-context learning methods do not adequately prepare LLMs for this specific challenge.","Adapting LLMs for domain-specific Retrieval-Augmented Generation (RAG) tasks where the LLM needs to leverage a given set of documents (e.g., legal, medical, enterprise, code repositories) and be robust to noisy retrieval outputs that may contain irrelevant information. The primary goal is to maximize accuracy based on provided documents rather than general knowledge.","A novel finetuning recipe that trains the LLM on question-answer pairs (Q-A) alongside a meticulously prepared set of documents (Dk). This set includes 'golden documents' (containing the answer) and 'distractor documents' (irrelevant). The answers (A) are generated in a Chain-of-Thought style, including verbatim citations from the relevant documents. A crucial aspect is that for a portion (1-P fraction) of the training data, the golden document is intentionally omitted, compelling the model to learn robustness against missing context or to memorize when appropriate.","Significantly improves LLM performance in domain-specific RAG settings, enhances the model's ability to discern and disregard irrelevant information, and boosts factual accuracy and explainability by teaching it to extract and cite relevant information effectively. It consistently outperforms supervised finetuning (with or without RAG) and general-purpose models with RAG.","['Retrieval-Augmented Generation (RAG)', 'Training with Distractor Documents', 'Chain-of-Thought Reasoning']","Domain-specific Question Answering, enhancing LLM robustness to imperfect retrieval, improving factual consistency and explainability in RAG systems, adapting LLMs to specialized knowledge domains.",21,"MLOps, LLM-specific, Prompt Design, Knowledge & Reasoning"
Training with Distractor Documents,"Large Language Models (LLMs) are vulnerable to irrelevant text (distractors) within the context provided by retrieval systems, particularly in top-k RAG scenarios where high recall often means including some noise. Training solely with perfectly relevant documents diminishes the model's ability to discern pertinent information from noise.","Designing robust Retrieval-Augmented Generation (RAG) systems where the LLM needs to process a context window that may contain a mix of relevant and irrelevant information from external sources, and the model must learn to focus only on the pertinent parts.","During the LLM finetuning process, intentionally compose training examples by including both 'golden' (relevant) documents and 'distractor' (irrelevant) documents in the input context. Experiment with varying numbers of distractor documents and, for a proportion of the training data, even omit the golden document to further compel the model to handle uncertainty or memorization.","Enhances the LLM's robustness against irrelevant text, improves its ability to discern and disregard irrelevant content, and makes the model more resilient to fluctuations in the number and quality of documents encountered during testing.",['Retrieval Augmented Fine Tuning (RAFT)'],"Improving LLM robustness against noisy retrieval, enhancing context processing capabilities, preventing distraction by irrelevant information in RAG, preparing models for real-world imperfect retrieval scenarios.",1,"MLOps, LLM-specific"
Chain-of-Thought Reasoning,"Large Language Models (LLMs) can struggle with complex, multi-step reasoning tasks, often providing direct answers without showing their derivation. This can lead to a lack of transparency, reduced accuracy for intricate problems, and potential overfitting to concise answer formats.","Training or prompting LLMs for tasks requiring logical deduction, multi-hop question answering, or improved explainability, where the process of arriving at an answer is as important as the answer itself.","Generate training data or craft prompts that explicitly guide the LLM to produce an intermediate, step-by-step reasoning process (a 'chain of thought') that leads to the final answer. In the context of Retrieval Augmented Generation (RAG), this often includes citing specific parts of the provided context verbatim to justify each step.","Improves the model's ability to perform complex reasoning, enhances overall accuracy, makes training more robust by enriching the model's understanding, and provides greater transparency into the model's decision-making process.",['Retrieval Augmented Fine Tuning (RAFT)'],"Complex Question Answering, improving LLM reasoning capabilities, enhancing explainability and trustworthiness, robust model training.",9,"Prompt Design, Knowledge & Reasoning"
Retrieval-Augmented Generation (RAG),"Large Language Models (LLMs) have a knowledge cutoff date (limited by their training data), may 'hallucinate' incorrect facts, or lack specific, up-to-date, or proprietary domain expertise, making them unsuitable for applications requiring high factual accuracy or current information.","Deploying LLMs in applications where access to external, dynamic, or specialized knowledge bases is critical for generating accurate, relevant, and reliable responses. The LLM needs to integrate current or domain-specific information beyond its pre-trained knowledge.","Integrate a retrieval module with an LLM. Given a user query, the retriever fetches relevant documents, passages, or data snippets from an external knowledge source (e.g., database, document store, web search). These retrieved documents are then appended to the user's prompt as additional context, allowing the LLM to generate an informed response grounded in external evidence.","Enables LLMs to leverage external, up-to-date, and domain-specific information; significantly reduces factual errors and hallucinations; improves the relevance, accuracy, and trustworthiness of generated content; and allows LLMs to adapt to new information without full retraining.",['Retrieval Augmented Fine Tuning (RAFT)'],"Knowledge-intensive Question Answering, factual summarization, enterprise search, chatbots, content generation requiring external validation, reducing LLM hallucinations.",18,"Generative AI, LLM-specific, Knowledge & Reasoning, Tools Integration"
Retrieval-Augmented Generation for Blackbox LLMs,"Large Language Models (LLMs) tend to hallucinate and lack access to up-to-date, domain-specific, or proprietary external knowledge, especially when used as blackbox models where fine-tuning is infeasible or too costly.","Blackbox LLMs (e.g., ChatGPT) are deployed in mission-critical applications that require factual accuracy and grounding in specific external information (e.g., latest news, task-specific databases, customer reviews).","Augment the blackbox LLM with a 'Knowledge Consolidator' module. This module comprises a knowledge retriever, an entity linker, and an evidence chainer. It generates search queries, retrieves raw evidence from various external knowledge sources (web, databases), enriches it with related context, prunes irrelevant information, and forms concise evidence chains. This consolidated evidence is then injected into the LLM's prompt.","The LLM generates responses that are factually grounded in external knowledge, significantly reducing hallucinations and improving informativeness without requiring expensive fine-tuning of the LLM itself.","['Self-Correction with Automated Feedback', 'Adaptive LLM Orchestration']","Open-domain Question Answering, Information-Seeking Dialog, Customer Service, any application requiring LLMs to use external, dynamic, or private knowledge.",18,
Self-Correction with Automated Feedback,"Large Language Models (LLMs) can generate responses that do not meet desired quality criteria (e.g., factuality, coherence, alignment with user expectations or business rules) in a single pass.","Blackbox LLMs are used for tasks where high-quality, verifiable, and aligned responses are crucial, and direct programmatic control over LLM generation is limited.","Implement a 'Utility' module that evaluates candidate LLM-generated responses using task-specific utility functions (model-based or rule-based) to produce a utility score and verbalized feedback. This feedback is then used by a 'Prompt Engine' to revise the original prompt, querying the LLM again for an improved response. This process iterates until a candidate response meets the verification criteria.","LLM responses are iteratively refined and improved in terms of alignment with specific quality metrics (e.g., groundedness, factuality, informativeness), making them more suitable for mission-critical applications.","['Retrieval-Augmented Generation for Blackbox LLMs', 'Adaptive LLM Orchestration']","Information-Seeking Dialog, Customer Service, Question Answering, content generation requiring specific quality checks, scenarios where LLM output needs to conform to strict rules or factual constraints.",6,
Adaptive LLM Orchestration,"Effectively managing the interactions between a blackbox LLM, external knowledge sources, and feedback mechanisms to achieve complex, multi-step goals in a dynamic environment. Deciding when to perform which action (e.g., retrieve knowledge, query LLM, apply feedback, respond to user) is non-trivial.","Building agentic AI systems around blackbox LLMs that need to perform complex tasks, often involving multiple turns of interaction and external tool use, while optimizing for a long-term reward.","Design a 'Policy' module that, based on the current dialog state stored in 'Working Memory,' selects the optimal next system action. This policy can be rule-based (for initial bootstrapping) or trainable (e.g., using Reinforcement Learning with a neural network model like T5). Actions include acquiring evidence via the Knowledge Consolidator, calling the Prompt Engine to generate a candidate response from the LLM, or sending a verified response to the user.","The LLM-augmented system gains the ability to make intelligent, adaptive decisions about its operational flow, optimizing for desired outcomes (e.g., maximizing response quality, minimizing hallucination) and efficiently utilizing its various augmentation modules.","['Retrieval-Augmented Generation for Blackbox LLMs', 'Self-Correction with Automated Feedback']","Task-oriented Dialog Systems, Autonomous Agents, Complex Question Answering, any multi-turn AI system where dynamic decision-making for LLM and tool interaction is required.",35,
Conversable Agents,"Designing individual agents that are capable, reusable, customizable, and effective in multi-agent collaboration within LLM applications.","Building multi-agent LLM applications where agents need to interact, provide feedback, and make progress through conversation.","Implement a generic agent design that can leverage LLMs, human inputs, tools, or a combination thereof. Make agents 'conversable' so they can receive, react, and respond to messages, maintaining internal context. Allow for convenient creation of agents with different roles or responsibilities through configuration or extension.","Enables modular combination of broad LLM capabilities, allows agents to hold multi-turn conversations autonomously or with human involvement, and facilitates the creation of specialized agents for diverse tasks.","['Conversation Programming', 'Unified Conversation Interfaces and Autoreply Mechanisms', 'Composable Conversation Patterns']","General multi-agent LLM application development, role-playing scenarios, task-specific agents (e.g., AssistantAgent, UserProxyAgent).",2,
Conversation Programming,Streamlining the development of complex LLM applications that rely on multi-agent interactions and require flexible control over conversation flow and agent actions.,Developing intricate multi-agent systems where workflows are best expressed as inter-agent conversations rather than rigid sequences.,"Adopt a programming paradigm centered around inter-agent conversations. This involves two primary steps: 1) defining a set of conversable agents with specific capabilities and roles, and 2) programming the interaction behavior between agents via conversation-centric computation and control, using a fusion of natural language and code.","Simplifies and unifies complex LLM applications, facilitates intuitive reasoning about complex workflows through agent actions and message passing, and supports flexible multi-agent conversation patterns.","['Conversable Agents', 'Unified Conversation Interfaces and Autoreply Mechanisms', 'Control by Fusion of Programming and Natural Language', 'Composable Conversation Patterns']","General multi-agent LLM application development, defining complex workflows, agent interaction scripting.",2,
Unified Conversation Interfaces and Autoreply Mechanisms,"Managing the communication and control flow in multi-agent systems in a decentralized, modular, and unified way, without requiring a separate, complex control plane.","Agents needing to send, receive, and generate replies in multi-turn conversations, where the conversation flow should be naturally induced by agent interactions.","Provide low-level, unified conversation interfaces (`send/receive`, `generate_reply`, `register_reply`). Implement a default 'agent autoreply mechanism' where an agent automatically invokes `generate_reply` and sends a response after receiving a message, unless a termination condition is met. Allow registration of custom reply functions to customize agent behavior.","Enables automated agent chats, simplifies workflow definition, and supports dynamic, conversation-driven control flow.","['Conversable Agents', 'Conversation Programming', 'Composable Conversation Patterns']","Automated agent chats, dynamic workflow definition, custom agent behaviors (e.g., nested chat implementation).",2,
Control by Fusion of Programming and Natural Language,"Achieving flexible and powerful control over multi-agent conversation flow, leveraging the strengths of both human-readable natural language instructions and precise programmatic logic.","Multi-agent systems where LLM-backed agents, human inputs, and tool executions interact, requiring adaptable control mechanisms.","Provide natural language control via prompting LLM-backed agents with instructions (e.g., system messages for role-playing, debugging, output confinement). Enable programming language control using code to specify termination conditions, human input modes, tool execution logic, or custom autoreply functions. Support seamless control transition between natural language and code (e.g., LLM inference with control logic in a reply function, LLM-proposed function calls triggering code).","Enables sophisticated and adaptive control over agent interactions, allowing for dynamic adjustments and complex logic that leverages both human intuition and computational precision.","['Conversation Programming', 'Conversable Agents', 'Unified Conversation Interfaces and Autoreply Mechanisms']","Guiding agent behavior, implementing complex decision trees, dynamic debugging, integrating human oversight, LLM function calling.",2,
Composable Conversation Patterns,"Building multi-agent applications that require diverse interaction structures (static or dynamic, simple or complex) beyond basic back-and-forth dialogues.","Designing multi-agent systems for varying application complexities, such as single-turn, multi-turn, human-involved, or dynamically evolving conversations.","Provide high-level interfaces for commonly used conversation patterns like Two-Agent Chat, Sequential Chat, Nested Chat, and Group Chat. Allow these patterns to be composed recursively using low-level interfaces (like `register_reply`) to create more complex and creative patterns. Enable dynamic conversation flows through custom reply functions, speaker transition conditions, and LLM-driven function calls.","Supports a broad spectrum of application needs, from simple two-agent interactions to complex, dynamically evolving multi-agent collaborations, enhancing flexibility and generalizability.","['Conversation Programming', 'Unified Conversation Interfaces and Autoreply Mechanisms']","Math problem solving (two-agent), retrieval-augmented QA (two-agent), decision making in embodied agents (two/three-agent), supply chain optimization (three-agent), dynamic task solving (group chat), conversational chess (multi-agent); implementing inner monologues or self-reflection (nested chat).",2,
Human-in-the-Loop Multi-Agent Systems,"Effectively integrating human feedback, guidance, and oversight into multi-agent AI workflows to improve performance, ensure safety, and enable collaborative problem-solving.","Challenging tasks where fully autonomous AI systems may fail, require validation, or where human expertise is essential. Scenarios involving multiple human users collaborating with AI agents.","Configure a UserProxyAgent to solicit human inputs at specific rounds or conditions (e.g., `human_input_mode=ALWAYS`). Enable multi-user participation by designing agents (e.g., Student, Expert) that can interact with the system and with each other, mediated by AI assistants.","Enhances the system's ability to solve complex problems, incorporates human intelligence, provides guardrails, improves user experience, and allows for interactive user instructions and multi-user collaboration.","['Conversable Agents', 'Composable Conversation Patterns']","Math problem solving (correcting LLM errors, multi-user scenarios), dynamic task solving, conversational games (AI-human chess), general AI oversight.",45,
Interactive Retrieval,"Mitigating the intrinsic limitations of LLMs (e.g., hallucination, outdated knowledge, lack of domain-specific context) in retrieval-augmented generation (RAG) systems when initial retrieval is insufficient.","Question-answering or code generation tasks where LLMs need to access external, up-to-date, or private knowledge bases, and the first attempt to retrieve relevant information might not be enough.","Design the LLM-backed assistant to explicitly signal 'UPDATE CONTEXT' when it cannot find information in the current context. This signal triggers a user proxy agent (or another specialized agent) to perform additional retrieval attempts (e.g., fetching next most similar document chunks) and provide updated context to the assistant.","Dynamically updates the context for the LLM, significantly boosting performance on QA and code generation tasks, and improving the LLM's ability to provide accurate and relevant responses.",[],"Natural language question answering, code generation based on private or latest codebases.",1,
Grounding Agent,"Preventing LLM-based agents from making commonsense errors, getting stuck in repetitive error loops, or violating task-specific rules in interactive decision-making environments.","Embodied agents or game-playing AI systems operating in environments that require adherence to physical laws, common sense, or explicit game rules.","Introduce a specialized 'grounding agent' that acts as a knowledge source or rule validator. This agent provides crucial commonsense knowledge or checks proposed actions for legality, intervening dynamically (e.g., when recurring errors are detected, or an illegal move is suggested) to guide the decision-making agent.","Significantly enhances the system's ability to avoid error loops, ensures adherence to rules (e.g., game integrity), and improves decision-making by supplying necessary external knowledge.",['Executor Agent'],"Decision making in embodied agents (ALFWorld), conversational games (Chess), preventing flawed plans.",43,
Adversarial/Collaborative Agent Interactions,"Ensuring robustness, safety, and quality in complex multi-agent tasks, especially those involving creative generation (like code) where errors or unsafe outputs are possible.","Tasks requiring a combination of generation, interpretation, and critical review or validation, such as software development or optimization solution interpretation.","Design a multi-agent system where some agents work collaboratively to generate solutions (e.g., a 'Writer' for code and interpretation, a 'Commander' for coordination) while others act adversarially to check for safety, errors, or adherence to constraints (e.g., a 'Safeguard' agent for code safety). Information is shared in a controlled manner between these roles.","Boosts performance (e.g., F1 score in identifying unsafe code), reduces errors, promotes modularity, and ensures a higher quality and safer output.",['Composable Conversation Patterns'],"Supply chain optimization (code generation and safety checks), software engineering tasks.",43,
Executor Agent,Decoupling the high-level decision-making and planning logic from the low-level interaction and execution within an external environment.,"Online decision-making tasks where an AI agent needs to perform actions in a simulated or real-world environment (e.g., embodied agents, web interaction tasks).","Implement a dedicated 'executor agent' that is responsible for: 1) receiving high-level action decisions from a planning/assistant agent, 2) translating and executing these actions in the environment, and 3) reporting the environment's state, observations, or feedback back to the decision-making agent. The assistant agent then focuses solely on planning and reasoning.","Simplifies the development of decision-making agents, allows for reuse of planning logic across different environments (with different executors), and streamlines agent-environment interactions by clearly separating concerns.",['Grounding Agent'],"Decision making in embodied agents (ALFWorld), online decision making in web interaction tasks (MiniWoB), robotics.",43,
Adaptive Retrieval-Augmented Generation (AdaptiveRAG),"Retrieval-Augmented Large Language Models (LLMs) often employ a 'one-size-fits-all' strategy, which leads to inefficiencies for simple queries (unnecessary computational overhead) or inadequacies for complex, multi-step queries (failure to address them effectively). Real-world user requests exhibit a wide range of complexities, making static approaches suboptimal.","LLM-based applications, particularly in Question Answering (QA), that need to deliver accurate and efficient responses by incorporating external knowledge, where the incoming user queries vary significantly in their complexity (from straightforward to multi-hop reasoning).","A novel adaptive framework that dynamically selects the most suitable retrieval-augmented LLM strategy based on the assessed complexity of the incoming query. This selection is operationalized by a 'Query Complexity Classifier' and allows for adaptation between three core strategies: 1) A non-retrieval approach for straightforward queries, 2) A single-step retrieval approach for queries of moderate complexity requiring external knowledge, and 3) A multi-step (iterative) retrieval approach for complex queries necessitating extensive reasoning and information synthesis from multiple sources.","Enhances the overall efficiency and accuracy of QA systems by balancing computational resources with task requirements. It provides a robust middle ground, preventing unnecessary overhead for simple queries while ensuring comprehensive handling of complex ones.",Query Complexity Classifier,"Open-domain Question Answering, intelligent chatbots, dynamic resource management for LLM services, personalized AI model routing.",46,
Query Complexity Classifier,"Dynamically adapting AI system strategies (e.g., choosing between different RAG methods) requires an automated and accurate assessment of the input query's complexity. However, pre-annotated datasets for query-complexity pairs are typically unavailable, making supervised training challenging.","AI systems, such as adaptive RAG frameworks, that need to adjust their operational behavior or select appropriate sub-models based on the intrinsic difficulty or type of the input query, in scenarios where human-labeled complexity data is scarce.","Train a smaller, dedicated Language Model (Classifier) to predict the complexity level of a given query. The training dataset for this classifier is automatically constructed without human labeling by leveraging two main strategies: 1) 'Outcome-based labeling' (silver data) where queries are labeled based on the successful performance of different existing AI strategies (e.g., if a non-retrieval LLM answers correctly, it's 'straightforward'). Simpler models are prioritized in case of ties. 2) 'Inductive bias labeling' where inherent characteristics of existing benchmark datasets (e.g., queries from single-hop QA datasets are labeled 'moderate', multi-hop datasets are labeled 'complex') are used to assign labels to previously unlabeled queries.","Enables the dynamic selection of optimal AI strategies by providing an automated and resource-efficient mechanism for query complexity assessment. This leads to improved overall performance and efficiency of the adaptive AI system, and reduces the dependency on costly manual data annotation.",Adaptive Retrieval-Augmented Generation (AdaptiveRAG),"Adaptive RAG, dynamic routing in conversational AI, intelligent workload distribution for AI inference, context-aware model selection.",46,
Interpretability-Constrained Learning,"Achieving high accuracy with black-box models often sacrifices interpretability, leading to a lack of understanding and trust in critical applications.","Designing and training AI/ML models, especially for high-risk applications (e.g., healthcare, finance, criminal justice), where both high performance and inherent interpretability are non-negotiable requirements.","Integrate interpretability criteria (e.g., model size, number of nodes/rules, number of non-zero weights) directly into the model's optimization problem during training, typically as a penalty term in the loss function or as explicit constraints.","Develops models that are inherently transparent and understandable by design, while still striving for high predictive performance, overcoming the traditional accuracy-interpretability tradeoff.",[],"Healthcare, criminal justice, finance, ethical AI, applications requiring regulatory compliance (e.g., GDPR 'right to explanation').",37,
Global Surrogate Model,"Understanding the overall logic and global behavior of a complex, black-box AI model that cannot be directly inspected.","When a high-performing black-box model is deployed, and a holistic, high-level understanding of its decision-making process across the entire dataset is required, for purposes like auditing or general comprehension.","Train a simpler, inherently interpretable model (e.g., decision tree, rule-based model) on the predictions generated by the original black-box model across the entire dataset. This interpretable model acts as a 'surrogate' that mimics the black-box model's global behavior.","Provides a global, understandable approximation of the black-box model's logic, offering insights into its overall decision-making patterns.","['Local Surrogate Model', 'Local Rule-Based Explanation']","Model auditing, general model understanding, compliance, comparing overall model behaviors.",3,
Local Surrogate Model,"Explaining why a specific individual prediction was made by a complex, black-box AI model, as its global behavior may be too complex to understand or faithfully mimic.","When a detailed, instance-level explanation is needed for a black-box model's output, especially for critical decisions, without needing to understand the entire model's logic.","In the vicinity of the specific instance to be explained, generate a local dataset (e.g., through perturbed samples or nearest neighbors). Then, train a simple, interpretable model (e.g., linear model, rule-based model) on this local dataset, using the black-box model's predictions as labels, to approximate the black-box model's behavior *only in that locality*.","Provides instance-specific, understandable explanations (e.g., feature importances, local rules) that are locally faithful to the black-box model's decision for that particular instance.","['Global Surrogate Model', 'Local Rule-Based Explanation', 'Shapley Value Explanation', 'Human-in-the-Loop Explanation']","Debugging individual predictions, building trust, human-in-the-loop decision support, understanding specific decision boundaries.",3,
Shapley Value Explanation,"Quantifying the fair and consistent contribution of each feature (or group of features) to a specific individual prediction from a black-box AI model, accurately reflecting feature interactions.","When a precise, fair, and axiomatic allocation of responsibility for an individual prediction across its input features is required, especially when feature interactions are significant.","Apply the Shapley value concept from cooperative game theory, treating input features as players in a game and the prediction (or prediction probability) as the game's payout. The Shapley value for a feature is its average marginal contribution across all possible permutations (coalitions) of features.","Provides a unique, fair, and consistent attribution of an individual prediction to its input features, inherently capturing interaction effects, which can be summarized in a feature importance vector.","['Local Surrogate Model', 'Local Rule-Based Explanation', 'Divergent Subgroup Analysis']","Explaining individual predictions, fairness analysis (e.g., for bias detection), understanding feature impact, model auditing.",22,
Local Rule-Based Explanation,"Providing an easily understandable, qualitative explanation for an individual prediction of a black-box AI model, highlighting specific combinations of feature values that led to the decision.","When human users (e.g., domain experts, end-users) need clear, logical 'if-then' rules to understand the local reasons for an AI model's decision, often for debugging, trust-building, or compliance.","Derive a set of simple, human-interpretable 'if-then' rules that accurately describe the black-box model's behavior in the local neighborhood of a specific instance. These rules capture the relevant associations of attribute values with the predicted class. The rules can be extracted from a local surrogate model (e.g., an associative classifier or decision tree) trained on local data.","Offers qualitative insights into individual predictions through interpretable rules, making the decision-making process transparent at an instance level and revealing specific feature value configurations that drive a decision.","['Local Surrogate Model', 'Human-in-the-Loop Explanation']","Human-in-the-loop inspection, debugging specific predictions, trust-building, compliance (e.g., GDPR 'right to explanation').",3,
Counterfactual Explanation,"Explaining why a specific prediction was made by an AI model and, more importantly, identifying the minimal changes to the input features that would alter that prediction to a desired alternative outcome.","When users need actionable guidance on how to achieve a different AI outcome (e.g., 'What if I had done X instead of Y?'), or for auditing models for potential biases by seeing what minimal changes would flip a discriminatory decision.","Given an instance and its prediction, search for a new, synthetic instance that is as close as possible to the original instance (e.g., in feature space) but results in a different, desired prediction from the black-box model. The differences between the original and the counterfactual instance form the explanation.","Provides intuitive and actionable explanations by showing the 'what-if' scenarios, enabling users to understand decision boundaries and potentially influence future AI outcomes.",['Human-in-the-Loop Explanation'],"User guidance for decision-making, fairness auditing, understanding decision boundaries, policy making, debugging.",45,
Divergent Subgroup Analysis,"Identifying and characterizing specific data subgroups where an AI model exhibits a significantly different or anomalous behavior (e.g., higher error rates, lower accuracy, specific biases) compared to its overall performance.","When auditing AI models for fairness, validating performance across diverse populations, debugging unexpected behaviors in specific data segments, or trying to uncover hidden biases not tied to predefined protected attributes.","Systematically explore a large number of data subgroups (often defined by combinations of attribute values, i.e., patterns or itemsets). For each subgroup, quantify its 'divergence' by comparing a chosen performance metric (e.g., false positive rate, false negative rate, accuracy) within the subgroup to the same metric calculated over the entire dataset. This often involves leveraging efficient data mining techniques (like frequent pattern mining) to find sufficiently represented subgroups and statistical tests to assess significance.","Reveals critical data segments where the model performs poorly or exhibits bias, enabling targeted debugging, fairness interventions, and model improvement. It can also identify 'corrective items' that reduce divergence when added to a pattern.","['Shapley Value Explanation', 'Human-in-the-Loop Explanation']","Model validation, fairness auditing, bias detection, error analysis, targeted interventions, understanding model generalization.",45,
Human-in-the-Loop Explanation,"AI systems, especially black-box models, often lack transparency, hindering human understanding, trust, and the ability of domain experts to debug or improve them.","Designing and deploying AI systems in critical applications where continuous human oversight, validation, debugging, and collaboration with AI are essential, and where users need to actively investigate and query AI decisions.","Implement interactive interfaces and tools that allow users to: Actively query and inspect AI explanations (e.g., local rules, feature importances) for individual predictions or specific data subgroups; Perform 'what-if' analyses by interactively changing input features and observing the impact on predictions and explanations; Test their own hypotheses or user-defined rules against the model's behavior; Compare explanations across different models or for different target classes; Aggregate local explanations to gain global insights.","Fosters trust, facilitates model debugging, allows for active hypothesis testing, supports continuous learning and improvement of AI systems based on human feedback, and enables expert-AI collaboration.","['Local Rule-Based Explanation', 'Counterfactual Explanation', 'Divergent Subgroup Analysis']","Model validation, debugging, trust-building, compliance, expert-AI collaboration, active learning, policy making.",45,
RAG Knowledge Cache,"High computation and memory costs in Retrieval-Augmented Generation (RAG) systems due to long input sequences from retrieved documents, leading to redundant recomputation of Key-Value (KV) tensors for frequently accessed knowledge. Existing LLM inference optimizations do not fully leverage RAG-specific characteristics.","RAG systems where Large Language Models (LLMs) are augmented with external knowledge, resulting in long input sequences. Many requests share common retrieved documents, and document access patterns are skewed (a small fraction of documents is frequently accessed). GPU memory is limited for storing KV tensors of long sequences.",Implement a multilevel dynamic caching system (RAGCache) that stores the intermediate Key-Value (KV) tensors of retrieved documents. This cache hierarchically spans fast GPU memory (for hot documents) and slower host memory (for less frequent documents). The system reuses these cached KV tensors across multiple requests to avoid redundant computation.,"Significantly reduces Time to First Token (TTFT) by up to 4% and improves throughput by up to 21% by minimizing KV tensor recomputation. Lowers prefill latency, even with host-GPU memory transfer overhead. Outperforms state-of-the-art LLM serving systems by effectively managing KV cache for RAG's long sequences.","['Prefix-Aware KV Cache Replacement Policy (PGDSF)', 'RAG Cache-Aware Request Reordering', 'Dynamic Speculative RAG Pipelining']","Accelerating LLM inference in RAG systems, improving efficiency for knowledge-intensive NLP tasks (e.g., question answering, content creation), and optimizing resource utilization in RAG serving.",15,
Prefix-Aware KV Cache Replacement Policy (PGDSF),"Efficiently managing a hierarchical Key-Value (KV) cache for RAG documents is challenging because LLM attention mechanisms are sensitive to document order, making KV tensors context-dependent. Traditional caching policies (LRU, LFU, generic GDSF) do not account for variable document sizes, access costs (recomputation time), or the prefix-dependent nature of KV tensors, leading to suboptimal cache hit rates and evictions.","A RAG Knowledge Cache (multilevel, spanning GPU and host memory) storing KV tensors of retrieved documents. The order of documents in an LLM's input sequence affects the KV tensor values. Cache capacity is limited, necessitating intelligent eviction.","Organize document KV tensors in a 'knowledge tree' structure, where paths represent document sequences and nodes hold KV tensors, enabling sharing of common prefixes. Employ a Prefix-aware GreedyDualSizeFrequency (PGDSF) replacement policy. PGDSF calculates a priority for each node based on access frequency, KV tensor size, recency (last access time), and a prefix-aware recomputation cost. Nodes with lower priority are evicted first, with a 'swap-out-only-once' strategy to minimize GPU-host data transfer.","Maximizes cache hit rates (10-21% improvement over GDSF, LRU, LFU) and minimizes cache miss rates by making informed eviction decisions that account for LLM's prefix sensitivity and hierarchical memory characteristics. Achieves 10-29% lower average TTFT compared to baseline policies. Ensures that valuable prefixes remain in faster memory.",['RAG Knowledge Cache'],"Optimizing memory management and cache performance in RAG systems, particularly for order-sensitive LLM KV caches.",15,
RAG Cache-Aware Request Reordering,"Unpredictable arrival patterns of user requests in RAG systems can lead to inefficient cache utilization and 'cache thrashing,' where documents are frequently swapped in and out, resulting in a low cache hit rate and increased recomputation costs.","A RAG system utilizing a Key-Value (KV) cache (like the RAG Knowledge Cache) to store intermediate states of retrieved documents. Requests arrive asynchronously, and the system needs to process them in an order that maximizes cache reuse.","Implement a request scheduling algorithm that uses a priority queue to reorder incoming RAG requests. Requests are prioritized based on a metric, `OrderPriority = Cached Length / Computation Length`, which favors requests with a larger proportion of their required context already in the cache relative to the portion that needs recomputation. This strategy aims to maximize cache hits. A 'window' mechanism is used to ensure fairness and prevent request starvation.","Improves cache hit rates and reduces total computation time (by 12-21% average TTFT reduction) by strategically processing requests that can benefit most from the existing cache content. Mitigates cache volatility and enhances overall system throughput, especially under high request rates.",['RAG Knowledge Cache'],Enhancing the efficiency and throughput of RAG serving systems by optimizing the order of request processing based on cache state.,36,
Dynamic Speculative RAG Pipelining,"Sequential execution of the retrieval (CPU-bound) and LLM generation (GPU-bound) steps in RAG systems leads to significant end-to-end latency and underutilization of GPU resources during the retrieval phase. Additionally, initiating speculative generations without careful consideration can introduce unnecessary LLM computation, degrading performance under high system loads.","A RAG workflow where relevant documents are first retrieved (e.g., via vector search) and then used to augment an LLM prompt for generation. The retrieval process may produce early, incomplete results before the final set of documents is determined.","Dynamically overlap the knowledge retrieval and LLM inference steps. The vector search is broken into stages, continuously sending candidate documents to the LLM engine for speculative generation. If the newly retrieved candidates differ from previous ones, the ongoing speculative generation is terminated, and a new one begins. This speculative generation is dynamically enabled only when retrieved documents change and the number of pending LLM requests is below a predetermined maximum batch size, balancing latency reduction with computational overhead.",Reduces end-to-end latency and Time to First Token (TTFT) by up to 16% by initiating LLM inference earlier and minimizing the idle time of the GPU. Improves resource utilization by leveraging available GPU capacity during retrieval. Decreases non-overlapping vector search time by 15-43%.,['RAG Knowledge Cache'],"Accelerating RAG systems, particularly when retrieval latency is a bottleneck, by intelligently overlapping computationally distinct stages and managing speculative execution.",30,
Direct Prompting,"To obtain a basic, single-turn response from an LLM.","Used as a baseline for evaluating more complex strategies, or for straightforward tasks where extensive reasoning or tool use is not required.","The user's query is input directly into the LLM, possibly along with relevant information, without explicit instructions for intermediate steps or iterative processes.","Provides a direct response. The paper's results (Table 3) show it yields very low final pass rates for complex, multi-constraint tasks like TravelPlanner, highlighting its insufficiency.",[],"Baseline evaluation, simple information retrieval.",46,Prompt Design
Chain-of-Thought (CoT),"Large Language Models (LLMs) often struggle with complex reasoning tasks, providing direct but incorrect answers without showing their thought process.","LLMs are applied to problems requiring multi-step reasoning, logical deduction, or problem-solving, where explicit intermediate steps can aid in finding a solution.","The prompt is designed to instruct the LLM to generate a series of intermediate reasoning steps before arriving at the final answer. A common technique (Zero-Shot Chain-of-Thought, ZSCoT) involves adding a phrase like 'Let's think step by step.'","Improves the LLM's reasoning ability and capacity to handle complex problems by making the thought process transparent. In TravelPlanner, it shows a slight improvement over Direct Prompting for GPT3.5-Turbo but remains insufficient for the benchmark's complexity.","['Task Decomposition', 'ReAct (Reasoning and Acting)', 'Tree-of-Thoughts (ToT) / Graph-of-Thoughts (GoT)']","Enhancing reasoning in LLMs, complex problem-solving, planning.",9,Prompt Design
ReAct (Reasoning and Acting),"Language agents need to effectively interact with dynamic, partially observable environments by interleaving reasoning with actions and observing their outcomes to make progress.","LLM-powered agents are deployed in environments where tasks require information collection, tool use, and dynamic decision-making, such as travel planning scenarios with various search tools.","The framework guides the LLM to alternate between 'Thought' (reasoning about the current situation and next steps), 'Action' (executing a tool or interacting with the environment), and 'Observation' (receiving feedback or results from the action). This iterative cycle incorporates environmental feedback into the reasoning process.","Enhances the language agent's reasoning ability and enables effective iteration with tools for information collection. However, for highly complex, multi-constraint tasks like TravelPlanner, it is found to be insufficient, often failing to convert reasoning into correct actions or track multiple constraints.","['Tool Use / Tool Augmentation', 'Task Decomposition', 'Chain-of-Thought (CoT)', 'Reflexion (Self-Correction with Verbal Reinforcement Learning)']","Information collection, planning in dynamic environments, tool-augmented LLMs, language agents.",11,Agentic AI
Reflexion (Self-Correction with Verbal Reinforcement Learning),"Language agents often get trapped in dead loops, make invalid actions, or fail to dynamically adjust their plans based on environment feedback, indicating a struggle to identify and correct flawed reasoning.","Language agents are performing complex tasks where errors are common, and there's a need for a mechanism to learn from past failures and improve decision-making.",This approach utilizes a 'reflection model' that provides high-level insights or critiques on previous erroneous attempts. This reflective guidance aids language agents in identifying and correcting flawed reasoning and actions.,"Aims to improve the agent's ability to self-correct and dynamically adjust plans, potentially reducing persistent errors and dead loops. While showing some benefits over basic ReAct in some metrics, it is still insufficient for the multi-constraint complexity of TravelPlanner. The paper notes agents struggle to align actions with reasoning despite recognizing flaws.","['ReAct (Reasoning and Acting)', 'Chain-of-Thought (CoT)']","Error correction, improving reasoning, learning from past failures, enhancing agent robustness.",11,Agentic AI
Tool Use / Tool Augmentation,"Language agents have inherent limitations in their capabilities (e.g., knowledge cut-off, lack of real-time data, inability to perform specific computations) and need to proactively acquire necessary information from external sources.","LLM-powered agents require access to dynamic, external, or specialized information (e.g., flight schedules, restaurant details, accommodation availability) or need to interact with external systems to complete complex tasks.","Empower LLMs to interact with a 'Toolbox' of external tools (e.g., CitySearch, FlightSearch, DistanceMatrix, RestaurantSearch, AttractionSearch, AccommodationSearch). Agents are instructed to employ these tools to gather information from a partially observable environment.","Significantly expands the potential capabilities of language agents by allowing them to collect necessary information and interact with the real world. However, agents still face challenges like argument errors in tool use and getting trapped in dead loops during information collection.","['ReAct (Reasoning and Acting)', 'Memory Augmentation (Long-term and Short-term)']","Information collection, expanding LLM capabilities, interacting with dynamic environments, complex planning (e.g., travel planning).",11,Tools Integration
Memory Augmentation (Long-term and Short-term),"Language agents have limited cognitive capacity, especially for long-horizon tasks, and struggle with working memory management, leading to issues like context accumulation exceeding token limits or forgetting past information.","LLM-powered agents need to maintain context over extended interactions, store intermediate plans, and recall information to make informed decisions for multi-day itineraries or complex planning scenarios.","Integrate mechanisms for both short-term and long-term memory. Short-term memory is managed through techniques like context summarization and a 'NotebookWrite' tool to record necessary information, preventing token limit issues. Long-term memory involves parametric memory inherent in LLMs and retrieval techniques for acquiring and processing information.","Enhances the agent's ability to acquire and process information, manage working memory, and maintain context over longer tasks. However, the paper suggests that existing methods are still insufficient for the demands of complex, long-horizon planning.","['Tool Use / Tool Augmentation', 'ReAct (Reasoning and Acting)']","Managing context, storing intermediate plans, retrieving information, long-horizon tasks, conversational AI.",26,Knowledge & Reasoning
Task Decomposition,"Complex, long-horizon tasks, such as planning a multi-day itinerary with numerous interdependent decisions and constraints, are difficult for language agents to handle directly.","LLM-powered agents are faced with multi-step, intricate problems (e.g., travel planning involving places, lodging, transportation, dining) that require breaking down into more manageable parts.","Language agents are endowed with the capability to decompose complex tasks into a series of reasoned actions or sub-tasks, addressing individual components of the larger problem sequentially or hierarchically.","Enables language agents to approach and make progress on complex problems by breaking them into simpler components. However, for multi-constraint, long-horizon tasks, agents still struggle to holistically consider all constraints during decomposition and execution.","['Chain-of-Thought (CoT)', 'ReAct (Reasoning and Acting)', 'Tree-of-Thoughts (ToT) / Graph-of-Thoughts (GoT)']","Planning, problem-solving, multi-step instruction following, agentic control.",11,Planning
Tree-of-Thoughts (ToT) / Graph-of-Thoughts (GoT),"Optimizing solution searches for complex problems often requires more than linear reasoning, necessitating exploration of multiple reasoning paths and potentially backtracking.","LLM-powered agents need to find optimal solutions in scenarios that involve a large search space and complex interdependencies, where evaluating multiple alternatives is beneficial.","Employ classical data structures like trees (Tree-of-Thoughts) or graphs (Graph-of-Thoughts) to structure the search for solutions. This allows for generating and evaluating multiple candidate 'thoughts' or reasoning steps, enabling a more deliberate and potentially more optimal exploration of the problem space.","Implied to enhance planning capabilities by optimizing solution searches in fewer steps for less complex problems. The paper notes these methods were 'prohibitively costly' and thus not directly evaluated for TravelPlanner's complexity, despite their proven effectiveness in prior studies.","['Task Decomposition', 'Chain-of-Thought (CoT)', 'ReAct (Reasoning and Acting)']","Optimizing solution searches, complex reasoning, strategic problem-solving.",41,Knowledge & Reasoning
Tool Augmentation,"Large Language Models (LLMs) suffer from challenges such as hallucination, weak numerical reasoning, and a lack of up-to-date or domain-specific knowledge, as their internal knowledge is limited by their pre-training data.","LLMs are deployed for tasks requiring factual accuracy, precise numerical computation, or interaction with dynamic, external, or specialized knowledge sources.","Enhance LLMs' capabilities by integrating them with external tools. These tools can include retrieval systems for external knowledge, math tools (e.g., calculators, WolframAlpha), code interpreters (e.g., Python, SQL), and database operation tools. The LLM then leverages these tools to obtain information or perform computations that it cannot reliably do internally.","Mitigates hallucinations by providing verified external facts, improves numerical reasoning, and grants access to current, domain-specific, and dynamic information, expanding the LLM's problem-solving scope.","['Retrieval Augmentation', 'LLM as a Planner', 'Self-Correction / Feedback Loop', 'LLM as an Orchestrator']","Question Answering, Fact Checking, Mathematical Reasoning, Code Generation, Robotics, Information Retrieval.",35,
Faithful Tool-Use Evaluation Dataset Generation,"Existing evaluation methodologies for tool-augmented LLMs are often biased. LLMs might answer questions by recalling memorized pre-training information rather than genuinely utilizing external tools, making it difficult to accurately assess their true tool-use reasoning abilities.","Developing and evaluating LLMs designed to interact with external tools for knowledge acquisition, computation, or complex problem-solving, where an unbiased and precise assessment of tool-use is crucial.","Curate a benchmark dataset (like ToolQA) through a multi-phase, automated process: 1) **Reference Data Collection**: Gather diverse public corpora (text, tables, graphs) from various domains, ensuring they have minimal or no overlap with LLM pre-training data and contain context-sensitive facts. 2) **Human-Guided Question Generation**: Use LLMs to generate questions based on pre-defined, human-validated templates. These templates are designed such that the generated questions can *only* be answered by using available tools over the collected reference corpora, preventing LLM internal knowledge recall. 3) **Programmatic Answer Generation**: Implement operators corresponding to the defined tools and construct tool chains. These tool chains are then programmatically executed over the reference data to automatically generate accurate ground-truth answers for the generated questions.","A high-quality, scalable dataset that enables faithful and precise evaluation of LLMs' ability to leverage external tools for problem-solving, minimizing the influence of pre-trained knowledge and providing verifiable answers.",[],"Benchmarking tool-augmented LLMs, developing new methods for LLM tool integration, ensuring unbiased evaluation in LLM research and development.",18,
LLM as a Planner / Decomposed Planning,"LLMs struggle with complex, long-horizon tasks that require a sequence of intermediate steps, logical decomposition, or strategic action generation.","Embodied agents, multi-step question answering, or automated systems where LLMs need to interact with an environment or multiple tools to achieve a goal that cannot be solved directly.","Decompose complex tasks by leveraging the LLM's reasoning capabilities to generate a high-level plan or a sequence of intermediate reasoning steps. The LLM acts as a 'planner' that breaks down the main task into smaller, manageable subtasks, each potentially executable by a specific tool or a simpler LLM inference. This can involve techniques like Chain-of-Thought prompting or more advanced planning frameworks (e.g., Tree of Thoughts).","Improves the LLM's ability to tackle complex, multi-step tasks by providing a structured approach to problem-solving, making the process more efficient and coherent.","['Tool Augmentation', 'Self-Correction / Feedback Loop', 'LLM as an Orchestrator', 'Chain-of-Thought Prompting']","Agentic AI, Robotics, Complex Question Answering, Automated Workflow Generation, Strategic Game Playing.",0,
Self-Correction / Feedback Loop,"LLMs, especially in multi-step or tool-augmented tasks, can make errors in their reasoning, generate infeasible actions, or misinterpret observations, leading to incorrect or suboptimal outcomes.","LLMs operating as agents, interacting with external tools or dynamic environments where real-time feedback on actions is available and crucial for refining performance.","Design an iterative process where the LLM generates an action or a reasoning step, receives feedback or an observation from the environment (e.g., tool execution trace, error message, state change), and then uses this observation to reflect on its previous decision, identify errors, and generate a refined or corrected subsequent action or reasoning step. This pattern is exemplified by methods like ReAct (Reasoning and Acting) and Self-Refine.","Increases the robustness, accuracy, and adaptability of LLM-driven agents by enabling them to learn from and recover from errors, iteratively improving their problem-solving capabilities over time.","['Tool Augmentation', 'LLM as a Planner', 'LLM as an Orchestrator']","Agentic AI, Interactive Problem Solving, Debugging LLM outputs, Complex Multi-step Tasks, Robotics.",11,
LLM as an Orchestrator,"Combining multiple, diverse external tools to solve complex problems requires intelligent coordination, selection, and management of information flow among them. Manually defining tool chains is often impractical or insufficient.","LLMs needing to leverage a dynamic pool of specialized tools (e.g., text retrieval, database operations, mathematical calculators, code interpreters, graph tools) where the optimal sequence and interaction of tools are not fixed but depend on the specific task.","Employ the LLM as a central controller or orchestrator. The LLM is responsible for understanding the user's query, selecting the most appropriate tools from a given pool based on their descriptions, determining the optimal order of execution, dynamically generating arguments for tool calls, and managing the input/output flow between different tools and the LLM itself. This pattern is exemplified by methods like Chameleon.","Enables LLMs to synergize various functional tools effectively, expanding their problem-solving capabilities by composing complex workflows that adapt to task requirements, rather than being limited to single-tool usage.","['Tool Augmentation', 'LLM as a Planner', 'Self-Correction / Feedback Loop']","Multi-tool agent systems, complex scientific computing, data analysis workflows, domain-specific problem-solving, automated task execution.",35,
Retrieval Augmentation,"LLMs are prone to hallucination and may lack up-to-date, specific, or proprietary factual knowledge required for many tasks, as their internal knowledge is static and limited by their pre-training data.","Information-seeking tasks, open-domain question answering, or fact-checking where factual accuracy, currency, and grounding in verifiable external information are critical.","Augment the LLM's generation process with a retrieval mechanism. Before or during generation, a retrieval system queries an external knowledge base (e.g., text corpora, vector databases, knowledge graphs) to fetch relevant documents or facts. This retrieved information is then provided to the LLM as additional context alongside the original prompt, guiding its response generation.","Mitigates hallucinations, provides LLMs with access to external, up-to-date, and verifiable knowledge, and grounds their responses in factual evidence, leading to more accurate and reliable outputs.","['Tool Augmentation', 'Memory Augmentation']","Open-domain Question Answering, Fact Checking, Information Extraction, Timely Information Queries, Personalized Recommendations.",18,
Chain-of-Thought Prompting,"LLMs often struggle with complex reasoning tasks, providing direct answers that may be incorrect or lack transparency without revealing their intermediate thought processes.","LLMs solving reasoning-intensive problems such as mathematical word problems, multi-step logic puzzles, or tasks requiring sequential deduction, where the solution involves more than a single inference step.","Instruct the LLM, through specific prompting techniques (e.g., adding phrases like 'Let's think step by step', providing few-shot examples that include intermediate thoughts), to generate a series of explicit reasoning steps or 'thoughts' before arriving at the final answer. This encourages the LLM to break down the problem and articulate its internal reasoning process.","Significantly improves the LLM's reasoning capabilities and accuracy on complex tasks, makes its decision-making process more transparent, and can serve as a scaffold for more advanced planning or self-correction mechanisms.",['LLM as a Planner'],"Complex Question Answering, Mathematical Reasoning, Logical Deduction, Problem Solving, Code Generation with reasoning.",9,
Memory Augmentation,"LLMs have inherent limitations in their context window size, preventing them from retaining and effectively leveraging information from long documents, past interactions, or extensive knowledge bases for subsequent reasoning or task execution.","Long-running conversations, agentic systems requiring knowledge of past experiences or learned behaviors, or tasks involving information synthesis from very large corpora that exceed a single prompt's capacity.","Integrate external memory systems with LLMs. This external memory (e.g., vector databases, knowledge graphs, specialized episodic memory modules) stores and retrieves relevant past information, extending the LLM's effective context beyond its immediate input window. The LLM can write to or read from this memory, allowing it to maintain state and access a broader knowledge base over time.","Enables LLMs to maintain coherence over extended interactions, learn and adapt based on past experiences, and access a vast amount of contextual information, improving performance on long-horizon and stateful tasks.","['Tool Augmentation', 'Retrieval Augmentation', 'Self-Correction / Feedback Loop']","Conversational AI, Agentic AI, Personalized Assistants, Long-Document Understanding, Continual Learning.",26,
Tool-Integrated Reasoning Agent (TORA),Large Language Models (LLMs) struggle with complex mathematical reasoning tasks that require both abstract analysis and planning (where natural language excels) and precise computation or symbolic manipulation (where programs and external tools are superior). Existing approaches (rationale-only or program-only) are insufficient on their own.,"An AI system (specifically an LLM) needs to solve problems that demand a combination of nuanced natural language reasoning and the rigorous, efficient application of external computational or symbolic tools. This is particularly relevant for quantitative, logical, or scientific problem-solving.","Design an agent that iteratively interleaves natural language rationales with program-based tool use. The agent first generates natural language reasoning for analysis, planning, or problem decomposition. When a sub-task requires precise computation, symbolic manipulation, or algorithmic processing, the agent generates a program (code) for an external tool (e.g., a computation library, symbolic solver). The tool executes the program, and its output is then fed back to the agent. The agent uses this output to continue its natural language reasoning, adjust its approach, solve subsequent sub-tasks, or finalize the answer. This cycle repeats until the problem is solved.","Synergistically combines the analytical prowess and planning capabilities of natural language with the computational efficiency and rigor of external tools. This significantly improves performance on complex quantitative tasks, surpassing models that rely solely on natural language or program generation.","['Output Space Shaping', 'Trajectory Synthesis for Training']","Mathematical problem-solving, scientific reasoning, complex quantitative analysis, agentic tasks requiring external computation, any domain where LLMs need to interact dynamically with structured tools for precise operations.",0,
Output Space Shaping,"Training Large Language Models (LLMs) via imitation learning on limited, often single-trajectory, high-quality data can restrict the model's output space. This limitation hinders the model's flexibility in exploring diverse, plausible reasoning steps during inference and can lead to improper tool-use behavior, especially in complex, multi-step tasks.","Fine-tuning LLMs for complex reasoning tasks, particularly those involving tool use, where the initial training data (e.g., from human annotation or an expert model) may not fully capture the diversity of valid reasoning paths or sufficiently address error correction scenarios.","Enhance the model's training and output diversity by augmenting the training data beyond initial high-quality examples. This involves a multi-step process: 
1. **Sampling Diverse Valid Trajectories:** Use the current model (e.g., an imitation-learned model) to generate multiple diverse reasoning trajectories for existing training questions. Filter and retain only the trajectories that lead to correct answers and are free of tool-use errors.
2. **Teacher-Corrected Invalid Trajectories:** For trajectories that lead to incorrect answers or contain errors, identify the point of failure. Leverage a more capable 'teacher model' (e.g., a larger or more advanced LLM) to correct the subsequent portions of these partially incorrect trajectories, thereby generating valid continuations.
3. **Retraining:** Retrain the LLM on a combined dataset comprising the initial high-quality trajectories, the newly sampled valid trajectories, and the teacher-corrected valid trajectories. This broadens the model's learned output distribution and improves its robustness.","Significantly boosts the model's reasoning capabilities, encourages the generation of diverse and plausible reasoning steps, mitigates improper tool-use behavior, and improves generalization across different problem types and difficulty levels. It allows smaller models to achieve performance comparable to or exceeding larger baselines.","['Tool-Integrated Reasoning Agent', 'Trajectory Synthesis for Training']","Improving robustness and flexibility of LLMs in complex reasoning, enhancing tool-use reliability, fine-tuning agentic models, mitigating data scarcity issues in specialized domains, knowledge distillation, and improving the diversity and quality of generated outputs.",32,
Trajectory Synthesis for Training,"A significant challenge in training tool-integrated AI agents is the absence of high-quality, interactive tool-use annotations in existing datasets. These datasets typically provide only natural language rationales or final code, lacking the dynamic, interleaved interaction required for effective training.","Developing and training AI agents or LLMs that need to learn from multi-step, interactive processes involving both natural language reasoning and external tool calls, especially when creating human-annotated data for such complex interactions is prohibitively expensive or time-consuming.","Leverage a powerful, pre-trained Large Language Model (e.g., GPT-4) as an 'expert annotator' to generate a corpus of interactive tool-use trajectories. This involves: 
1. **Prompt Curation:** Designing detailed few-shot prompts that instruct the LLM to generate trajectories in a desired interleaved format (e.g., natural language rationale -> program -> tool execution output -> next rationale). These prompts include descriptive examples to guide the generation style and format.
2. **Interactive Generation:** For each problem, the LLM iteratively generates segments of the trajectory (rationale, then program, then processes tool output, then generates the next rationale) until a solution is reached.
3. **Sampling and Filtering:** Employ strategies like greedy decoding to obtain initial trajectories. For problems where greedy decoding fails, use nucleus sampling to generate multiple, diverse candidate trajectories. Retain only the trajectories that yield correct answers and are free of tool-use errors, forming a high-quality synthetic dataset.","Creates a rich, high-quality, and diverse corpus of interactive tool-use trajectories, effectively overcoming the data scarcity problem. This synthetic dataset serves as effective training data (e.g., for imitation learning) to fine-tune other LLMs or smaller models to learn complex interleaved reasoning and tool-use behaviors.","['Tool-Integrated Reasoning Agent', 'Output Space Shaping']","Bootstrapping training data for agentic LLMs, developing datasets for tool-augmented language models, creating synthetic expert demonstrations for imitation learning, enhancing ML model capabilities in domains requiring complex interactive problem-solving, and accelerating the development of specialized AI agents.",32,
Iterative Invocation Tool Learning,"LLMs struggle with complex, multi-step tasks that might fail or require adjustments based on intermediate results, leading to brittle, one-shot solutions.","LLMs need to solve complex tasks that might require multiple steps, interaction with external tools, and dynamic adjustment in real-world scenarios.","The LLM does not commit to a complete task plan upfront. Instead, it iteratively interacts with tools, adjusting subtasks progressively based on feedback (results or errors) from tool execution, refining its plan continuously.","Improves problem-solving capabilities by allowing LLMs to refine their plan continuously and address problems step-by-step, making the system more resilient and adaptive.","['Single Invocation Tool Learning', 'Task Decomposition', 'ReAct (Reasoning and Acting)', 'Error Handling for Tool Calling']","ToolLLaMA, APIBank, Confucius, RestGPT, and general agentic systems requiring dynamic plan adjustment.",7,
Single Invocation Tool Learning,"Early LLM tool integration approaches lacked adaptability to execution errors or dynamic environments, relying on a fixed, pre-planned sequence of actions.","Simpler tasks where the complete sequence of actions can be planned upfront, or where feedback from tool execution is not integrated back into planning.","Upon receiving a user question, the LLM analyzes the user intent and immediately plans all the subtasks needed to solve the problem. It then directly generates a response based on the results returned by the selected tools without considering the possibility of errors during the process or altering the plan based on tool feedback.","Provides a straightforward, one-shot solution for tool-augmented tasks, suitable for less complex problems where dynamic adaptation is not critical.","['Iterative Invocation Tool Learning', 'Task Decomposition']","Toolformer, Chameleon, HuggingGPT, and early tool learning systems.",7,
Task Decomposition,"User queries often embody complex intent that cannot be solved directly by a single tool or a simple LLM response, requiring a structured approach to problem-solving.","LLMs receive complex, multi-step user queries.","The LLM analyzes the user's intent and breaks down the complex query into multiple, potentially solvable subquestions. It also identifies dependencies and the execution sequence of these subtasks.","Facilitates the structured execution of complex tasks, enabling LLMs to address questions step-by-step and demonstrating logical analysis capabilities.","['Chain-of-Thought (CoT) Planning', 'ReAct (Reasoning and Acting)', 'Fine-tuned Task Planner', 'Iterative Invocation Tool Learning']","ART, RestGPT, HuggingGPT, ToolChain, TPTU, ControlLLM, Attention Buckets, PLUTO, ATC, SGC, Sum2Act, BTP, DRAFT, Toolformer, TaskMatrixAI, Toolink, TPTUv2, UMi, COA, DEER, OpenAGI, SOAY, TPLLAMA, APIGen.",41,
Chain-of-Thought (CoT) Planning,"LLMs struggle with complex reasoning and multi-step task planning, especially in zero-shot or few-shot settings, often leading to incorrect or incomplete plans.","LLMs are used for task planning, particularly when explicit fine-tuning is not feasible or desired.","The LLM is prompted to generate intermediate reasoning steps (a 'chain of thought') before producing the final plan or action sequence. This can involve explicit directives like 'let's think step by step,' leveraging the LLM's in-context learning abilities.","Enhances the LLM's ability to decompose complex tasks into simpler subtasks, facilitates structured action planning, and improves the quality of task plans without explicit fine-tuning.","['Task Decomposition', 'ReAct (Reasoning and Acting)', 'LLM-based Tool Selection']","CoT, ReACT, ART, RestGPT, HuggingGPT, TPTU, ToolChain, ControlLLM, Attention Buckets, PLUTO, ATC, SGC, Sum2Act, BTP, DRAFT.",41,
ReAct (Reasoning and Acting),"LLMs need to perform complex tasks that require both reasoning about the task and taking specific actions (e.g., using tools), and adapt their reasoning based on feedback from those actions.",LLMs are used as agents in environments where they need to interact with external tools or systems.,"This framework integrates reasoning (Chain-of-Thought) with actions. The LLM generates thoughts to reason about the task, decides on an action (e.g., tool call), executes it, and then observes the outcome. This feedback (observation) is then used to refine subsequent reasoning and actions.","Enhances the adaptability and decision-making capabilities of LLMs by fostering a dynamic interaction between reasoning and action, leading to more robust and accurate task execution, especially in iterative tool learning scenarios.","['Chain-of-Thought (CoT) Planning', 'Iterative Invocation Tool Learning', 'Task Decomposition', 'Error Handling for Tool Calling']","ReACT, and general agentic systems requiring dynamic interaction between reasoning and action.",11,
Fine-tuned Task Planner,"Zero-shot or few-shot task planning with LLMs, while impressive, often lacks the precision, domain-specific awareness, and overall effectiveness of models specifically trained for tool usage and task decomposition.","Developing LLM-based agents or systems that require high-performance, domain-specific, or highly accurate task planning and tool-calling capabilities.","A base LLM is fine-tuned on carefully curated datasets that include examples of task decomposition, tool usage, and API calls. This training can involve techniques like Reinforcement Learning from Human Feedback (RLHF) or behavior cloning to enhance tool awareness and capability.","Significantly enhances the LLM's awareness and capability to utilize tools effectively, improves task planning, and allows for better performance in domain-specific tasks compared to tuning-free methods.","['Task Decomposition', 'Parameter Extraction and Formatting']","Toolformer, TaskMatrixAI, Toolink, TPTUv2, UMi, COA, DEER, OpenAGI, SOAY, TPLLAMA, APIGen.",7,
Retriever-based Tool Selection,"Real-world systems often have a vast number of available tools, making it impractical to include descriptions of all tools in the LLM's input context due to length limitations and latency constraints.",LLMs need to select tools from a very large pool of available options.,"An efficient tool retrieval system is employed as a preliminary step. This system uses methods (e.g., TFIDF, BM25, SentenceBert, ANCE, TASB, Contriever, coCondensor, GNNs) to identify and filter the top-K most relevant tools for a given user query or subquestion. These top-K tools are then presented to the LLM for final selection.","Bridges the gap between broad LLM capabilities and practical input size limitations, enabling efficient and effective tool selection from vast tool sets, reducing latency and improving relevance.",['LLM-based Tool Selection'],"Gorilla, ToolLLaMA, Confucius, TPTUv2, CRAFT, ProTIP, ToolRerank, COLT.",19,
LLM-based Tool Selection,"Given a set of candidate tools, LLMs need to accurately select the most appropriate tool(s) for a specific subquestion, often requiring reasoning about tool descriptions, parameters, and potential sequential dependencies.",LLMs are provided with a limited list of candidate tools (either from a small library or pre-filtered by a retriever).,"The LLM receives the user query (or subquestion) along with the descriptions and parameter lists of the candidate tools. It then uses its reasoning capabilities (e.g., in-context learning, Chain-of-Thought, fine-tuning) to select the optimal tool(s) and, if applicable, determine the correct invocation order for serial tool calling.","Enables LLMs to make informed tool choices based on semantic understanding of the query and tool functionalities, improving the accuracy and effectiveness of tool usage.","['Retriever-based Tool Selection', 'Chain-of-Thought (CoT) Planning', 'ReAct (Reasoning and Acting)', 'Fine-tuned Task Planner']","CoT, ReACT, ToolLLaMA, Confucius, ToolBench, RestGPT, HuggingGPT, ChatCoT, ToolNet, ToolVerifier, TRICE, AnyTool, GeckOpt.",19,
Parameter Extraction and Formatting,"LLMs need to accurately extract required parameters from user queries and format them according to specific tool API specifications, while avoiding superfluous sentences or incorrect data types, to ensure successful tool invocation.","After selecting a tool, the LLM needs to invoke it by providing the correct input parameters.","The LLM processes the user query and the selected tool's documentation (including parameter specifications). It then extracts the necessary parameter content and ensures it adheres strictly to the prescribed format (e.g., data type, range, comma-separated lists). This can be achieved through few-shot prompting, rule-based methods, or fine-tuning.","Enables successful and error-free invocation of external tools, preventing common API calling errors and ensuring the tool receives valid input.",['Fine-tuned Task Planner'],"RestGPT, Reverse Chain, ControlLLM, EasyTool, ToolNet, ConAgents, Gorilla, GPT4Tools, ToolkenGPT, Themis, STE, ToolVerifier, TRICE.",27,
Error Handling for Tool Calling,"Tool invocations can frequently fail due to various reasons (e.g., incorrect parameter formatting, parameters out of range, tool server errors), which can disrupt the agent's workflow and lead to poor user experience.","LLMs are engaged in tool learning, and tool calling is an inherent part of the process where failures can occur.","Mechanisms are integrated to detect and process error messages returned upon tool calling failure. The LLM then refines its subsequent actions (e.g., re-plan, re-select tools, adjust parameters) based on this error feedback.","Enhances the resilience and adaptability of the tool learning system, ensuring continuity and efficiency even in the face of operational disruptions during tool invocation.","['Iterative Invocation Tool Learning', 'ReAct (Reasoning and Acting)']","Robust agentic systems, real-world applications with unreliable or complex APIs, systems requiring high fault tolerance.",7,
Information Integration for Response Generation,"Directly inserting raw tool outputs into an LLM's response can be impractical (due to diverse formats, length, or potential for affecting user experience) or insufficient, as LLMs need to synthesize information and integrate their own knowledge for a coherent reply.","LLMs have received outputs from external tools and need to formulate a coherent, comprehensive, and user-friendly response.","The tool outputs are incorporated into the LLM's context as input. The LLM then processes this information, synthesizes it with its internal knowledge, and generates a refined, natural-language response tailored to the user's original query. Methods to handle lengthy outputs (e.g., compression, summarization, truncation) are often employed.","Produces superior, more comprehensive, and user-friendly responses by effectively combining external data with the LLM's generative capabilities, enhancing user experience and accuracy.",['Direct Insertion for Response Generation'],"RestGPT, ToolLLaMA, ReCOMP, ConAgents, HuggingGPT.",27,
Direct Insertion for Response Generation,"Complex synthesis of tool outputs may be unnecessary for simple queries, but raw insertion can lead to less natural responses or issues if tool outputs are unpredictable.","LLMs have received outputs from tools, particularly for simple queries where the raw output is directly relevant and easily understandable.","The output of the tool is directly inserted or substituted into a pre-defined placeholder within the generated response template, often without further LLM-based synthesis.","Provides a straightforward and efficient way to integrate tool results for simple cases, but may lead to less natural or potentially confusing responses if tool outputs are complex or unexpected.",['Information Integration for Response Generation'],"TALM, Toolformer, ToolkenGPT, and early tool learning systems.",27,
Distinguish Business Logic from ML Models,"ML application systems are complex because their ML components must be retrained regularly and have an intrinsic nondeterministic behavior. Business requirements and ML algorithms change over time, making isolation difficult.","ML application systems where business logic and ML models need independent evolution, management, and debugging.",Isolate the business logic from the ML models by defining clear APIs between traditional and ML components. Place business and ML components with different responsibilities into distinct layers and separate their dataflows.,"Enables easier debugging, independent monitoring, and adjustment of ML components to meet user requirements and changing inputs without impacting the core business logic.","['ClosedLoop Intelligence', 'DataAlgorithm Serving Evaluator']","Any ML application system with outputs that depend on ML techniques, such as Chatbot systems.",13,
DataAlgorithm Serving Evaluator,"Prediction systems need to connect various data processing pipeline pieces into one coherent system, and prototyping predictive models can be challenging.",Designing and prototyping prediction systems with machine learning capabilities.,"Separate data (data source and data preparator), algorithms (serving), and evaluator components, applying an MVC-like pattern specifically for ML.","Creates a coherent and structured system for building and prototyping predictive models, improving overall system design and manageability.","['ClosedLoop Intelligence', 'Distinguish Business Logic from ML Models']","Prediction systems, systems for prototyping predictive models.",28,
Event-driven ML Microservices,"Frequent prototyping of ML models and constant changes necessitate agile development teams to build, deploy, and maintain complex data pipelines efficiently.",Development teams managing complex ML data pipelines that require high agility and responsiveness to changes.,"Construct ML data pipelines by chaining together multiple microservices, where each microservice listens for data arrival and performs its designated task.","Enhances agility in building, deploying, and maintaining complex ML data pipelines, adapting to frequent changes and prototyping needs.",['Daisy Architecture'],"Complex ML data pipelines, applications requiring agile development for ML model prototyping and deployment.",4,
ParameterServer Abstraction,Widely accepted abstractions are lacking for implementing distributed machine learning.,Developing and deploying distributed learning systems.,"Distribute both data and workloads over worker nodes, while central server nodes maintain globally shared parameters, typically represented as vectors and matrices.","Provides a structured and scalable approach for distributed machine learning training, enabling efficient parameter management across nodes.",[],"Distributed machine learning, large-scale model training.",29,
Daisy Architecture,Acquiring the ability to scale content production processes via ML and extending the coverage of ML tooling to a broader range of content.,Organizations aiming to scale ML-driven content production and improve the efficiency of ML tooling across their content.,"Utilize Kanban scaling and microservices to implement pull-based, automated, on-demand, and iterative processes for ML-driven content production.","Enables scalable, automated, and iterative ML-driven content production processes.",['Event-driven ML Microservices'],"ML-driven content production processes, systems requiring scalable and automated content generation.",4,
ClosedLoop Intelligence,"Addressing big, open-ended, time-changing, or intrinsically hard problems with machine learning.","ML systems dealing with complex, dynamic problems where continuous feedback and interaction are crucial for improvement.","Connect machine learning directly to the user, closing the feedback loop by designing clear interactions along with implicit and direct outputs.","Improves the system's ability to address complex, evolving problems by incorporating continuous user feedback and interaction into the learning process.","['Distinguish Business Logic from ML Models', 'DataAlgorithm Serving Evaluator']","Systems addressing big, open-ended, time-changing, or intrinsically hard problems where human-in-the-loop or continuous learning from interaction is beneficial.",28,
Federated Learning,"Standard machine learning approaches require centralizing training data on one machine or in a datacenter, which can lead to privacy concerns and logistical challenges.","Scenarios where data privacy, decentralization, or on-device learning is paramount, such as with mobile devices.","Employ Federated Learning, which enables mobile phones or other edge devices to collaboratively learn a shared prediction model while keeping all the training data on the device.","Facilitates privacy-preserving, decentralized machine learning by allowing models to be trained on local data without centralizing sensitive user information.",['Secure Aggregation'],"Mobile applications, privacy-sensitive ML scenarios, edge computing.",29,
ML Versioning,"ML models and their several versions may change the behavior of the overall ML applications, and ensuring reproducibility of training processes is difficult.","Managing the lifecycle and evolution of ML models within an application, where reproducibility is critical.","Record the ML model structure, the specific training data used, and details of the training system to ensure a reproducible training process.",Ensures reproducible ML training processes and provides better management and traceability of model evolution and behavior changes.,['Decouple Training Pipeline from Production Pipeline'],"Any ML application where reproducibility, model lifecycle management, and auditing are critical.",12,
Isolate and Validate Output of Model,"Machine learning models are known to be unstable, vulnerable to adversarial attacks, noise in data, and data drift over time.","Deploying ML models in production environments where robustness, security, and reliability are critical.",Encapsulate ML models within rule-based safeguards and use redundant and diverse architectures to mitigate and absorb the inherent low robustness of ML models.,"Enhances the robustness, security, and reliability of deployed ML models against various vulnerabilities, including attacks, noise, and data drift.",['Canary Model'],"Critical ML applications, models deployed in dynamic or adversarial environments, systems requiring high model integrity.",12,
Canary Model,A surrogate ML model that approximates the behavior of the best ML model must be built to provide explainability or monitor subtle changes in prediction behavior.,"Deploying new or monitoring existing ML models, especially for A/B testing, understanding behavior, or ensuring safe deployment.",Run a canary (surrogate) inference pipeline in parallel with the primary inference pipeline to continuously monitor and compare prediction differences.,"Provides a mechanism for monitoring prediction differences, aiding in model explainability, detecting regressions, and enabling safer deployment strategies.",['Isolate and Validate Output of Model'],"Model monitoring, A/B testing for ML models, explainable AI, safe model deployment.",12,
Design Holistically about Data Collection and Feature Extraction,"The system to prepare data in an ML-friendly format can easily become a 'pipeline jungle,' making management difficult and costly.","Designing and managing data pipelines for machine learning, especially for data collection and feature engineering.","Avoid complex, unmanageable 'pipeline jungles' by thinking holistically about the entire process of data collection and feature extraction from the outset.",Dramatically reduces ongoing costs and complexity by preventing the proliferation of unruly data pipelines for ML data preparation.,[],"Designing and managing ML data pipelines, MLOps data engineering.",28,
Secure Aggregation,"The system needs to communicate and aggregate model updates in a secure, efficient, scalable, and fault-tolerant way, particularly in distributed learning settings.",Federated Learning environments or other distributed ML scenarios where privacy of individual contributions to model updates must be maintained.,Encrypt data from each mobile device in Federated Learning and calculate totals and averages of model updates without individual examination of raw data.,Ensures privacy and security of individual contributions during collaborative model training while maintaining efficiency and scalability.,['Federated Learning'],"Federated Learning, privacy-preserving distributed ML, secure multi-party computation in AI.",29,
Two-Phase Predictions,"Executing large, complex ML models is time-consuming and costly, especially when lightweight clients (e.g., mobile, IoT devices) are involved.",Systems where predictions are needed on resource-constrained clients or where high-latency predictions from large models are undesirable for all requests.,"Split the prediction process into two phases: first, a simple, fast model is executed on the client, and then, optionally, a large, complex model is executed in the cloud for deeper insights.","Reduced prediction response time for some cases, decreased number of large, expensive predictions, and a fallback model available on the client even without internet connection.",[],Voice activation in AI assistants like Alexa or Google Assistant.,34,MLOps
Encapsulating ML Models within Rule-based Safeguards,"The inherent non-guarantee of correctness in ML model predictions makes them unsuitable for direct use in safety or security-related functions, and they are vulnerable to adversarial attacks, data noise, and drift.",AI systems operating in safety-critical or security-sensitive environments where the unreliability of ML models poses significant risks.,"Introduce a deterministic rule-based mechanism that evaluates and decides how to handle ML prediction results, often based on additional quality checks.","Reduced risk of negative impacts from incorrect predictions, but the architecture becomes more complex.",[],[],12,"Agentic AI, AIHuman Interaction"
AI Pipelines,Complex prediction or synthesis use cases are often difficult to accomplish effectively with a single AI tool or model.,"AI systems requiring a sequence of specialized AI processing steps to achieve a high-quality final result, such as in computer vision or complex natural language generation.","Divide the overall problem into smaller, consecutive steps. Then, combine several existing AI tools or custom models into an inference-time AI pipeline, with each specialized tool or model responsible for a single step.","Higher quality results, with the ability to optimize each step individually, but requires integrating more tools and models.",[],Typical computer vision inference pipelines.,31,"Tools Integration, Classical AI"
Ethics Credentials,"Responsible AI requirements are frequently omitted or stated as vague high-level objectives, lacking explicit, verifiable specifications as system outputs, leading to user distrust or non-adoption.","AI systems where user trust, ethical compliance, and accountability are paramount, but current methods for demonstrating these are insufficient.","Provide verifiable ethics credentials for the AI system or component, leveraging publicly accessible and trusted data infrastructure for verification. Users may also need to verify their credentials to access the system.","Increased trust and system acceptance, raised awareness of ethical issues. Requires a trusted public data infrastructure and ongoing maintenance/refresh of credentials.",[],[],48,AIHuman Interaction
Distinguish Business Logic from ML Model,"ML systems are complex due to regular retraining requirements and intrinsic non-deterministic behavior of ML components, alongside evolving business requirements and ML algorithms.","Any system integrating ML components where clear separation of concerns is needed between the stable business logic and the dynamic, evolving ML models.","Define clear APIs between traditional (business logic) and ML components. Structure the system into layers, separating business and ML components based on their distinct responsibilities, and divide data flows accordingly.",Improved manageability and adaptability of ML systems by isolating the volatile ML parts from the stable business logic.,[],[],13,MLOps
Microservice Vertical Pattern,Managing several ML inferences that need to run in a specific order or have dependencies on each other within an ML system.,"AI systems requiring sequential execution of multiple prediction models, where the output of one model serves as input for the next.","Deploy individual prediction models as separate services (servers or containers). Prediction requests are executed synchronously from top to bottom, with results gathered to form the final response to the client.",Facilitates ordered execution and dependency management for multi-stage ML inference.,[],[],40,"MLOps, Tools Integration"
Microservice Horizontal Pattern,"Workflows that can execute multiple ML predictions in parallel, or when multiple predictions need to be integrated into a single response for one request.","AI systems that benefit from parallel execution of independent prediction models to handle a single request, or where an ensemble of models is used.",Deploy multiple prediction models in parallel as separate services. A single request can be sent to these models simultaneously to acquire multiple predictions or an integrated prediction.,Enables efficient parallel inference and integration of results from multiple models for a single request.,[],[],40,"MLOps, Tools Integration"
Deploy Canary Model,"Uncertainty about whether a newly trained ML model, assumed to have better prediction quality, will perform as expected in production, and to mitigate risks of new model issues affecting all users.","MLOps environments where new ML models are frequently deployed and need rigorous, real-world validation before full rollout.","Deploy the new model alongside existing ones and route a small fraction of production requests to it. Evaluate its performance with this limited exposure. If successful, replace existing models; otherwise, improve the new model.","Minimizes user exposure to potential bugs or low-quality predictions from new models, but requires additional serving and monitoring infrastructure.",[],[],12,MLOps
Agent Profiling,"To enable autonomous agents to perform tasks by assuming specific roles (e.g., coders, teachers, domain experts) and to influence their Large Language Model (LLM) behaviors effectively.","LLM-based autonomous agents operating in various application scenarios where distinct personalities, expertise, or social interactions are required.","Indicate the agent's profiles, typically by writing this information into the prompt. Profiles can encompass basic (age, gender, career), psychological (personalities), and social information (relationships between agents). Three common strategies for creating profiles are:
1.  **Handcrafting:** Manually specifying profiles (e.g., 'you are an outgoing person').
2.  **LLM-generation:** Automatically generating profiles based on rules and optional seed examples using LLMs.
3.  **Dataset alignment:** Obtaining profiles from real-world datasets to accurately reflect attributes of a real population.","Serves as the foundation for agent design, significantly influencing the agent's memorization, planning, and action procedures, allowing for more human-like or role-specific behaviors.","['Memory-Augmented Agent', 'LLM as a Planner', 'Multi-Agent Debate']","Generative Agent (defining name, objectives, relationships), MetaGPT (predefining roles and responsibilities in software development), ChatDev, Self-collaboration, PTLLM (exploring personality traits), RecAgent (generating diverse user profiles for recommendations), American National Election Studies (assigning demographic roles to GPT-3 for social simulation).",35,
Memory-Augmented Agent,"Large Language Models (LLMs) have limited context windows, making it challenging for autonomous agents to accumulate experiences, self-evolve, maintain consistent behavior over long interactions, and perform long-range reasoning in dynamic environments.","LLM-based autonomous agents that need to store information perceived from the environment, recall past behaviors, and use recorded memories to facilitate future actions.","Equip the agent with a memory module that stores and manages information. This module often incorporates principles from human memory, such as:
*   **Hybrid Memory Structure:** Explicitly modeling short-term memory (input information within the context window) for recent perceptions and long-term memory (external vector storage, e.g., vector database) for consolidating important information over extended periods.
*   **Diverse Memory Formats:** Storing information in natural language, embeddings (for retrieval efficiency), databases (for efficient manipulation via SQL), or structured lists (e.g., hierarchical trees for goals/plans).
*   **Memory Operations:** Implementing mechanisms for:
    *   **Memory Reading:** Extracting meaningful information based on criteria like recency, relevance, and importance.
    *   **Memory Writing:** Storing new information, addressing memory duplication (e.g., condensing similar sequences, count accumulation) and overflow (e.g., explicit deletion, FIFO overwriting).
    *   **Memory Reflection:** Emulating human ability to independently summarize past experiences into broader, more abstract insights and infer high-level information.","Enhances the agent's ability to perceive recent contexts, accumulate valuable experiences, perform long-range reasoning, and behave in a more consistent, reasonable, and effective manner in complex environments.","['Agent Profiling', 'LLM as a Planner', 'Planning with Feedback', 'Self-Driven Evolution', 'Trial-and-Error Learning Agent', 'Tool-Augmented LLM Agent']","Generative Agent, AgentSims, Reflexion, GITM (Ghost in the Minecraft), SCM, SimplyRetrieve, MemorySandbox, MemoryBank, ChatDB, DBGPT, RETLLM, Voyager.",26,
LLM as a Planner,"Autonomous agents need to break down complex, long-horizon tasks into simpler subtasks and generate effective action plans, which is difficult to do directly and often requires human-like strategic thinking.",LLM-based autonomous agents facing complex tasks that require multi-step reasoning and decomposition to achieve a final goal.,"Leverage the inherent reasoning capabilities of Large Language Models (LLMs) to generate plans. This can be achieved through:
*   **Single-path Reasoning:** Decomposing the final task into a sequence of intermediate steps, where each step leads to a single subsequent step (e.g., Chain of Thought prompting).
*   **Multi-path Reasoning:** Organizing reasoning steps into a tree-like or graph structure, allowing for exploration of multiple choices or 'thoughts' at each intermediate step, with selection based on LLM evaluation (e.g., Tree of Thoughts, Self-consistent CoT).
*   **External Planner Integration:** Transforming task descriptions into formal planning languages (e.g., PDDL) for external, specialized planners, which then rapidly identify optimal action sequences, with LLMs converting results back to natural language.","Empowers agents with human-like planning capabilities, enabling them to behave more reasonably, powerfully, and reliably, especially for tasks that benefit from strategic decomposition and exploration of alternatives.","['Agent Profiling', 'Memory-Augmented Agent', 'Planning with Feedback', 'Tool-Augmented LLM Agent', 'Grounded Replanning']","Chain of Thought (CoT), ZeroshotCoT, RePrompting, ReWOO, HuggingGPT, Self-consistent CoT (CoTSC), Tree of Thoughts (ToT), RecMind, GoT, AoT, RAP, LLMP, LLMDP, COLLM, DEPS, GITM, Voyager.",0,
Planning with Feedback,"Generating a flawless plan directly from the beginning is extremely difficult for complex, long-horizon tasks, as it requires considering various complex preconditions and is susceptible to unpredictable environmental dynamics, leading to initial plans becoming non-executable.","LLM-based autonomous agents operating in dynamic, real-world, or complex simulated environments where plans need to be iteratively made and revised based on real-time information.","Design planning modules where the agent can receive and incorporate feedback after taking actions to iteratively revise its plans. Feedback can originate from:
*   **Environmental Feedback:** Objective signals from the world or virtual environment, such as task completion signals, observations after an action, execution errors, or scene graph updates.
*   **Human Feedback:** Subjective signals from human users to align agent plans with human values, preferences, and to mitigate hallucination problems.
*   **Model Feedback:** Internal feedback generated by the agents themselves or other pretrained models (e.g., self-refine mechanisms, evaluation modules, detailed verbal critiques) on the agent's output or reasoning steps.","Enables agents to create more adaptive, robust, and effective plans, recover from unexpected situations, and address complex tasks involving long-range reasoning by simulating human-like iterative planning.","['LLM as a Planner', 'Memory-Augmented Agent', 'Grounded Replanning', 'Trial-and-Error Learning Agent', 'Self-Driven Evolution']","ReAct, Voyager, Ghost, SayPlan, DEPS, LLMPlanner, Inner Monologue, Self-refine, SelfCheck, InterAct, ChatCoT, Reflexion.",16,
Grounded Replanning,"LLM-generated plans may become invalid or non-executable during execution in real-world or embodied environments due to discrepancies between the planned state and the actual environment (e.g., object mismatches, unattainable plans, unexpected dynamics).",Embodied agents or agents interacting with physical/simulated environments where plans require dynamic adjustment based on real-time observations and environmental changes.,"Implement an algorithm that dynamically updates or revises existing LLM-generated plans when environmental discrepancies or execution failures are encountered. This involves perceiving the current environment state, identifying mismatches with the plan, and re-planning from the updated, 'grounded' state.","Improves the robustness, adaptability, and success rate of agents in dynamic and uncertain environments, allowing them to recover from unexpected situations and complete long-horizon tasks.","['LLM as a Planner', 'Planning with Feedback', 'Trial-and-Error Learning Agent']","LLMPlanner, Inner Monologue (by adjusting strategies based on scene descriptions and human feedback).",16,
Tool-Augmented LLM Agent,"Large Language Models (LLMs) may lack comprehensive expert knowledge for specific domains, encounter hallucination problems, or be unable to directly interact with external systems or perform complex computations required for tasks.","LLM-based autonomous agents needing to expand their action space and capabilities beyond the internal knowledge of the LLM to address domain-specific challenges, access real-time information, or perform precise operations.","Empower agents with the capability to call and integrate external tools into their action module. These tools can include:
*   **APIs:** Leveraging external APIs (e.g., HuggingFace models, Python interpreters, LaTeX compilers, RESTful APIs) to execute sophisticated computations, extract relevant content, or generate specific inputs for other systems.
*   **Databases/Knowledge Bases:** Integrating external databases or knowledge bases (e.g., using SQL statements) to obtain specific domain information for generating more realistic and accurate actions.
*   **External Models:** Utilizing other specialized pretrained models (e.g., language models for text retrieval, Codex for code generation, multimodal models for video/image/audio processing) to handle more complex or multimodal tasks.","Greatly expands the agent's operational scope, mitigates LLM limitations (like hallucination and lack of domain expertise), and enables interaction with diverse real-world systems and data sources.","['LLM as a Planner', 'Memory-Augmented Agent', 'Planning with Feedback']","HuggingGPT, TPTU, Gorilla, ToolFormer, APIBank, ToolBench, RestGPT, TaskMatrixAI, MRKL, OpenAGI, MemoryBank, ViperGPT, ChemCrow, MMREACT, ChatDB, DBGPT, DEPS, Voyager, GITM, ChatDev, MetaGPT, Self-collaboration.",35,
Multi-Agent Debate,"Individual LLM-based agents may produce inconsistent, incomplete, or suboptimal responses to complex questions or tasks, and a single agent's perspective might be limited.","Scenarios where multiple LLM-based agents can collaborate to solve a problem or refine a decision, leveraging collective intelligence.","Design a mechanism where different agents provide separate responses or solutions to a given question or subtask. If their responses are not consistent, they are prompted to incorporate the solutions or opinions from other agents and provide an updated, refined response. This iterative process continues until a final consensus answer is reached or the quality of the solution is significantly improved.","Enhances the capability and robustness of each agent by leveraging the 'wisdom of crowds,' leading to more consistent, comprehensive, and accurate outcomes that benefit from diverse perspectives and iterative refinement.","['Agent Profiling', 'Self-Driven Evolution', 'Planning with Feedback']","91 (debating mechanism), RoCo (multirobot collaboration where agents discuss and improve plans), ChatEval (LLMs critique and assess model results in a debate format), ChatDev (agents communicate to collectively accomplish tasks), MetaGPT (roles supervise code generation).",42,
Self-Driven Evolution,"Enabling LLM-based agents to autonomously learn, adapt, and improve their capabilities over time in open-ended or long-term interactive environments, without explicit, pre-defined curricula or continuous human intervention.","Autonomous agents operating in dynamic environments where continuous learning, goal-setting, and adaptation are crucial for long-term effectiveness and acquiring new knowledge/skills.","Design agents with mechanisms that allow them to:
1.  **Autonomously Set Goals:** Define their own objectives based on their current state and environment.
2.  **Explore and Act:** Interact with the environment to gather information and perform actions.
3.  **Receive Feedback:** Obtain feedback (e.g., from a reward function, environmental observations, or other agents) on the outcomes of their actions.
4.  **Gradually Improve:** Leverage this feedback and accumulated experience to refine their strategies, acquire new knowledge, develop new skills, or adapt their roles and relationships. This process can involve internal reflection and communication within multi-agent systems.","Agents acquire knowledge and develop capabilities according to their own preferences or environmental demands, leading to greater autonomy, adaptability, and generalized performance in complex scenarios.","['Memory-Augmented Agent', 'Planning with Feedback', 'Trial-and-Error Learning Agent', 'Multi-Agent Debate']","LMA3 (autonomously set goals, explore, receive reward feedback), SALLMMS (multi-agent system adapts and performs complex tasks via communication), CLMTWA (teacher LLM improves student LLM's reasoning), NLSOM (agents communicate and collaborate, dynamic role adjustment), Voyager (skill execution code refinement), AppAgent (constructs knowledge base via exploration and observation).",10,
Trial-and-Error Learning Agent,Agents need to learn from failures and adapt their plans or actions in dynamic environments where a perfect initial plan is often unattainable and unexpected situations arise.,LLM-based agents performing actions in an environment where the outcomes can be objectively or subjectively judged as satisfactory or unsatisfactory.,"Implement a cycle where the agent first performs an action or proposes a plan. A predefined critic (which can be the environment, a human, or another model/LLM) then evaluates the outcome. If the action or plan is deemed unsatisfactory (e.g., fails, differs from desired outcome), the critic generates specific feedback (e.g., failure information, detailed reasons for failure). This feedback is then incorporated by the agent to react, redesign its plan, or iteratively refine its subsequent actions or strategies.","Enables agents to acquire adaptive behaviors, correct planning errors, and improve performance over time by learning directly from the consequences of their interactions and the detailed feedback received.","['Planning with Feedback', 'Memory-Augmented Agent', 'Self-Driven Evolution', 'Grounded Replanning']","RAH (user assistant compares predicted response with human feedback), DEPS (explainer generates details for plan failure), RoCo (plan and waypoints validated by environment checks, feedback appended to prompt for discussion), PREFER (LLMs generate feedback on reasons for failure to refine actions).",16,
Loan Approval Solution Pattern,"Organizations struggle to effectively apply machine learning to complex business decisions like loan approval. This involves challenges in translating business needs into well-defined ML problems, selecting appropriate algorithms, configuring them, evaluating their performance, and managing quality requirements and data preparation.",A business process where a 'Loan expert' needs to make 'Decision on Loan Applications' and desires an 'ML-generated recommendation on a new case given past decisions'. The process requires a structured approach to integrate ML effectively.,"The pattern provides a structured ML design that:
1.  **Translates Business to ML:** Links business actors and decisions to specific ML 'Question Goals' (e.g., 'What would be the approval decision?') which are answered by 'Insight' elements (e.g., a 'Predictive model').
2.  **Guides ML Algorithm Selection:** Defines 'Analytics Goals' (e.g., 'Prediction' leading to 'Classification') and links them to alternative 'Algorithms' (e.g., kNearest Neighbor, Naive Bayes, Support Vector Machines).
3.  **Applies Contextual Guidance:** Incorporates 'User Contexts' (e.g., 'Users desire simplicity'), 'Data Contexts' (e.g., 'Features are independent'), and 'Model Contexts' (e.g., 'Decrease parameter C when dataset is noisy') to inform algorithm applicability and configuration.
4.  **Manages Evaluation & Non-Functional Requirements (NFRs):** Specifies 'Indicators' (e.g., Accuracy, Precision) for model evaluation, with 'Data Contexts' guiding metric choice (e.g., 'Precision should be used...when Users desire a low rate of false-positives'). It also considers 'Softgoals' (NFRs, e.g., 'Tolerance to missing values') and how algorithms contribute to or detract from them.
5.  **Directs Data Preparation:** Identifies relevant 'Entities' (data attributes, e.g., Age, Income, Loan amount) and 'Operators' for 'Data Preparation Tasks' (e.g., 'Perform data normalization on numerical features when using kMeans algorithm').","Provides a reusable, context-aware ML design that streamlines the development of loan approval systems. It ensures appropriate algorithm selection, configuration, data preparation, and evaluation, aligning with business needs and quality requirements, thereby reducing development complexity and time.","['Fraud Detection Solution Pattern', 'Task Assignment Solution Pattern']","Loan application approval systems, credit scoring, risk assessment in financial services, any business process requiring ML-driven binary classification decisions.",33,"['Classical AI', 'MLOps', 'Knowledge & Reasoning']"
Fraud Detection Solution Pattern,"Detecting fraudulent activities (e.g., in insurance claims) is challenging due to the inherent class imbalance in fraud datasets and the need to adapt ML approaches based on the type and availability of training data (e.g., presence of both fraud and non-fraud samples vs. only non-fraud samples).","Business processes involving the detection of anomalies or unusual patterns, such as identifying fraudulent insurance claims, where the characteristics of available training data significantly influence the choice and effectiveness of the ML approach.","This pattern guides the design of fraud detection systems by:
1.  **Focusing on Anomaly Detection:** Employs 'Anomaly Detection' as the primary 'Analytics Goal' for identifying unusual claims.
2.  **Adapting to Data Availability:** Provides guidance on selecting between supervised and semi-supervised anomaly detection approaches based on whether both fraud and non-fraud samples are available, or if only non-fraud examples are present.
3.  **Addressing NFRs:** Incorporates 'Softgoals' such as 'Tolerance to redundant attributes' and recommends specific ML techniques (e.g., neural networks) that are known to address these quality requirements.
4.  **Structured ML Design:** Adheres to the general 'solution pattern' framework, detailing relevant algorithms, metrics, contextual guidance (User, Data, Model Contexts), and data preparation steps specifically tailored for fraud detection scenarios.",Enables efficient and effective development of fraud detection systems by providing a structured design that adapts the ML approach based on data characteristics and prioritizes quality requirements. This reduces exploration and experimentation efforts and improves the reliability of fraud detection.,"['Loan Approval Solution Pattern', 'Task Assignment Solution Pattern']","Fraud detection in banking, insurance, and cybersecurity; anomaly detection in network intrusion, manufacturing quality control, and system health monitoring.",38,"['Classical AI', 'MLOps', 'Knowledge & Reasoning']"
Task Assignment Solution Pattern,"Developing ML solutions for task assignment involves challenges related to effective data preparation, ensuring efficient learning, and managing computational resources, especially when dealing with high-dimensional data or the need for fast model execution.","Business processes that require automated or ML-assisted task allocation, where factors like data efficiency, learning speed, and algorithm complexity are critical considerations for successful deployment and operation.","The pattern provides a structured design for ML-driven task assignment by:
1.  **Emphasizing Advanced Data Preparation:** Explicitly includes 'Data Preparation Tasks' like 'Dimensionality Reduction' to optimize complex datasets for ML algorithms, improving efficiency and performance.
2.  **Facilitating Fine-Grained Algorithm Decomposition:** Illustrates how complex algorithms (e.g., kNearest Neighbor) can be broken down into 'finer-grain tasks', offering more detailed guidance for their implementation and optimization.
3.  **Prioritizing Performance:** Integrates 'Softgoals' such as 'Speed of learning', linking algorithm choices and data preparation steps directly to their impact on these critical performance requirements.
4.  **Structured ML Design:** Adheres to the overall 'solution pattern' framework, integrating business goals, ML algorithms, contextual factors, and quality attributes to guide the comprehensive design of task assignment ML systems.","Facilitates the creation of robust and performant ML systems for task assignment by providing clear guidance on data preparation, algorithm breakdown, and ensuring that design choices are aligned with critical performance and quality objectives. This leads to more efficient and effective task allocation solutions.","['Loan Approval Solution Pattern', 'Fraud Detection Solution Pattern']","Workforce management, resource allocation, intelligent routing systems, project management task distribution, scheduling systems.",33,"['Classical AI', 'MLOps', 'Knowledge & Reasoning']"
Agentic Web Browser Interaction,"Large Language Models (LLMs) inherently lack access to up-to-date information, struggle with factual accuracy, and are unable to perform complex, multi-step information-seeking tasks that require real-time interaction with external tools like the web.","An LLM needs to answer open-ended, long-form questions that demand current information retrieval and synthesis from the web, beyond its pre-training data.","Design a text-based web-browsing environment that the LLM can interact with. The LLM is finetuned to act as an agent within this environment, receiving contextual prompts (e.g., the question, the current page's text, past actions) and issuing discrete commands (e.g., 'Search query', 'Clicked on link', 'Scrolled down', 'Quote text', 'End Answer') to navigate the web, retrieve information via an external search engine (like the Bing API), and synthesize a comprehensive answer.","Enables the LLM to perform complex information-seeking tasks, access up-to-date information, significantly improve factual accuracy, and generate comprehensive, referenced long-form answers. This approach allows for end-to-end improvement of both information retrieval and synthesis.","['Behavior Cloning (BC)', 'Reward Modeling (RM)', 'Reinforcement Learning from Human Feedback (RLHF)', 'Rejection Sampling (Best-of-N)', 'Reference Collection for Factual Accuracy']","Long-form question answering, fact-checking, knowledge acquisition for LLMs, autonomous web agents, interactive information retrieval.",1,
Behavior Cloning (BC),"Training an agent (LLM) to perform complex, multi-step tasks in an environment (like web browsing) where direct reward signals are sparse, difficult to define, or where a foundational policy is needed to mimic human-like interaction.","An LLM needs to learn how to use a text-based web browsing environment by mimicking human behavior to answer long-form questions, establishing a baseline for complex interactive tasks.","Collect human demonstrations of the full task, including sequences of browsing actions (commands issued in the environment) and the final answer. Finetune the LLM using supervised learning, where the human commands and generated text serve as labels for the model's actions.","The LLM acquires a foundational policy to interact with the environment and perform the task, learning to use the available tools and generate initial answers in a human-like manner. This provides a strong starting point for further optimization.","['Agentic Web Browser Interaction', 'Reward Modeling (RM)', 'Reinforcement Learning from Human Feedback (RLHF)', 'Rejection Sampling (Best-of-N)']","Initial training for agentic LLMs, learning complex human-like behaviors, bootstrapping reinforcement learning agents, acquiring basic tool-use capabilities.",17,
Reward Modeling (RM),"Quantifying the subjective quality of an AI's outputs (e.g., long-form answers, browsing trajectories) in a way that aligns with human preferences, especially when objective metrics are insufficient for criteria like factual accuracy, coherence, and overall usefulness.","An LLM generates outputs (e.g., answers with references) that need to be evaluated and optimized based on human judgments of quality. Human feedback is available in the form of pairwise comparisons.","Collect human comparisons where labelers express preference between two model-generated answers (each with its collected references) to the same question. Train a separate neural network (the reward model) to take a question and an answer with references as input and output a scalar reward (e.g., an Elo score) that predicts human preferences using a cross-entropy loss over the comparison labels.","A quantifiable proxy for human preferences that can be used for subsequent optimization (e.g., Reinforcement Learning, Rejection Sampling) and evaluation, effectively aligning the AI's quality assessment with human values and subjective criteria.","['Behavior Cloning (BC)', 'Reinforcement Learning from Human Feedback (RLHF)', 'Rejection Sampling (Best-of-N)', 'Reference Collection for Factual Accuracy']","Training for Reinforcement Learning from Human Feedback, evaluating generative models, fine-tuning for subjective quality, aligning LLMs with human values, creating preference-based metrics.",39,
Reinforcement Learning from Human Feedback (RLHF),"Improving an AI's ability to perform complex, sequential tasks in an environment or generate outputs where desired outcomes are subjective, difficult to programmatically define, and simple behavior cloning may not reach optimal performance. A key challenge is mitigating over-optimization of the potentially imperfect reward model and preventing policy divergence.",An LLM has learned basic interaction with an environment (via Behavior Cloning) and a Reward Model exists to quantify human preferences. The goal is to further improve the agent's policy to maximize human-preferred outcomes while maintaining reasonable behavior and preventing the policy from straying too far from the initial learned policy.,"Use a reinforcement learning algorithm (such as Proximal Policy Optimization - PPO) to finetune the behavior-cloned model. The reward signal for the RL agent during training is primarily derived from the reward model's score at the end of each episode. To prevent over-optimization and maintain stability, a KL divergence penalty from the original BC model (or a reference policy) is added at each token to the reward, regularizing the policy's updates.","The AI agent learns to generate outputs and/or perform actions that are highly preferred by humans, often surpassing the performance of models trained solely on demonstrations. This method improves robustness and alignment with human values while controlling for unwanted policy shifts.","['Agentic Web Browser Interaction', 'Behavior Cloning (BC)', 'Reward Modeling (RM)', 'Rejection Sampling (Best-of-N)']","Aligning LLMs with human values, training agents for complex tasks, improving generative model quality, reducing harmful or unhelpful outputs, fine-tuning interactive agents.",6,
Rejection Sampling (Best-of-N),"Improving the quality and consistency of AI-generated outputs, especially when the underlying generative model's single-shot output is not consistently high quality, without requiring additional training of the generative policy. The environment can be unpredictable, making direct policy optimization challenging.","A generative model (e.g., an LLM trained via Behavior Cloning or Reinforcement Learning) can produce multiple diverse outputs for a given input, and a Reward Model is available to score these outputs based on desired criteria (e.g., factual accuracy, coherence, human preference).","Generate 'N' diverse samples (e.g., long-form answers, potentially with their associated browsing trajectories) from the generative model for a given input. Use a pre-trained reward model to score each of these N samples based on human preferences. Select the sample with the highest reward score as the final output. This method trades increased inference-time compute for improved output quality.","Significantly improves the quality, factual accuracy, and alignment of the final output by leveraging the diversity of multiple samples and the discriminative power of the reward model. This approach can even outperform direct RL-trained policies in some cases by allowing the model to 'try' many more options (e.g., visiting more websites) and evaluate information with hindsight.","['Reward Modeling (RM)', 'Behavior Cloning (BC)', 'Reinforcement Learning from Human Feedback (RLHF)']","Enhancing generative model performance, improving factual accuracy, reducing hallucinations, aligning outputs with preferences, post-processing for LLMs, especially when inference-time compute is available.",39,
Reference Collection for Factual Accuracy,"Evaluating the factual accuracy of AI-generated long-form answers is difficult, subjective, and requires independent research from human labelers, leading to noisy, expensive, and less transparent feedback. AI models often 'hallucinate' or provide unverifiable information.","An AI system (e.g., an agentic LLM browsing the web) generates long-form answers that are intended to be factually accurate and need to be evaluated by humans (for training data collection or end-user consumption).","Design the AI to actively identify, extract, and 'quote' specific source passages (references) from the web pages it consults while synthesizing its answer. These collected references (including page title, domain name, and the extracted text) are then presented alongside the final answer to human evaluators and/or end-users.","Enables more accurate, less noisy, and transparent human feedback on factual accuracy, as labelers can directly assess whether claims in the answer are supported by the provided references without independent research. This also increases trust and allows end-users to verify information, and contributes to reducing non-imitative falsehoods (hallucinations) by grounding the answer in retrieved text.","['Agentic Web Browser Interaction', 'Reward Modeling (RM)', 'Reinforcement Learning from Human Feedback (RLHF)']","Improving factuality in LLM-generated content, enhancing transparency, facilitating human evaluation of information-seeking agents, reducing hallucinations, building trust in AI-generated information.",18,
ReAct (Reasoning and Acting),"Large Language Models (LLMs) struggle with complex, long-horizon tasks that require both dynamic reasoning (e.g., planning, tracking progress, handling exceptions) and grounded interaction with external environments (e.g., gathering information, updating facts). Reasoning-only methods (like Chain-of-Thought) can suffer from hallucination and error propagation due to lack of external grounding, while action-only methods lack abstract reasoning and high-level goal maintenance, leading to issues like repeating actions or failing to comprehend context.","An agent, powered by a Large Language Model (LLM), interacts with an environment for task solving. The LLM is prompted with few-shot in-context examples to guide its behavior. Environments can include knowledge bases (like Wikipedia API), text-based games (ALFWorld), or web navigation interfaces (WebShop).","The agent's action space is augmented to include both 'actions' (domain-specific interactions with the external environment) and 'thoughts' (free-form language reasoning traces). These thoughts and actions are generated in an interleaved manner. Thoughts compose useful information by reasoning over the current context to support future reasoning or acting (reason to act), while actions interface with external sources to incorporate additional information into reasoning (act to reason).","Synergizes reasoning and acting, leading to superior performance, improved human interpretability, trustworthiness, and diagnosability across diverse tasks. It effectively overcomes hallucination and error propagation issues common in reasoning-only methods and enhances abstract reasoning for action-only methods. The approach is generalizable, robust, and benefits from finetuning.","['Chain-of-Thought (CoT) Prompting', 'Act-Only', 'Inner Monologue', 'LLM as a Planner (SayCan)', 'Combining Internal and External Knowledge']","['Multihop Question Answering (HotpotQA)', 'Fact Verification (FEVER)', 'Text-based Games (ALFWorld)', 'Web Navigation (WebShop)']",11,
Chain-of-Thought (CoT) Prompting,"Large Language Models (LLMs) often struggle with complex reasoning tasks (e.g., arithmetic, commonsense, symbolic reasoning) that require multiple steps of inference, often producing direct, unreasoned answers that can be incorrect.",Large Language Models (LLMs) are used to solve problems requiring multi-step reasoning.,"The LLM is prompted to generate intermediate reasoning steps, a 'chain of thought,' before producing the final answer. This explicit step-by-step reasoning process is included in the prompt as part of the in-context examples.","Elicits emergent reasoning capabilities in LLMs, improving their performance on complex tasks. However, it is a static, internal process not grounded in external information, which can lead to issues like fact hallucination and error propagation.","['Self-Consistency (CoTSC)', 'ReAct (Reasoning and Acting)', 'Least-to-Most Prompting', 'Selection-Inference', 'Faithful Reasoning']","['Arithmetic reasoning', 'Commonsense reasoning', 'Symbolic reasoning tasks', 'Question Answering', 'Fact Verification']",9,
Self-Consistency (CoTSC),"Chain-of-Thought (CoT) reasoning, while effective, can be brittle, and a single reasoning path might lead to an incorrect answer. The model might generate different, equally plausible reasoning paths, but only one is correct.",Applying Chain-of-Thought (CoT) prompting with Large Language Models (LLMs) for complex reasoning tasks.,"Instead of relying on a single CoT trajectory, multiple diverse reasoning paths are sampled from the LLM (e.g., by using a non-zero decoding temperature during inference). The final answer is then determined by aggregating the results, typically by taking the majority vote among the answers derived from these different reasoning paths.",Consistently boosts performance over vanilla CoT by leveraging the principle that a correct answer is more likely to be consistently derived through multiple distinct reasoning paths. Improves the robustness of CoT reasoning.,"['Chain-of-Thought (CoT) Prompting', 'Combining Internal and External Knowledge']","['Enhancing Chain-of-Thought reasoning', 'Question Answering', 'Fact Verification']",9,
Combining Internal and External Knowledge,"Large Language Models (LLMs) possess strong internal knowledge and reasoning capabilities (e.g., via Chain-of-Thought), but this can lead to hallucinated facts or thoughts. Conversely, external knowledge retrieval (e.g., via ReAct) provides factual grounding but can be inflexible or suffer from non-informative search results, derailing reasoning.","Tasks that require both robust internal reasoning and accurate, up-to-date factual grounding from external sources.","Strategically combine methods that leverage LLM internal knowledge (like Self-Consistency with Chain-of-Thought, CoTSC) with methods that integrate external knowledge and action (like ReAct). Heuristics are used to decide when to switch between methods. For example, if ReAct fails to produce an answer within a set number of steps, back off to CoTSC. Alternatively, if CoTSC's majority answer is not confident (e.g., occurs less than n/2 times), back off to ReAct.","Achieves superior performance by leveraging the complementary strengths of both internal and external reasoning. This approach leads to more factual and robust problem-solving, mitigating the individual limitations of each method.","['ReAct (Reasoning and Acting)', 'Self-Consistency (CoTSC)', 'Chain-of-Thought (CoT) Prompting']","['Knowledge-intensive Question Answering (HotpotQA)', 'Fact Verification (FEVER)']",9,
LLM as a Planner (SayCan),"Embodied agents need to translate high-level language goals into a sequence of executable actions in a physical environment, which is a complex planning problem.","Robotic systems and embodied agents operating in physical or simulated environments, receiving high-level language instructions.","Large Language Models (LLMs) are used to directly predict a set of possible actions that a robot can take to achieve a given goal. These LLM-generated actions are then reranked by an affordance model, which grounds the linguistic actions in the visual and physical capabilities of the robot and the environment, for final action selection.","Enables LLMs to effectively guide robotic actions, bridging the gap between high-level language instructions and low-level physical execution, thereby grounding language plans in the real world.","['Act-Only', 'Inner Monologue', 'ReAct (Reasoning and Acting)']","['Robotic action planning', 'Embodied AI']",0,
Inner Monologue,"Embodied agents need a mechanism to motivate their actions, track progress, and react to environmental observations to achieve their goals, beyond just predicting raw actions.",Embodied agents interacting in dynamic environments where actions need to be guided by internal state and external feedback.,Actions of an embodied agent are motivated by an 'inner monologue' which is implemented as injected feedback from the environment. This monologue is specifically designed to reiterate observations of the environment state and what needs to be completed by the agent for the goal to be satisfied.,"Enables a closed-loop system for embodied reasoning, providing a form of internal feedback to guide agent behavior. It allows agents to react to the environment and track task completion, though it is noted to be more limited in flexibility and reasoning types compared to 'ReAct's' reasoning traces.","['ReAct (Reasoning and Acting)', 'LLM as a Planner (SayCan)']",['Embodied reasoning and planning'],11,
Self-Taught Reasoner (STaR),"Improving Large Language Model (LLM) reasoning often requires finetuning on correct rationales, but manually annotating high-quality rationales at scale is expensive and time-consuming.","Finetuning or training LLMs for tasks that benefit from explicit reasoning traces, such as complex question answering or problem-solving.","A bootstrapping approach where the model generates its own rationales. The LLM first generates a rationale and an answer. A verifier (which can be a simple check or another model) then evaluates if the answer is correct. If the answer is correct, the generated rationale is automatically added to a dataset of correct rationales, which is then used to finetune the LLM further.","Enables LLMs to learn to generate better rationales and improve reasoning performance without extensive manual annotation, by iteratively improving on self-generated high-quality reasoning traces.","['Chain-of-Thought (CoT) Prompting', 'Scratchpads']","['Improving LLM reasoning', 'Generating high-quality rationales for training']",9,
Scratchpads,"Large Language Models (LLMs) can struggle with multi-step computation or reasoning problems where intermediate steps are crucial for correctness but are not explicitly part of the final output, making it difficult for the model to learn and reproduce the full reasoning process.",Finetuning LLMs for tasks involving multi-step computation or complex reasoning where intermediate calculations or thoughts are important.,"The LLM is finetuned on data that includes explicit intermediate computation steps, effectively teaching the model to 'show its work' in an internal 'scratchpad' before producing the final answer. This makes the intermediate reasoning process visible and learnable by the model.","Improves performance on multi-step computation problems by enabling the model to explicitly represent and learn from intermediate steps, leading to more accurate and robust solutions.","['Chain-of-Thought (CoT) Prompting', 'Self-Taught Reasoner (STaR)']","['Multi-step computation problems', 'Complex reasoning tasks']",9,
Selection-Inference,"Complex logical reasoning tasks can be challenging for Large Language Models (LLMs) to handle directly, leading to less interpretable or error-prone reasoning.",Applying LLMs to tasks requiring interpretable logical reasoning.,"The overall reasoning process is divided into two distinct steps: a 'selection' step and an 'inference' step. The selection step identifies relevant information or premises, and the inference step then uses this selected information to derive conclusions.","Provides a more structured and interpretable approach to logical reasoning, allowing LLMs to tackle complex problems by breaking them down into manageable, sequential stages.","['Chain-of-Thought (CoT) Prompting', 'Faithful Reasoning', 'Least-to-Most Prompting']",['Interpretable logical reasoning'],9,
Faithful Reasoning,"Multi-step reasoning tasks can be complex and difficult for a single Large Language Model (LLM) to perform faithfully, potentially leading to errors or ungrounded conclusions.",Applying LLMs to multi-step reasoning tasks where fidelity and accuracy of each step are critical.,"The multi-step reasoning process is decomposed into three distinct steps, with each step performed by a dedicated Large Language Model (LM) or a specialized component of a single LM. This modular approach ensures that each stage of reasoning is handled by a component optimized for that specific sub-task.","Enhances the faithfulness and accuracy of multi-step reasoning by distributing the cognitive load across specialized components, leading to more reliable and verifiable conclusions.","['Chain-of-Thought (CoT) Prompting', 'Selection-Inference', 'Least-to-Most Prompting']",['Multi-step reasoning'],9,
Act-Only,"Agents in interactive environments need to perform domain-specific actions to achieve goals, but without explicit verbal reasoning, they can struggle with abstract goals, maintaining working memory, or recovering from errors (e.g., hallucinating actions).","Using pretrained language models for planning and acting in interactive environments (e.g., web browsers, text-based games).","Multimodal observations from the environment are converted into text, and the language model is then used to generate domain-specific actions or plans. A controller then executes these actions. This approach focuses solely on action prediction without the LLM generating explicit intermediate verbal reasoning traces ('thoughts').","Enables LLMs to interact with and perform actions in dynamic environments. However, without explicit reasoning, these agents may lack the ability to reason abstractly about high-level goals, track progress, or handle exceptions effectively, often leading to repetitive or ungrounded actions.","['ReAct (Reasoning and Acting)', 'LLM as a Planner (SayCan)']","['Interactive environments', 'Web browsing for question answering (WebGPT)', 'Task-oriented dialogue systems']",11,
Least-to-Most Prompting,"Large Language Models (LLMs) can struggle with complex reasoning tasks that inherently require breaking down a problem into smaller, interdependent subproblems, as they might attempt to solve the entire problem in one go.",Solving complicated reasoning tasks with Large Language Models (LLMs).,"The LLM is first prompted to decompose a complex problem into a series of simpler, sequential subproblems. Then, each subproblem is solved one by one, with the solution or output of a previous subproblem being explicitly fed as additional context or input to the LLM for solving the next subproblem. This continues until the final answer to the original complex problem is derived.","Enables LLMs to effectively tackle complex reasoning tasks by mimicking a human-like step-by-step problem-solving approach, where solutions to simpler parts build up to the solution of the whole.","['Chain-of-Thought (CoT) Prompting', 'Selection-Inference', 'Faithful Reasoning']","['Complex reasoning tasks', 'Problem decomposition']",9,
Retrieval-Augmented Generation (RAG),"Large Language Models (LLMs) have limited parametric knowledge, struggle to access up-to-date or domain-specific information, and can hallucinate. Modifying model weights for new knowledge is computationally expensive and difficult.","Building LLM applications that require access to dynamic, external, or specialized knowledge beyond what the base model was trained on.","A pipeline where a retriever first fetches relevant external contexts (documents, passages) for a given user query. These retrieved contexts are then provided to the LLM along with the query, enabling the LLM to generate an answer grounded in the external information.","LLMs can handle long-tail knowledge, provide up-to-date information, adapt to specific domains/tasks without costly retraining, and reduce factual errors or hallucinations by grounding responses in retrieved evidence.","['Retrieve-Rerank-Generate Pipeline', 'Unified Instruction Tuning for RAG and Ranking']","Knowledge-intensive NLP tasks, open-domain question answering, chatbots, information retrieval, domain adaptation for LLMs.",18,
Retrieve-Rerank-Generate Pipeline,"In standard RAG, the initial retriever may return a large number of contexts, some of which are irrelevant or noisy. Providing too many contexts to the LLM can degrade generation accuracy and efficiency, even with long context windows, due to the LLM's limited capacity to effectively process and prioritize a large volume of information. Relying solely on a retrieval model might be inadequate for high recall.","Retrieval-Augmented Generation systems where the initial retrieval step often yields a superset of relevant and irrelevant information, and the LLM performs better with a smaller, highly relevant set of contexts.","Extend the standard RAG pipeline by adding an intermediate reranking step. First, a retriever fetches a broad set of 'N' candidate contexts. Second, a dedicated reranker (or a reranking-capable LLM) evaluates the relevance of these 'N' contexts to the query and selects a refined, smaller set of 'k' (where 'k < N') most relevant contexts. Finally, the LLM generates the answer using only these 'k' highly relevant, reranked contexts.","Significantly improves the quality and relevance of the contexts provided to the LLM, leading to higher accuracy and robustness in the generated answers. Mitigates the 'lost in the middle' or 'too much noise' problem for LLMs and optimizes the use of context window.","['Retrieval-Augmented Generation (RAG)', 'Unified Instruction Tuning for RAG and Ranking']","High-stakes knowledge-intensive QA, complex information retrieval, improving the robustness and accuracy of RAG systems, scenarios where initial retrieval is noisy.",30,
Unified Instruction Tuning for RAG and Ranking (RankRAG Framework),"Traditional instruction tuning for RAG often focuses only on generation from given contexts, which can be suboptimal with poor initial retrieval. Separate expert ranking models lack the zero-shot generalization capabilities of versatile LLMs and require dedicated training. The goal is to create a single LLM capable of both effectively ranking contexts and generating high-quality answers, overcoming the limitations of separate models and improving robustness to irrelevant context.","Developing more capable and robust LLMs for Retrieval-Augmented Generation, aiming to integrate context selection (ranking) and answer generation capabilities into a single model.","Instruction-tune a single LLM using a specialized, multi-stage training blend that explicitly teaches both context ranking and answer generation. This involves: 1. An initial Supervised Fine-Tuning (SFT) stage on diverse instruction-following datasets to imbue basic capabilities. 2. A second instruction tuning stage using a blend of: general instruction-following data, context-rich QA data (to enhance context utilization), retrieval-augmented QA data (including hard negatives for robustness), context ranking data (training the LLM to identify relevant/irrelevant passages), and retrieval-augmented ranking data (training the LLM to identify relevant passages from a set of multiple retrieved passages). A key aspect is unifying all these diverse tasks into a standardized (question, context, target_output) format during instruction tuning to maximize knowledge transfer and mutual enhancement between ranking and generation capabilities.","Produces a single, data-efficient LLM that excels at both context ranking and answer generation within the RAG framework. This LLM outperforms specialized ranking models and improves robustness to irrelevant contexts, demonstrating strong generalization capabilities to new domains. It simplifies the RAG pipeline by using one model for two critical functions.","['Retrieval-Augmented Generation (RAG)', 'Retrieve-Rerank-Generate Pipeline']","Training foundation models for advanced RAG applications, enhancing LLM robustness to noisy retrieval, building efficient RAG systems with a unified model.",30,
Reasoning on Graphs (RoG),"Large Language Models (LLMs) lack up-to-date knowledge, experience hallucinations during reasoning, and overlook the structural information of Knowledge Graphs (KGs), diminishing their performance and trustworthiness in knowledge-intensive tasks.","Knowledge Graph Question Answering (KGQA) tasks or other knowledge-intensive reasoning tasks where LLMs need to access and leverage structured, external knowledge for faithful and interpretable results.","A planning-retrieval-reasoning framework that synergizes LLMs with KGs. The 'planning module' first generates 'relation paths' (sequences of relations in a KG) grounded by KGs as faithful plans. These plans are then used by the 'retrieval-reasoning module' to retrieve valid 'reasoning paths' (instances of relation paths with entities) from the KGs. Finally, the LLM conducts faithful reasoning based on these retrieved, structured paths.",Achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results by grounding LLM reasoning in KG structure and knowledge.,"['LLM-KG Integration', 'Retrieval-Augmented Generation (RAG) for LLMs', 'Plan-and-Solve Reasoning', 'Agentic LLM for External Interaction']","Knowledge Graph Question Answering (KGQA), multi-hop reasoning, factual verification.",14,
LLM-KG Integration,"LLMs often lack structured, up-to-date, and explicit factual knowledge, leading to hallucinations and poor performance on knowledge-intensive tasks, especially those requiring multi-hop reasoning or structural understanding.","Knowledge-intensive applications, complex question answering over structured data (KGs), and scenarios requiring interpretable and faithful reasoning.",Combine Large Language Models with Knowledge Graphs. This can involve various strategies: 1) Retrieving KG facts or paths to augment LLM prompts (a form of RAG). 2) Using LLMs to generate queries for KGs (Semantic Parsing). 3) Distilling KG knowledge into LLMs through training. 4) Allowing LLMs to act as agents that interact with KGs. 5) Using KG structure to guide LLM planning and reasoning.,"Improves factual accuracy, enables multi-hop and structural reasoning, reduces hallucinations, and enhances interpretability and faithfulness of LLM outputs.","['Retrieval-Augmented Generation (RAG) for LLMs', 'Semantic Parsing for LLMs', 'Agentic LLM for External Interaction', 'Reasoning on Graphs (RoG)']","Knowledge Graph Question Answering (KGQA), fact extraction, knowledge base construction, explainable AI, scientific discovery.",14,
Retrieval-Augmented Generation (RAG) for LLMs,"LLMs suffer from knowledge cutoff, lack of up-to-date information, and hallucinations, leading to unfaithful or incorrect responses, particularly in knowledge-intensive tasks.","Knowledge-intensive tasks, questions requiring factual accuracy, domain-specific information, or dynamic, real-time data.","Integrate a retrieval mechanism that fetches relevant information (e.g., documents, passages, knowledge graph triples, or reasoning paths) from an external, up-to-date knowledge source. This retrieved context is then provided to the LLM alongside the original prompt to ground and guide its generation.","Reduces hallucinations, improves factual accuracy, provides access to up-to-date and external knowledge, and enhances the trustworthiness and specificity of LLM outputs.","['LLM-KG Integration', 'Agentic LLM for External Interaction', 'Reasoning on Graphs (RoG)']","Open-domain Question Answering, fact-checking, information synthesis, knowledge graph question answering, personalized content generation.",18,
Plan-and-Solve Reasoning (Task Decomposition for LLMs),"LLMs struggle with complex, long-horizon tasks, or tasks requiring multiple, intricate reasoning steps, often leading to errors or incomplete solutions.","LLMs solving complex problems, multi-step instructions, code generation, or deep reasoning tasks where a monolithic approach is insufficient.","Prompt LLMs to first generate a high-level plan or decompose the complex task into a series of smaller, more manageable subtasks. Then, the LLM executes each step of the plan or solves each subtask sequentially, often leveraging intermediate results.","Improves the LLM's ability to handle complexity, reduces errors by breaking down problems, makes reasoning more structured, and enhances the transparency of the solution process.","['Chain-of-Thought (CoT) Reasoning', 'Advanced Reasoning Structures (Tree/Graph of Thoughts)', 'Reasoning on Graphs (RoG)']","Complex reasoning, multi-step problem-solving, code generation, robotics planning, strategic decision-making.",41,
Agentic LLM for External Interaction,"LLMs are typically static models with limited ability to access real-time information, perform actions, or interact with external environments or tools.","Tasks requiring dynamic information retrieval, tool use (e.g., APIs, web search), environmental interaction, or complex multi-step problem-solving that goes beyond the LLM's internal knowledge.","Design the LLM to act as an agent that can interleave reasoning steps with calls to external tools, APIs, or knowledge bases. The LLM observes the environment or tool output, uses it to update its internal state, and plans subsequent actions. This often involves specific prompting strategies to guide the LLM's 'thought' and 'action' cycles.","Extends LLM capabilities to dynamic, real-world tasks; provides access to up-to-date information; enables complex action sequences; improves groundedness and reduces hallucinations.","['Retrieval-Augmented Generation (RAG) for LLMs', 'LLM-KG Integration', 'Plan-and-Solve Reasoning']","Web browsing, API interaction, robotics, scientific discovery, dynamic question answering, complex code execution.",35,
Semantic Parsing for LLMs,"Directly querying structured knowledge bases (like KGs or databases) requires specialized query languages (e.g., SPARQL, SQL), which natural language models cannot generate or execute directly, limiting their ability to precisely extract or infer information from structured data.","Knowledge Graph Question Answering (KGQA), structured database querying, or tasks where natural language questions need to be translated into formal, executable queries.","Use LLMs to convert natural language questions into formal, executable logical queries (e.g., SPARQL, lambda calculus). These generated queries can then be executed by a dedicated query engine on a Knowledge Graph or database to obtain precise answers.","Enables precise and interpretable querying of structured knowledge, leveraging the LLM's understanding of natural language while maintaining the accuracy and executability of formal queries.",['LLM-KG Integration'],"Knowledge Graph Question Answering, database querying from natural language, report generation from structured data.",14,
Chain-of-Thought (CoT) Reasoning,"LLMs often struggle with complex multi-step reasoning tasks, producing direct answers that may be incorrect or lack transparency regarding their derivation.","Complex problem-solving, mathematical questions, logical puzzles, or any task requiring intermediate thinking steps and justification.",Prompt the LLM to generate a series of intermediate reasoning steps or a 'thought process' before arriving at the final answer. This is typically achieved through few-shot prompting with examples that explicitly demonstrate the reasoning chain.,"Improves the LLM's ability to perform complex reasoning, increases transparency and interpretability of its decisions, and often leads to more accurate and reliable results.","['Plan-and-Solve Reasoning', 'Advanced Reasoning Structures (Tree/Graph of Thoughts)', 'Self-Correction/Verification for LLMs']","Mathematical word problems, logical inference, common-sense reasoning, multi-step instructions, code explanation.",41,
Advanced Reasoning Structures (Tree/Graph of Thoughts),"Linear Chain-of-Thought reasoning can be insufficient for highly complex problems, potentially leading to errors or missing optimal solutions due to its sequential nature or lack of broader exploration.","Tasks requiring exploration of multiple reasoning paths, complex decision-making, creative problem-solving, or scenarios where backtracking and re-evaluation of intermediate thoughts are beneficial.","Extend linear reasoning chains into more complex, non-linear structures. 'Tree of Thoughts' involves generating multiple divergent reasoning paths, exploring them in parallel, and allowing for backtracking and pruning based on evaluation. 'Graph of Thoughts' further models reasoning as a graph, enabling non-linear progression, merging of ideas, and aggregation of information from different branches or sub-problems.","Enhances LLM's ability to tackle more intricate problems, improves robustness by exploring diverse solutions, and facilitates more sophisticated planning and problem-solving than linear CoT.","['Chain-of-Thought (CoT) Reasoning', 'Plan-and-Solve Reasoning', 'Monte-Carlo Planning for Faithful Reasoning']","Strategic game playing, complex scientific discovery, multi-agent coordination, creative writing, advanced logical puzzles.",41,
Self-Correction/Verification for LLMs,"LLMs are prone to hallucinations, logical inconsistencies, and errors in their reasoning steps or generated outputs, especially in complex or high-stakes scenarios where accuracy is critical.","Critical applications requiring high factual accuracy, faithfulness, or logical soundness (e.g., legal, medical, scientific); complex reasoning tasks where intermediate steps can go wrong.","Implement mechanisms for the LLM itself, or an auxiliary component, to evaluate, validate, or self-correct its own generated reasoning steps or final answers. This can involve generating multiple solutions and checking for consistency, using a separate 'verifier' model, or prompting the LLM to explicitly check its own work based on provided criteria or external knowledge.","Improves the reliability, faithfulness, and accuracy of LLM outputs; enhances trustworthiness in critical applications by reducing errors and inconsistencies.","['Chain-of-Thought (CoT) Reasoning', 'Monte-Carlo Planning for Faithful Reasoning']","Fact-checking, mathematical problem-solving, code generation, legal/medical reasoning, content moderation.",23,
Monte-Carlo Planning for Faithful Reasoning,"Ensuring the faithfulness and correctness of reasoning steps generated by LLMs, particularly in open-ended or complex tasks where direct, deterministic verification is challenging.","LLM-driven reasoning requiring high confidence, exploration of multiple reasoning trajectories, and validation against a ground truth or logical constraints to minimize hallucinations.","Apply Monte-Carlo planning techniques (e.g., Monte-Carlo Tree Search) to explore and evaluate different reasoning paths. The LLM generates potential reasoning steps, and the planning algorithm guides the search, evaluates outcomes, and prunes unpromising paths to converge on a faithful reasoning sequence. This often involves simulating reasoning outcomes or using a scoring mechanism.","Improves the faithfulness and reliability of LLM-generated reasoning, provides a systematic way to explore and validate reasoning steps, and significantly reduces the likelihood of hallucinations in complex reasoning tasks.","['Advanced Reasoning Structures (Tree/Graph of Thoughts)', 'Self-Correction/Verification for LLMs', 'Plan-and-Solve Reasoning']","Complex logical reasoning, strategic planning, decision-making in uncertain environments, ensuring factuality in generated explanations, game AI.",23,
