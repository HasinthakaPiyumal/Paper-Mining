Pattern Name,Problem,Context,Solution,Result,Related Patterns,Uses,Cluster Number,Category
LLM-Augmented Knowledge Base,"Traditional knowledge graphs for recommender systems are often sparse, limited, and expensive to construct or complete, leading to ignored user preferences and suboptimal recommendations. They also lack comprehensive cross-domain information.","Recommender systems that rely on knowledge graphs for rich semantic information, improved accuracy, and explainability.","Leverage Large Language Models (LLMs) for their ability to retrieve factual knowledge, complete missing facts, and construct knowledge graphs (including entity discovery, coreference resolution, and relation extraction) from text corpora. LLMs can also distill common sense facts into knowledge graphs.","More extensive, up-to-date, and comprehensive knowledge graphs, leading to enhanced recommendation accuracy, relevance, personalization, and improved cross-domain recommendation capabilities.","['LLM as Semantic Content Encoder', 'Tool-Augmented LLM']","Recommender systems, cross-domain recommendations, knowledge graph completion, knowledge graph construction.",0,
LLM as Semantic Content Encoder,"Traditional content-based recommenders struggle to capture deep semantic representations and world knowledge from textual features, leading to limited understanding of item properties and user preferences. This also impacts cold-start and cross-domain recommendation scenarios.","Recommender systems that need to process and understand textual content (e.g., item descriptions, reviews, news articles) to infer user preferences and item properties.","Utilize pretrained Large Language Models (LLMs) as powerful semantic encoders to transform textual data into rich, context-aware feature embeddings. This involves adapting LLMs through fine-tuning or task-specific pretraining to align with recommendation objectives. Techniques like knowledge distillation and model optimization can be employed to reduce inference latency for online serving.","Enhanced understanding and interpretation of textual content, improved feature representations, better capture of user interests and item properties, and alleviation of cold-start and cross-domain recommendation challenges.","['Instruction Tuning for Recommendation', 'LLM-Augmented Knowledge Base']","Content-based recommender systems, news recommendation, sequential recommendation, tag recommendation, cold-start recommendation, cross-domain recommendation.",0,
Instruction Tuning for Recommendation,General-purpose Large Language Models (LLMs) are not inherently optimized for diverse recommendation tasks and may perform poorly in zero-shot or few-shot scenarios without specific adaptation.,"Adapting general LLMs to effectively perform various recommendation tasks (e.g., rating prediction, item recommendation, sequential recommendation, personalized search) and generalize across domains.","Formulate recommendation tasks as instruction-following procedures. This involves designing various instruction templates to accommodate different recommendation tasks, generating high-quality instruction data by converting user interaction history, retrieved candidates, and potentially LLM-generated reasoning features into natural language instructions, and then fine-tuning LLMs with this instruction data.","LLMs demonstrate superior performance in few-shot learning and cross-domain generalization for recommendation tasks, effectively handling a wide range of user information requirements.","['LLM as Semantic Content Encoder', 'LLM as Direct Recommender (via In-Context Learning)', 'LLM as Conversational Recommender Agent', 'LLM as Personalized AIGC Creator']","Sequential recommendation, rating prediction, item recommendation, personalized search, cross-domain recommendation, domain adaptation for conversational agents.",0,
LLM as Explainable Recommender,"Traditional recommender systems are often black boxes, leading to a lack of user trust. Existing explanation methods are often inflexible, lack diversity, coherence, and generalizability across different recommendation models.","Recommender systems where user trust and understanding of recommendations are crucial, and explanations need to be natural, customized, and model-agnostic.","Leverage Large Language Models (LLMs) for generating natural language explanations. LLMs can craft customized, precise, and adaptable explanations by harnessing their understanding of human language, context, and complex syntax. They utilize in-context learning (zero-shot, few-shot, Chain-of-Thought prompting) to generate explanations in real-time, incorporating user feedback and fostering human-machine alignment. This approach provides model-agnostic interpretations, explaining the reasoning behind recommendations from various underlying models.","Improved transparency, persuasiveness, and reliability of recommendations; enhanced user trust and satisfaction; more diverse, coherent, and adaptable explanations; and a versatile, scalable interpretational framework.","['Chain-of-Thought Prompting for Reasoning', 'LLM as Direct Recommender (via In-Context Learning)']","Explainable recommender systems, drug recommendations, general AI model interpretation.",0,
LLM as Direct Recommender (via In-Context Learning),"Adapting traditional recommendation models often requires extensive tuning and data. There's a need for flexible, quick adaptation to new recommendation tasks or domains, especially in zero-shot or few-shot scenarios.","Recommender systems requiring rapid deployment or adaptation to new tasks/domains, or operating with limited explicit training data.","Utilize Large Language Models (LLMs) with their in-context learning capabilities to directly generate recommendations. This involves providing natural language instructions and/or a few input-output pairs (shots) as demonstrations within the prompt. LLMs then generate recommendations (e.g., rating predictions, item rankings) without explicit fine-tuning. For ranking tasks, a candidate generation module might be integrated to narrow down items before LLM reranking.","Enables zero-shot and few-shot recommendations, quick adaptation to new tasks, and leverages LLMs' commonsense knowledge. Performance can be improved with multi-step reasoning strategies like Chain-of-Thought prompting.","['Chain-of-Thought Prompting for Reasoning', 'Instruction Tuning for Recommendation', 'LLM as Explainable Recommender']","Rating prediction, ranking prediction, sequential recommendation, direct recommendation, open-domain recommendations (movies, books).",0,
Chain-of-Thought Prompting for Reasoning,"Large Language Models (LLMs) may struggle with complex tasks requiring multi-step reasoning or logical deduction, leading to suboptimal or incorrect conclusions.","Tasks that can be broken down into intermediate steps, where explicit reasoning paths can guide the LLM towards a more accurate final answer.","Employ Chain-of-Thought (CoT) prompting strategies. This involves providing the LLM with prompts that include previous intermediate reasoning steps, or explicitly instructing the LLM to 'think step by step.' This can be combined with multi-step prompt designs (e.g., NIR for recommendation).","Elicits emergent reasoning abilities in LLMs, enabling them to solve complex tasks by breaking them into subproblems, improving accuracy and performance in tasks like mathematical word problems or multi-step recommendation.","['LLM as Explainable Recommender', 'LLM as Direct Recommender (via In-Context Learning)', 'Tool-Augmented LLM']","Complex task solving, mathematical word problems, multi-step recommendation, automated selection, general reasoning tasks.",0,
LLM for AutoML and Architecture Search,"Automated Machine Learning (AutoML) and Neural Architecture Search (NAS) are computationally expensive and complex, requiring iterative sampling and evaluation of architectures or features.","Optimizing ML models (e.g., recommender systems) by automatically searching for optimal embedding sizes, features, feature interactions, or model architectures.","Leverage Large Language Models (LLMs) for their generative, memorization, and reasoning capabilities to assist or perform AutoML tasks. This includes generating network architectures directly, acting as black-box agents to propose better-performing architectures based on previous trials, or integrating LLMs into existing search strategies (e.g., genetic algorithms) as mutation and crossover operators to generate candidate architectures or modifications.","Reduces the search space, generates reasonable architectures, and potentially improves the efficiency and effectiveness of AutoML processes for recommender systems and other ML tasks.",[],"Neural Architecture Search (NAS), automated feature selection, automated feature interaction search, general AutoML.",0,
LLM as Conversational Recommender Agent,"Building effective conversational recommender systems (CRS) requires real-time understanding of user intent, adaptation to feedback, and handling domain-specific knowledge and long conversation contexts. General LLMs lack awareness of private domain data and have token limits for long dialogues.",Developing interactive recommender systems that engage users in natural language dialogue to uncover preferences and provide personalized recommendations.,"Employ Large Language Models (LLMs) as the core of conversational recommender agents. This involves domain adaptation through fine-tuning LLMs with private, domain-specific dialogue data (potentially generated by LLM-based user simulators). It also includes tool integration, treating traditional recommendation models as external tools that the LLM can invoke to obtain recommendations, and memory augmentation by incorporating memory modules or user profile modules to store and retrieve meaningful, enduring facts about users from long conversations, overcoming token limits.","Enables LLMs to act as intelligent conversational agents, providing personalized recommendations, understanding user intent in real-time, adapting to feedback, and handling domain-specific knowledge and long dialogue histories.","['Tool-Augmented LLM', 'Memory-Augmented LLM', 'Instruction Tuning for Recommendation']","Conversational recommender systems, personalized assistance, customer service chatbots.",0,
Memory-Augmented LLM,"Large Language Models (LLMs) have limited context windows (token limits), making it challenging to maintain coherence and leverage historical information in long conversations or when dealing with extensive user profiles.","LLM-based applications (e.g., conversational agents, personalized systems) that require retaining and utilizing information from extended interactions or large knowledge bases beyond the immediate prompt.","Augment LLMs with external memory modules. This involves extracting meaningful and enduring facts about users from historical conversations and storing them in a dedicated user memory (e.g., a factual statement database). When processing new user queries, relevant facts are retrieved from the memory based on text similarity or other indexing mechanisms and incorporated into the LLM's prompt to provide context and enhance its long-dialogue memory capability.","Overcomes LLM token limits, improves comprehension and coherence in long conversations, enables better utilization of historical user information, and enhances the accuracy of personalized responses.","['LLM as Conversational Recommender Agent', 'Tool-Augmented LLM']","Conversational AI, personalized assistants, long-context understanding, user profile management.",0,
Tool-Augmented LLM,"Large Language Models (LLMs) have impressive general knowledge and reasoning but lack specific, up-to-date, or private domain knowledge, struggle with complex computations, and cannot directly interact with external systems or real-world environments. This limits their task-solving capabilities.","LLM-based systems needing to perform complex tasks that require specialized knowledge, real-time data, external computation, or interaction with other software/APIs.","Augment LLMs with external tools, where the LLM acts as a controller or orchestrator. The LLM comprehends user input, breaks down complex tasks into subtasks, and decides which specialized tools (e.g., search engines, recommendation engines, calculators, databases, other AI models, APIs) to invoke. It generates both reasoning paths and task-specific actions alternately (e.g., ReAct), delegating action execution to tools and using external feedback to guide further reasoning. Finally, the LLM integrates the outputs from the tools to complete the end-to-end task and present a coherent response. Advanced approaches may even empower LLMs to directly generate new tools.","Enhances LLMs' task-solving capabilities, provides access to external, up-to-date, and domain-specific knowledge, enables complex computations, and allows interaction with real-world systems, overcoming limitations like hallucinations and lack of domain awareness.","['LLM as Conversational Recommender Agent', 'Chain-of-Thought Prompting for Reasoning', 'LLM-Augmented Knowledge Base', 'Memory-Augmented LLM']","Conversational AI, personalized systems, complex task automation, question answering, code generation, visual tasks, web browsing, scientific reasoning, robotics.",0,
LLM as Personalized AIGC Creator,"Traditional recommender systems only suggest existing items. Creating customized, appealing content (e.g., ad titles, descriptions, images, music) that precisely matches individual user interests and preferences is labor-intensive and difficult to scale. User feedback for content generation is often sparse.","E-commerce, online advertising, customer service, and other domains where personalized, dynamically generated content can enhance user engagement and experience.","Leverage Large Language Models (LLMs) and AI-Generated Content (AIGC) techniques to create personalized content. LLMs reason about user personalized intent and interests from instructions or feedback, then generate various forms of content (text, images, multimodal) based on their knowledge and the inferred user intent. Strategies like Reinforcement Learning from Human Feedback (RLHF) or iterative conversational feedback are employed to fine-tune LLMs, allowing them to better capture explicit user preferences and guide content generation. LLMs' cross-modal knowledge bases can be utilized for realistic and diverse content creation.","Efficient and accessible creation of highly customized and appealing content, improved user engagement, alleviation of sparse feedback problems in content generation, and enhanced personalized experiences across various business scenarios.","['Instruction Tuning for Recommendation', 'LLM as Conversational Recommender Agent']","Online advertising, e-commerce product descriptions, customer service chatbots, personalized media generation (images, music).",0,
Agent-Computer Interface (ACI),"Language Model (LM) agents struggle to reliably interact with complex digital environments (like software engineering tools) when using interfaces designed for humans (e.g., Linux shell), leading to poor performance, inefficient actions, and difficulty in error recovery.","LM agents are tasked with complex, multi-step operations in digital environments (e.g., software engineering, code generation, navigation). Existing human-centric interfaces are not optimized for LM capabilities and limitations (e.g., lack of visual understanding, fixed cost of context window, difficulty with verbose or unstructured information).","Introduce an abstraction layer, an Agent-Computer Interface (ACI), between the LM agent and the computer. This interface provides a tailored set of commands and structured feedback, designed to complement the LM's abilities and mitigate its weaknesses. It manages context, simplifies actions, and incorporates guardrails to enhance reliability and efficiency.","Substantially enhances LM agents' ability to perform complex tasks, improving performance, efficiency, and error recovery compared to using human-centric interfaces. It unifies tool use, prompting techniques, and code execution within a single framework.","['Concise and Structured Environment Feedback', 'Guardrails for Agent Actions', 'Compact and Efficient Agent Actions']","Automated software engineering, digital environment interaction for LM agents, code generation, web navigation, computer control, any domain where LM agents interact with complex systems.",0,
Concise and Structured Environment Feedback,"Language Model (LM) agents are sensitive to context window size and distracting information. Verbose or unstructured environment feedback can consume excessive tokens, reduce performance, and make it difficult for the agent to identify relevant information.","LM agents interact with digital environments, receiving observations after each action. All content in the context window has a fixed cost in memory and computation for LMs, and distracting context can harm performance.","Design environment feedback to be informative but concise. This includes providing substantive information about the current state and action effects without unnecessary details, contextualizing code snippets (e.g., with prepended line numbers, indicators of omitted lines), and collapsing old observations into single-line summaries to maintain essential information while reducing unnecessary context.","Improves LM agent efficiency by reducing context window bloat, allowing more interaction cycles, and enhancing the agent's ability to focus on relevant information, leading to better downstream task performance and reduced cost.","['Agent-Computer Interface (ACI)', 'Guardrails for Agent Actions', 'Compact and Efficient Agent Actions']","Any LM agent interacting with a digital environment, especially for code editing, file navigation, debugging, and managing long-running tasks where context management is critical.",0,
Guardrails for Agent Actions,"Language Model (LM) agents make mistakes (e.g., syntax errors in code edits, malformed commands, overly broad searches) and struggle to recover from these errors. This leads to cascading failures, wasted turns, and increased computational cost.","LM agents perform actions in a digital environment where errors can have significant negative consequences (e.g., invalid code breaking the system, inefficient searches consuming budget). Recovery from self-incurred errors is difficult for LMs.","Implement guardrails within the agent-computer interface to automatically detect and prevent common mistakes or provide immediate, specific feedback for recovery. Examples include a code syntax checker for edits (discarding invalid edits and prompting retry), limiting verbose search results (suggesting more specific queries), and providing error responses for malformed generations.","Mitigates error propagation, hastens recovery from mistakes, and improves the reliability and efficiency of agent actions, leading to higher task success rates and reduced wasted resources.","['Agent-Computer Interface (ACI)', 'Concise and Structured Environment Feedback', 'Compact and Efficient Agent Actions']","Automated software engineering, any agentic system where action validity and error recovery are critical, robust ML workflow execution, prompt design for error handling.",0,
Compact and Efficient Agent Actions,"Language Model (LM) agents can be inefficient when performing higher-order operations that require composing many simple, granular actions across multiple turns. This leads to increased cost, context window usage, and slower progress towards a goal.","LM agents interact with digital environments using a set of available commands. Granular commands (like many bash commands) require complex composition for common tasks, which is not efficient for LMs due to their sequential nature and context limitations.","Consolidate important operations (e.g., multi-line file editing, efficient search, file navigation) into as few actions as possible. Provide simple commands with concise documentation and few options. Design actions to make meaningful progress towards a goal in a single step, rather than requiring composition across multiple turns.","Improves agent efficiency by reducing the number of turns required for complex tasks, lowering computational cost, and making actions easier for agents to use reliably, thereby accelerating problem-solving and improving overall performance.","['Agent-Computer Interface (ACI)', 'Concise and Structured Environment Feedback', 'Guardrails for Agent Actions']","Automated software engineering, digital environment interaction, task planning for agents, optimizing LLM agent workflows, robotics for high-level action abstraction.",0,
LLM-Assisted Symbolic World Model Construction,"Large Language Models (LLMs) directly used as planners are impractical due to limited correctness of plans, strong reliance on online feedback from interactions with simulators or the actual environment, and inefficiency in utilizing human feedback for complex, long-horizon planning tasks.","Designing AI agents for sequential decision-making or planning problems where reliability and correctness are paramount. Users may not be experts in formal planning languages (like PDDL), and the system needs to leverage LLMs' common-sense knowledge for knowledge acquisition and representation.","Leverage LLMs to construct an explicit, symbolic world model (e.g., in PDDL) from natural language descriptions of actions and domains. The LLM also acts as an interface, translating the symbolic model to natural language for human inspection and incorporating natural language corrective feedback (from humans or automated validators) back into the symbolic model through an iterative dialogue. This corrected symbolic model is then used by sound, domain-independent planners.","Provides correctness guarantees from external planners, reduces human involvement by shifting correction effort to the model construction phase, and conceals the complexity of formal planning languages from non-expert users. It enables LLMs to excel at modeling causal dependencies rather than combinatorial search.","['LLM with External Tools', 'Corrective Reprompting / Iterative Plan Refinement', 'LLM as Heuristic/Seed Planner']","Task planning for embodied agents (e.g., household robots), classical planning domains (e.g., IPC domains), knowledge acquisition for AI agents.",0,
LLM as Heuristic/Seed Planner,"Off-the-shelf LLMs often struggle to produce fully accurate and executable plans for complex tasks due to limitations in reasoning and handling long-term dependencies, but they can provide sensible high-level guidance or initial suggestions based on their broad common-sense knowledge.","Planning problems where an LLM's common-sense knowledge can be beneficial for guiding the search, but its direct planning capabilities are insufficient for generating reliable, executable plans. The system needs to combine the strengths of LLMs with the precision of specialized planners.","Use an LLM to generate a preliminary, high-level plan, score potential actions, or provide a 'seed' plan. This LLM output is then passed to a more specialized, reliable external planner (e.g., a classical domain-independent planner, a low-level grounding planner, or a local-search planner) which refines, validates, or grounds these suggestions to determine executability and ensure correctness.","Improves the overall planning process by combining the LLM's broad knowledge and high-level reasoning with the precision and correctness of specialized planners, potentially accelerating plan search and making it more robust.","['LLM with External Tools', 'LLM-Assisted Symbolic World Model Construction']","Robotics (e.g., scoring high-level actions, grounding actions), accelerating local-search planners, general sequential decision-making, hybrid planning systems.",0,
LLM with External Tools,"Large Language Models (LLMs) are approximately omniscient but may not always outperform specialized models or tools in specific, precise, or computationally intensive subtasks (e.g., arithmetic, logical reasoning, sound planning, factual retrieval, code execution).","Designing AI systems where an LLM needs to perform tasks that require capabilities beyond its inherent generative or reasoning abilities, or where higher reliability and accuracy are required for specific sub-components. The LLM needs to act as an orchestrator.","Enable the LLM to identify when an external tool is needed, formulate appropriate input for that tool, invoke the tool, and then interpret and integrate the tool's output back into its reasoning or generation process. This creates a hybrid system where the LLM orchestrates specialized tools to achieve complex goals.","Augments the LLM's capabilities, leading to more reliable, accurate, and robust performance on complex tasks by offloading specialized subtasks to dedicated, proven tools. It allows LLMs to act as controllers or orchestrators for a suite of specialized functionalities.","['LLM-Assisted Symbolic World Model Construction', 'LLM as Heuristic/Seed Planner', 'Corrective Reprompting / Iterative Plan Refinement']","Arithmetic, logical reasoning, planning, code generation, data retrieval, interacting with APIs, scientific computation, embodied agents.",0,
Corrective Reprompting / Iterative Plan Refinement,"LLM-generated outputs (e.g., plans, code, text) often contain errors, inconsistencies, or fail to meet specific constraints. LLMs may also struggle to self-correct effectively or get stuck in repetitive error loops when attempting to refine their outputs.","Situations where an LLM is used to generate complex outputs that require high accuracy or adherence to specific rules, and where an external mechanism can provide structured, actionable feedback to guide the LLM's refinement process.","An external validator (which could be a symbolic simulator, a formal checker, a human expert, or even another LLM acting as a critic) evaluates the LLM's output. If errors or discrepancies are detected, the validation results are translated into natural language feedback and provided back to the LLM in a subsequent prompt. The LLM then uses this feedback to iteratively refine its output until it meets the desired criteria or a stopping condition is met.","Significantly improves the correctness, consistency, and quality of LLM-generated outputs by enabling a structured feedback loop, reducing the need for extensive manual correction or reliance on costly online execution. It helps LLMs overcome their limitations in precise reasoning and self-correction.","['LLM with External Tools', 'LLM-Assisted Symbolic World Model Construction']","Iterative plan generation, debugging LLM reasoning, improving code generation, refining text outputs, correcting symbolic models, agentic behavior with self-reflection.",0,
Interactive Intent Clarification,"User queries are often vague, imprecise, or polysemous, making it difficult for the AI controller to accurately infer the user's intended meaning.","Foundation models interacting with users in real-world tool learning scenarios, where user instructions can be ambiguous or diverse.","The AI controller actively engages with users to clarify any ambiguity in their instructions, for example, by asking follow-up questions or seeking clarifications about a previous user query. This also involves leveraging user feedback to adapt the model to individual users' unique ways of expressing intentions.","More accurate understanding of user intent, leading to more personalized and precise responses and improved user experience.","['Personalized Tool Manipulation', 'Proactive Agent Design']","Dialogue systems, customer service AI, agentic AI, personalized assistants.",0,
Prompt-Based Tool Understanding,The AI controller needs to comprehend the functionalities and usage of available tools to effectively bridge the gap between user intent and the toolset.,Tools are typically accompanied by manuals or tutorials. Foundation models possess strong few-shot and zero-shot learning capabilities.,"Construct suitable task-specific prompts that describe API functionalities (zero-shot prompting, including input/output formats and parameters) or provide concrete tool-use demonstrations (few-shot prompting) to the model.","Foundation models can effectively unravel tool functionalities and understand how to use them proficiently with minimal human effort, and prompts can be easily adjusted for tool modifications.",[],"LLM-powered agents, tool-augmented language models, code generation, robotic control.",0,
Introspective Planning,"Decomposing complex, high-level tasks into subtasks and generating a multi-step plan for tool use without immediate environmental feedback, which can lead to unrealistic or nonsensical plans.","Foundation models with reasoning capabilities are used as controllers for tasks requiring sequential decisions, such as embodied agents or program generation.","The controller directly generates a static, multi-step plan for tool use without knowing intermediate execution results. This can involve generating intermediate reasoning steps (e.g., Python code) or emphasizing actions the agent is permitted to execute to ensure physical grounding.","Models are capable of generating executable programs for agents and anticipating possible anomalies in the plan execution, leading to more physically grounded plans.",['Extrospective Planning (Iterative Replanning)'],"Embodied agents, robotics, complex problem-solving, program-aided language models.",0,
Extrospective Planning (Iterative Replanning),"Introspective planning cannot adapt the plan in response to intermediate execution results, leading to inflexibility and potential failures in complex, dynamic tasks.","Complex tasks (e.g., multi-step Question Answering, embodied learning) where decision-making at each step depends on preceding context and environmental feedback.","The controller generates plans incrementally, one step at a time, by taking environmental feedback and user feedback into account. Subsequent plans are dependent on previous execution results, allowing for dynamic adjustment and regeneration of plans in case of execution failure.","Better adaptation to complex and dynamic tasks, improved accuracy, more feasible plans, ability to handle exceptions, and development of more refined subsequent plans through a closed-loop interaction.","['Introspective Planning', 'Feedback-Driven Tool Learning']","Multi-step Question Answering, embodied learning, dynamic task execution, agentic AI.",0,
Multi-Agent Collaboration,"Complex tasks often demand collaboration among multiple agents, each possessing unique abilities and expertise, which single-agent problem-solving cannot fully address.","Foundation models can simulate human behaviors, including interpersonal communication, making them suitable for modeling individual agents.","Design methods for communication, coordination, and negotiation among multiple agents (each modeled with a foundation model) to ensure seamless collaboration and optimal task execution for complex objectives.",More effective and efficient problem-solving approaches for complex tasks that benefit from diverse perspectives and specialized capabilities.,['Proactive Agent Design'],"Complex task automation, interactive simulations, distributed AI systems.",0,
Formalized Reasoning Augmentation,"Predominant research utilizes plain natural text to facilitate agents' reasoning and planning, which may limit performance in complex reasoning tasks.","LLM-based agents inherently comprehend and generate language, but external formalisms can provide structured and precise reasoning capabilities.","Incorporate external formalisms, such as mathematical tools (e.g., probabilistic graph models, Agentic Process Automation), or non-natural language forms to significantly enhance agents' performance in complex reasoning tasks.","Significantly enhanced agents' performance in complex reasoning tasks, improved decision-making capabilities, and increased controllability of agent behavior.",['Knowledge Conflict Resolution'],"Complex reasoning tasks, decision-making in agentic systems, mathematical problem-solving, robotic process automation.",0,
Parallel Tool Execution,Sequential execution of tools can be inefficient for complex tasks where certain subtasks do not depend on each other.,"Multi-step, multi-tool scenarios where subtasks might be independent and could be processed concurrently.",Determine the dependencies among different subtasks and effectively switch between parallel and sequential execution of tools to optimize efficiency.,Improved execution efficiency for complex tasks by allowing independent subtasks to be processed simultaneously.,[],"Workflow orchestration, complex task automation, multi-tool agents.",1,
Demonstration-Based Tool Learning,"Training foundation models to use tools effectively, especially when direct human guidance is limited or costly.","Human experts can provide tool-use demonstrations, which can be recorded as data.",Train models to mimic expert behavior through imitation learning (behavior cloning). This can be achieved via: 1) Supervised Learning: Finetuning models on human-annotated tool-oriented tasks. 2) Semisupervised Learning: Using less capable models to generate pseudolabels from unlabeled data. 3) Self-supervised Learning: Leveraging in-context learning to bootstrap tool-use examples from a few human-written examples.,"Improved in-domain performance, better out-of-distribution generalization, and reduced reliance on extensive human annotation for tool-use capabilities.","['Feedback-Driven Tool Learning', 'Curriculum Tool Learning']","Robotic control, web-based agents, autonomous vehicles, training LLM agents.",0,
Feedback-Driven Tool Learning,"Manual annotation for tool-use examples is time-consuming and labor-intensive, and models need to adapt to the consequences of their actions in dynamic environments.",Humans learn from trial and error to correct and rectify their tool-use behaviors. Feedback can come from the environment or humans.,"Optimize model parameters through open explorations, using feedback from the environment or humans. This includes: 1) Environment Feedback: Result feedback (task success/failure) or intermediate feedback (state changes). 2) Human Feedback: Explicit (ratings) or implicit (user behavior). 3) Reinforcement Learning from Human Feedback (RLHF): Training a reward model to imitate human preferences for policy optimization.","Models understand action consequences, adapt behaviors, and align with human preferences, leading to improved and more robust tool-use capabilities.","['Demonstration-Based Tool Learning', 'Extrospective Planning (Iterative Replanning)']","Reinforcement learning agents, human-in-the-loop AI, personalized AI systems, LLM alignment.",0,
Unified Tool Interface,Difficulty in knowledge transfer and generalization across a massive and rapidly expanding array of tools due to varied interfaces and protocols.,Models need to manipulate various tools in a consistent and standardized manner to facilitate knowledge transfer.,"Design a standardized interface for tool manipulation. This can be: 1) Semantic Interface: Using specific text spans as action triggers. 2) GUI Interface: Mapping predicted tokens to human-like mouse movements and keyboard inputs in a virtual environment. 3) Programming Interface: Allowing models to specify actions using program code (e.g., function calls).","Models can more easily identify and abstract essential features of tools, facilitating knowledge transfer and adaptation to new scenarios and tools.",['AI-Optimized Tool Design'],"Generalizable AI agents, multi-tool systems, code generation, robotic control.",0,
Meta Tool Learning,"Adapting to unfamiliar situations and transferring tool-use strategies to new tasks or domains, which is a crucial aspect of human intelligence (metacognition).",Models need to generalize tool-use knowledge beyond specific training examples.,"Train the model not just to use a tool, but also to learn the optimal strategy for its use, identifying common underlying principles or patterns in tool-use strategies and transferring them to new contexts.","Models can generalize tool use to different types of problems and become more adaptable and intelligent, aligning with the algorithms and user interface of new tools.",['Curriculum Tool Learning'],"Transfer learning, adaptable AI models, generalization across tools/domains, few-shot learning.",0,
Curriculum Tool Learning,Effectively introducing models to complex tools and building upon prior knowledge in a manageable and effective way.,A pedagogical strategy that starts with simple concepts and gradually introduces more complex ones.,"Start with simple tools and basic operations, then gradually introduce the model to more complex tools and tasks, allowing it to build upon its prior knowledge and develop a deeper understanding of the tool's features and functionalities.","Models master essential features, identify similarities and differences between situations, adjust their approach, and handle a wider range of tasks, enhancing generalization and adaptability.","['Meta Tool Learning', 'Demonstration-Based Tool Learning']","Progressive skill acquisition, complex tool mastery, educational AI, training adaptable models.",0,
AI-Optimized Tool Design,"Most existing tools are specifically designed for human use, making them sub-optimal for AI models due to different information processing methods and interaction paradigms.",AI models interact with tools that were not originally built with AI in mind.,"Create tools specifically suited for AI models by: 1) Modularity: Decomposing tools into smaller, more modular units to make them more adaptable and flexible for AI models. 2) New Input/Output Formats: Developing new input and output formats that are specifically tailored to the needs of AI models for seamless integration and communication.","Improved interaction and utilization of tools by AI models, enabling more fine-grained and compositional use, and better alignment with AI's information processing.","['Unified Tool Interface', 'AI-Driven Tool Creation']","Designing tools for AI agents, improving AI-tool interaction, modular AI systems.",0,
AI-Driven Tool Creation,"The traditional limitation of tool creation to human intelligence, and the need for AI to autonomously develop sophisticated solutions or enhance existing tools.",Large code models can generate executable programs from language descriptions. Foundation models can encapsulate existing APIs into more advanced functions.,"Enable foundation models to autonomously generate executable programs based on language descriptions or encapsulate existing tools/APIs into more advanced, specialized functions.","AI systems transition from merely tool users to tool makers, developing sophisticated solutions, extending existing functionalities, and potentially creating novel tools.",['AI-Optimized Tool Design'],"Automated software development, advanced function encapsulation, scientific discovery, generative AI.",0,
Personalized Tool Manipulation,"Foundation models struggle to process personal information and provide personalized assistance to users with varying needs for tool learning, due to heterogeneous user information and diverse preferences for tool planning and selection.","Users have unique ways of expressing intentions and different preferences for tool planning, selection, and input generation.","Model diverse user information (e.g., language style, social networks) into a unified semantic space, develop personalized tool execution plans based on user preferences, and adaptively generate different inputs for tools based on individual user needs.","Tailored assistance, more personalized tool planning, and adaptive tool calls that align with individual user needs and preferences, enhancing user experience.","['Interactive Intent Clarification', 'Proactive Agent Design']","Personalized assistants, adaptive user interfaces, context-aware AI, dialogue systems.",0,
Proactive Agent Design,"Most foundation models are designed as reactive systems, only responding to user queries without initiating any actions on their own, limiting their utility in dynamic environments.",The desire for AI systems to take action on behalf of the user and continually improve performance based on interaction history.,"Design systems that can initiate actions autonomously by leveraging the history of user interactions, continually improving their performance and tailoring responses to specific users. This requires incorporating safety mechanisms to prevent unintended consequences.","More personalized and seamless user experience, with systems that can anticipate needs and act autonomously, fostering higher-order thinking and decision-making for users.","['Interactive Intent Clarification', 'Personalized Tool Manipulation', 'Multi-Agent Collaboration']","Autonomous agents, intelligent assistants, predictive AI systems, personalized recommendations.",0,
Knowledge Conflict Resolution,"Discrepancies and conflicts arise between a model's internalized knowledge and augmented knowledge from tools, or among knowledge from different tools, leading to inaccurate and unreliable predictions.","Model knowledge can be outdated or contain false beliefs from pretraining data. Tool execution results can be misleading, and different tools may have varying credibility or biases.","Equip models with the ability to detect potential conflicts among different knowledge sources, verify their reliability, choose reliable sources, and provide explanations for their decisions by interpreting which knowledge source is considered and how it is augmented into the final response.","Models can correct their own beliefs, discern knowledge conflicts, adjust responses, and provide explainable and reliable predictions, crucial for high-stakes applications.",['Formalized Reasoning Augmentation'],"Fact-checking AI, reliable information retrieval, medical assistance systems, legal advice AI, robust LLM generation.",0,
Task Decomposition,"Complex, multi-step tasks are difficult for LLM agents to plan directly in a single step.","LLM-based agents facing complicated, real-world tasks with variability.","Adopt the 'divide and conquer' idea, decomposing the complicated task into several simpler subtasks, and then sequentially planning for each subtask. This can be done by decomposing all subtasks first or interleaving decomposition with subtask planning.","Simplifies complex tasks, making them manageable for LLM agents. Reduces the risk of task forgetting and hallucinations (decomposition-first) or improves fault tolerance (interleaved decomposition).","['Multiplan Selection', 'External Planner-Aided Planning', 'Reflection and Refinement', 'Memory-Augmented Planning']","General LLM agent planning for complex tasks. Specific methods include:
- **Decomposition-First:** HuggingGPT (multimodal tasks), PlanandSolve (mathematical, commonsense, symbolic reasoning), ProgPrompt (robot task planning).
- **Interleaved Decomposition:** Chain-of-Thought (CoT) series (reasoning about complex problems), ReAct (decoupling reasoning and planning), Visual ChatGPT (image processing capabilities), PAL (mathematical and symbolic reasoning by generating code), Program-of-Thought (PoT) (formalizing reasoning as programming).",0,
Multiplan Selection,A single plan generated by an LLM is likely to be suboptimal or even infeasible due to task complexity and LLM's inherent uncertainty.,LLM agents generating plans for complex tasks where plan quality and feasibility are critical.,Generate various alternative plans for a task (multiplan generation) and then employ a task-related search algorithm to select the optimal plan (optimal plan selection).,"Provides a broader exploration of potential solutions, increasing the likelihood of finding an optimal or feasible plan.","['Task Decomposition', 'External Planner-Aided Planning', 'Reflection and Refinement', 'Memory-Augmented Planning']","Improving plan quality and robustness for LLM agents. Specific methods include:
- **Multiplan Generation:** Self-consistency (sampling distinct reasoning paths), Tree-of-Thought (ToT) (sample and propose strategies), Graph-of-Thought (GoT) (extends ToT with thought aggregation), LLMMCTS, RAP (LLM as heuristic policy for MCTS).
- **Optimal Plan Selection:** Majority vote (Self-consistency), Tree search algorithms (BFS, DFS in ToT), Monte Carlo Tree Search (LLMMCTS, RAP), A* algorithm (LLM-A*).",0,
External Planner-Aided Planning,"LLMs struggle with intricate constraints (e.g., mathematical problems, generating admissible actions) and efficiency issues, despite their powerful reasoning and task decomposition capabilities.","LLM agents needing to generate plans that adhere to strict constraints or require high efficiency, especially in domains with well-defined planning models.","Integrate LLMs with external planners (symbolic or neural). The LLM primarily formalizes tasks and provides additional reasoning information, while the external planner handles the constrained planning or efficient execution.","Elevates the planning procedure by addressing issues of efficiency and infeasibility of generated plans. Combines LLM's semantic understanding and code generation capabilities with the external planner's precision, theoretical completeness, stability, and interpretability.","['Task Decomposition', 'Multiplan Selection', 'Reflection and Refinement', 'Memory-Augmented Planning']","Planning in environments with intricate constraints, mathematical problem-solving, generating admissible actions, improving planning efficiency. Specific methods include:
- **Symbolic Planner Integration:** LLMP, LLMDP, LLMPDDL (formalizing tasks into PDDL and using solvers like FastDownward, BFS, LPG), LLMASP (transforming problems into ASP and using CLINGO).
- **Neural Planner Integration:** CALM (combining language model with RL-based neural planner for action generation/reranking), SwiftSage (dual-process theory with DT model for fast thinking and LLM for slow thinking).",0,
Reflection and Refinement,"LLM agents may make errors, get stuck in thought loops, or suffer from hallucinations during planning due to limited feedback and insufficient reasoning abilities for complex problems.","LLM agents operating in environments where errors can occur, and fault tolerance and error correction are critical.","Encourage the LLM to reflect on failures or generated plans, generate feedback (self-reflection or external validation), and then refine the plan iteratively. This process mimics reinforcement learning updates through textual feedback.","Enhances fault tolerance and error correction capabilities, allowing agents to correct errors and break out of loops. Improves plan quality and reliability over iterations.","['Task Decomposition', 'Multiplan Selection', 'External Planner-Aided Planning', 'Memory-Augmented Planning']","Improving robustness and reliability of LLM agent planning. Specific methods include: SelfRefine (iterative generation, feedback, refinement), Reflexion (evaluator assesses trajectories, LLM generates self-reflections upon error detection), CRITIC (uses external tools like Knowledge Bases and Search Engines for validation and self-correction), InteRecAgent (ReChain mechanism for self-correction), LEMA (gathers mistaken planning samples, uses powerful LLM for correction, then finetunes).",0,
Memory-Augmented Planning,"LLM agents need to leverage valuable information (commonsense knowledge, past experiences, domain-specific knowledge) to enhance planning capabilities and support growth, but LLMs have context length limitations and may 'forget' information.","LLM agents requiring access to long-term or external knowledge to inform planning, especially for tasks requiring accumulated experience or domain-specific facts.","Enhance planning with an extra memory module where valuable information is stored and retrieved when planning, serving as auxiliary signals. This can involve external retrieval or embedding memories into model parameters.","Enhances planning capabilities, growth, and fault tolerance by providing access to stored knowledge and experiences, overcoming context length limitations.","['Task Decomposition', 'Multiplan Selection', 'External Planner-Aided Planning', 'Reflection and Refinement']","Improving planning with historical data, commonsense knowledge, and domain-specific information. Specific methods include:
- **RAG-based Memory:** Generative Agents, MemoryBank, TiM, RecMind (storing and retrieving text memories), MemGPT (multi-level storage abstraction), REMEMBER (Q-value table for positive/negative memories).
- **Embodied Memory (Finetuning-based):** CALM (finetuning GPT2 with action trajectories), TDT (finetuning Text Decision Transformer with MDP data), AgentTuning (finetuning LLaMA with plan trajectories in dialogue form).",0,
LLM as a Generative Planner,"Existing embodied agents require large labeled datasets for each task, hindering versatility and quick learning. Traditional LLM-based planners often rely on ranking admissible skills, which assumes prior knowledge of the environment and can be inefficient.","Building versatile and sample-efficient embodied agents (e.g., robots) that can follow natural language commands for complex, long-horizon tasks in diverse, partially observable environments.","Utilize a Large Language Model (LLM) to directly generate high-level plans (sequences of subgoals) from natural language instructions, rather than ranking a pre-defined list of admissible skills. This approach obviates the need for extensive a priori environmental knowledge and reduces the number of calls to LLMs.","Enables few-shot planning, significantly reducing data cost and improving sample efficiency. Allows for more versatile agents capable of quickly learning many tasks.","['Hierarchical Planning with LLMs', 'Grounded Replanning', 'Prompt Design for LLM Planning']","['Embodied instruction following', 'Robotics', 'Vision-and-language navigation', 'Rapid task learning for agents']",0,
Hierarchical Planning with LLMs,"Complex, long-horizon tasks are difficult for embodied agents to plan directly. Traditional hierarchical planners may lack the commonsense knowledge or few-shot learning capabilities of LLMs.","Embodied agents needing to execute multi-step instructions in complex, real-world-like environments.","Decompose the overall planning problem into a high-level planner and a low-level planner. The LLM serves as the high-level planner, generating a sequence of abstract subgoals (e.g., 'Navigation potato', 'Pickup potato'). A separate, specialized low-level planner then translates each subgoal into a sequence of primitive actions executable in the environment.","Improves planning efficiency and tractability for long-horizon tasks by leveraging LLMs' commonsense knowledge for high-level reasoning while delegating low-level execution details to a more specialized, efficient component. Makes low-level planning conditionally independent of the natural language instruction.","['LLM as a Generative Planner', 'Grounded Replanning']","['Embodied instruction following', 'Vision-and-language navigation', 'Robotics']",0,
Grounded Replanning,"LLM-generated plans, while plausible, often lack physical grounding to the current environment, leading to unexecutable actions, references to non-existent objects, or the agent getting stuck. Static plans cannot adapt to dynamic environmental changes or execution failures.","Embodied agents executing LLM-generated high-level plans in partially observable, dynamic environments where initial plans may become invalid or suboptimal.","Implement a closed-loop system where the LLM dynamically updates its high-level plan based on environmental feedback. When the agent encounters an issue (e.g., fails an action, takes too long to complete a subgoal), the LLM is re-prompted. This re-prompt includes a description of the current environment (e.g., a list of observed objects from a vision model) and the partial plan already completed. The LLM then generates a new, grounded continuation of the plan. Logit biases can be used to prioritize observed objects.","Enables LLMs to dynamically adapt plans to the current physical reality, overcoming execution failures, producing more robust and physically grounded plans, and improving task completion rates in diverse environments.","['LLM as a Generative Planner', 'Hierarchical Planning with LLMs', 'Prompt Design for LLM Planning']","['Adaptive planning for embodied agents', 'Robust execution in dynamic environments', 'Real-time plan correction']",0,
Prompt Design for LLM Planning,"Unleashing the full potential of LLMs for planning requires careful guidance, as their output quality is highly sensitive to prompt structure, example selection, and output constraints. Achieving few-shot learning and grounding requires specific prompt engineering.","Adapting pre-trained LLMs to generate structured, executable high-level plans for embodied agents with minimal task-specific data and ensuring physical grounding.","Systematically design the LLM prompt to include: 
1.  **Task Explanation:** An intuitive description of the task and allowed high-level actions. 
2.  **In-context Examples:** A small number of exemplar instruction-high-level plan pairs to demonstrate the desired planning behavior (in-context learning). 
3.  **Dynamic Example Retrieval:** For each test case, dynamically select the most relevant in-context examples using a similarity metric (e.g., k-NN on instruction embeddings). 
4.  **Output Constraints/Biases:** Apply logit biases to favor allowable actions and objects, and especially objects observed in the current environment, to guide the LLM towards valid and grounded plans. 
5.  **Contextual Information:** For replanning, include completed subgoals and a list of currently observed objects.","Enables effective few-shot planning, improves the quality and relevance of generated plans, constrains LLM output to valid actions and objects, and facilitates dynamic adaptation and grounding.","['LLM as a Generative Planner', 'Grounded Replanning']","['Few-shot learning for LLM-based agents', 'Guiding LLMs for structured output', 'Improving plan quality and executability', 'Enhancing physical grounding']",0,
Retrieval-Augmented Generation (RAG),"Large pretrained language models (LLMs) exhibit limitations in knowledge-intensive tasks, including: 1) difficulty in accessing and precisely manipulating factual knowledge stored implicitly in their parameters, 2) a tendency to hallucinate or generate factually incorrect information, 3) lack of provenance or explainability for their predictions, 4) inability to easily expand or revise their world knowledge without costly retraining, and 5) lagging performance compared to task-specific architectures on knowledge-intensive tasks.","Designing AI systems for knowledge-intensive Natural Language Processing (NLP) tasks such as open-domain question answering, abstractive summarization, fact verification, and complex text generation. This pattern is applicable when factual accuracy, up-to-date information, and interpretability are critical, and when leveraging large pretrained language models while needing to overcome their inherent limitations regarding external, dynamic knowledge.","Combine a pretrained *parametric memory* (a seq2seq language model, e.g., BART) with an *explicit nonparametric memory* (a dense vector index of external knowledge, e.g., Wikipedia). A pretrained neural retriever (e.g., Dense Passage Retriever - DPR) is used to dynamically access relevant information from the nonparametric memory based on the input query. The retriever converts the input into a dense vector and performs Maximum Inner Product Search (MIPS) against the document index. The generator model then conditions its output on both the original input sequence and the retrieved documents. The entire system is finetuned end-to-end, treating the retrieved documents as latent variables. The document encoder and index can be kept fixed during finetuning for efficiency, while the query encoder and generator are updated. Two main approaches for marginalizing over retrieved documents are: 
1. **RAG-Sequence:** The model uses the same retrieved document to generate the complete output sequence, marginalizing over the top-K retrieved documents to compute the sequence probability.
2. **RAG-Token:** The model can draw a different latent document for each target token during generation, allowing it to combine information from multiple documents. 
The nonparametric memory is designed to be human-readable (raw text) and human-writable, facilitating interpretability and dynamic updates.","1) **Improved Factual Accuracy & Reduced Hallucinations:** Generates more factual, specific, and diverse language, significantly reducing hallucinations compared to parametric-only models. 
2) **State-of-the-Art Performance:** Achieves state-of-the-art results on various knowledge-intensive NLP tasks, including open-domain question answering. 
3) **Dynamic Knowledge Updates:** Enables easy and dynamic updating of the model's world knowledge by simply replacing or editing the nonparametric memory (index hotswapping) without requiring costly retraining of the parametric model. 
4) **Enhanced Interpretability:** The explicit nature of the nonparametric memory (raw text documents) allows for inspection of the accessed knowledge, providing a form of interpretability. 
5) **Flexibility:** Combines the generation flexibility of closed-book models with the performance of open-book retrieval-based approaches. 
6) **Unified Architecture:** Provides a general-purpose finetuning recipe applicable across a wide range of seq2seq tasks.",[],"['Open-domain Question Answering (QA)', 'Abstractive Question Answering', 'Jeopardy Question Generation', 'Fact Verification', 'General knowledge-intensive Natural Language Processing (NLP) tasks']",0,
Cognitive Architecture for Language Agents (CoALA),"Lack of a unified framework to organize existing language agents and guide future development, leading to custom terminology and difficulty comparing agents.","Designing general-purpose language agents that interact with internal state and external environments, leveraging Large Language Models (LLMs) as a core component.","Structure language agents with modular memory components (working, episodic, semantic, procedural), a structured action space (grounding, retrieval, reasoning, learning), and a generalized decision-making process (planning, execution loop). This framework positions the LLM as the central computational unit.","Provides a conceptual framework for characterizing and designing agents, organizes diverse empirical work, and identifies future directions for developing more capable and human-like agents.","['Working Memory for LLM Agents', 'Episodic Memory for LLM Agents', 'Semantic Memory for LLM Agents', 'Procedural Memory for LLM Agents', 'Grounding Actions', 'Retrieval Augmented Generation (RAG) for Agents', 'LLM-based Reasoning', 'Agent Self-Improvement (Learning Actions)', 'Deliberative Decision Making (Planning with LLMs)']","Organizing, designing, and comparing language agents; guiding the development of new LLM-based agents.",0,
Working Memory for LLM Agents,Large Language Models (LLMs) are stateless; information needs to persist across calls and be readily available for the current decision cycle to enable multi-step interactions.,"Language agents engaged in multi-step interactions with environments or users, needing to track current circumstances, active goals, perceptual inputs, and intermediate reasoning results.","Maintain an explicit data structure (working memory) that persists across LLM calls. LLM input is synthesized from a subset of working memory (e.g., a prompt template and relevant variables), and LLM output is parsed back into working memory variables (e.g., action names and arguments).","Enables statefulness for LLM agents, allows the LLM to operate on a consistent and updated context, and serves as a central hub connecting different agent components (long-term memories, grounding interfaces).","['Cognitive Architecture for Language Agents (CoALA)', 'LLM-based Reasoning', 'Deliberative Decision Making (Planning with LLMs)']","Tracking dialogue state, active knowledge, perceptual inputs, goals, intermediate reasoning steps in language agents.",0,
Episodic Memory for LLM Agents,Language agents need to learn from past experiences and retrieve relevant events to inform future decision-making and adapt their behavior over time.,"Agents interacting over multiple episodes (e.g., game trajectories, conversational turns, task executions), requiring the ability to reflect on past behaviors or use prior experiences as examples.","Store experience from earlier decision cycles (e.g., training input-output pairs, history event flows, game trajectories) in a dedicated episodic memory module. During the planning stage of a decision cycle, relevant episodes can be retrieved into working memory to support reasoning.","Supports reasoning by providing concrete examples, enables learning from experience, and allows agents to adapt their behavior based on their history.","['Cognitive Architecture for Language Agents (CoALA)', 'Retrieval Augmented Generation (RAG) for Agents', 'Agent Self-Improvement (Learning Actions)']","Reinforcement learning agents, Generative Agents (Park et al. 2023), Reflexion (Shinn et al. 2023) for reflecting on failed episodes.",0,
Semantic Memory for LLM Agents,"Language agents require access to explicit world knowledge and self-knowledge beyond what is implicitly stored in LLM weights, and the ability to incrementally build this knowledge from experience.","Tasks requiring external knowledge support, reasoning, or decision-making; agents needing to accumulate and store facts about the world or their operational domain.","Store an agent's knowledge about the world and itself (e.g., facts, game manuals, unstructured text, inferences) in a semantic memory module. This memory can be initialized from external databases (e.g., Wikipedia) or updated by LLM-based reasoning actions.","Provides explicit knowledge support, enhances reasoning capabilities, and allows for incremental knowledge building from experience, reducing reliance on the LLM's parametric memory for factual recall.","['Cognitive Architecture for Language Agents (CoALA)', 'Retrieval Augmented Generation (RAG) for Agents', 'Agent Self-Improvement (Learning Actions)', 'LLM-based Reasoning']","Retrieval-augmented NLP, reading to learn in reinforcement learning, Reflexion (Shinn et al. 2023) for storing inferences, Generative Agents (Park et al. 2023) for storing reflections.",0,
Procedural Memory for LLM Agents,"Language agents need to store and execute rules, skills, and decision-making logic, both implicitly (in LLM weights) and explicitly (as agent code), and to learn new procedures to adapt their capabilities.","Agents requiring specific behaviors, algorithms, or the ability to learn new skills; agents needing to adapt their 'source code' or operational logic.","Maintain two forms of procedural memory: implicit knowledge stored in LLM weights and explicit knowledge written in the agent's code. The explicit code includes procedures for actions (reasoning, retrieval, grounding, learning) and decision-making itself. These procedures can be retrieved and executed, and new ones can be learned and written to procedural memory.","Defines agent behavior, enables skill learning and reuse, allows for complex control flow, and supports self-modification of agent logic, leading to more adaptable and capable agents.","['Cognitive Architecture for Language Agents (CoALA)', 'Agent Self-Improvement (Learning Actions)', 'Grounding Actions', 'Retrieval Augmented Generation (RAG) for Agents']","Voyager (Wang et al. 2023a) for maintaining a library of code-based skills, Soar architecture for storing productions.",0,
Grounding Actions,"Large Language Models (LLMs) primarily operate on text, but agents need to interact with and perceive real-world (physical, digital, human dialogue) environments.","Agents deployed in physical robots, digital environments (e.g., games, APIs, websites), or engaging in dialogue with humans or other agents, requiring interaction beyond pure text generation.","Implement procedures that execute external actions in the environment and process environmental feedback into working memory as text. This often involves converting multimodal input (e.g., vision, audio) to text (e.g., via Vision-Language Models) and translating text commands into physical or digital actions.","Simplifies the agent's interaction with the outside world by abstracting it as a text-based interface, enables LLMs to control embodied agents, and allows interaction with diverse and complex environments.","['Cognitive Architecture for Language Agents (CoALA)', 'Working Memory for LLM Agents', 'Procedural Memory for LLM Agents']","Robotics (Ahn et al. 2022), web manipulation (Yao et al. 2022a), dialogue systems, game agents, API interaction (often packaged as 'tools').",0,
Retrieval Augmented Generation (RAG) for Agents,"Large Language Models (LLMs) have limited context windows and may lack specific, up-to-date, or domain-specific information needed for tasks, leading to hallucinations or incomplete responses.","Language agents needing to access and utilize information from their long-term memories (episodic, semantic, procedural) to inform current decisions, generate more accurate responses, or retrieve relevant skills.","Implement procedures to read relevant information from long-term memories into working memory, which then augments the LLM's input prompt. This can involve various retrieval methods such as rule-based, sparse (e.g., BM25), or dense (e.g., embedding-based) retrieval.","Augments the LLM's knowledge base, supports more informed reasoning and decision-making, enables efficient use of stored information, and reduces reliance on the LLM's parametric memory alone, improving factual accuracy and relevance.","['Cognitive Architecture for Language Agents (CoALA)', 'Episodic Memory for LLM Agents', 'Semantic Memory for LLM Agents', 'Procedural Memory for LLM Agents', 'LLM-based Reasoning']","Voyager (Wang et al. 2023a) for skill retrieval, Generative Agents (Park et al. 2023) for event retrieval, DocPrompting (Zhou et al. 2022a) for code generation, general knowledge-intensive NLP tasks.",0,
LLM-based Reasoning,"Agents need to process and generate new information from their current working memory to support learning or decision-making, beyond simple input-output mapping, especially for complex tasks.","Agents needing to summarize observations, distill insights, reflect on trajectories, or process retrieved information; tasks requiring intermediate thought steps, planning, or self-correction.","Use the Large Language Model (LLM) to process the contents of working memory and generate new information (e.g., analyses, plans, reflections, intermediate steps). This often involves specific prompting techniques (e.g., Chain-of-Thought, ReAct format) to elicit targeted reasoning steps, and the generated information can be written back to working memory or to long-term memory.","Generates new knowledge, supports learning, provides additional context for subsequent LLM calls, and enables more sophisticated decision-making and problem-solving capabilities for agents.","['Cognitive Architecture for Language Agents (CoALA)', 'Working Memory for LLM Agents', 'Prompt Engineering & Chaining', 'Agent Self-Improvement (Learning Actions)', 'Deliberative Decision Making (Planning with LLMs)']","ReAct (Yao et al. 2022b) for synergizing reasoning and acting, Reflexion (Shinn et al. 2023) for reflecting on failed episodes, Generative Agents (Park et al. 2023) for generating reflections, Tree of Thoughts (Yao et al. 2023) for deliberate problem solving.",0,
Agent Self-Improvement (Learning Actions),"Language agents need to continuously adapt and improve their capabilities over time based on new experiences and knowledge, beyond initial training or fixed programming.","Agents operating in dynamic environments, requiring lifelong learning, or needing to acquire new skills, knowledge, or modify their internal state or code to enhance performance.","Implement procedures to write information to long-term memory. This encompasses various forms of learning: updating episodic memory with experience, updating semantic memory with knowledge (e.g., inferences, reflections), updating LLM parameters (e.g., finetuning via supervised, imitation, reinforcement learning, human/AI feedback), and updating agent code (procedural memory) for reasoning (e.g., prompt templates), grounding (e.g., code-based skills), or retrieval procedures.","Enables lifelong learning, adaptation to new tasks and environments, acquisition of new skills and knowledge, and self-modification of agent behavior and logic, leading to more robust and autonomous agents.","['Cognitive Architecture for Language Agents (CoALA)', 'Episodic Memory for LLM Agents', 'Semantic Memory for LLM Agents', 'Procedural Memory for LLM Agents', 'LLM-based Reasoning', 'Deliberative Decision Making (Planning with LLMs)']","Reflexion (Shinn et al. 2023) for learning from failed episodes, Voyager (Wang et al. 2023a) for learning new code-based skills, Generative Agents (Park et al. 2023) for generating and storing reflections, finetuning LLMs for specific domains.",0,
Deliberative Decision Making (Planning with LLMs),"Agents need a structured and strategic way to choose which action (grounding, learning, reasoning, retrieval) to apply in a given situation, especially for complex, multi-step tasks where direct action generation is insufficient.","Agents facing multi-step problems, requiring planning, evaluation of alternatives, and selection of the best action; tasks where the consequences of actions need to be considered before execution.","Structure the top-level agent program into decision cycles, each involving a planning stage (propose, evaluate, and select candidate actions using LLM-based reasoning and retrieval) and an execution stage. This can involve iterative proposal and evaluation, and the use of search algorithms (e.g., tree search) to explore potential action sequences.","Enables agents to make more informed and strategic choices, supports complex planning, allows for iterative improvement of candidate solutions, and reduces the myopia of single-step action generation, leading to more robust and goal-oriented behavior.","['Cognitive Architecture for Language Agents (CoALA)', 'LLM-based Reasoning', 'Retrieval Augmented Generation (RAG) for Agents', 'Prompt Engineering & Chaining', 'Working Memory for LLM Agents']","Tree of Thoughts (Yao et al. 2023), RAP (Hao et al. 2023) for implementing BFS/DFS and MCTS with LLMs, SayCan (Ahn et al. 2022) for evaluating actions, ReAct (Yao et al. 2022b) for planning and remaking action plans.",0,
Prompt Engineering & Chaining,"Large Language Models (LLMs) are stateless and their output distribution needs to be biased towards high-quality, task-specific productions; single LLM calls are limited in complexity and multi-step reasoning.","Using LLMs for various tasks, including few-shot learning, question answering, and tasks requiring sequential reasoning, iterative refinement, or the execution of complex algorithms.","Manipulate the LLM's input string (prompt) to control its behavior. This involves preprocessing the input by concatenating additional text, selecting relevant examples, or eliciting targeted reasoning. For multi-step tasks, multiple LLM calls are chained, where the output of one call informs the input of the next, defining a sequence of 'productions'.","Biases the LLM towards desired outputs, enables complex algorithms, facilitates multi-step reasoning, and allows for dynamic, context-sensitive interactions with the LLM.","['LLM-based Reasoning', 'Deliberative Decision Making (Planning with LLMs)']","Question answering, few-shot learning (Brown et al. 2020), dynamic context-sensitive prompts (Liu et al. 2021), self-critique (Wang et al. 2022b), selection-inference (Creswell et al. 2023), multi-step problem solving.",0,
End-to-End Retrieval Augmented Generation for Domain Adaptation (RAGend2end),"Original Retrieval Augmented Generation (RAG) models are not optimized for specialized domains (e.g., healthcare, news) because their external knowledge base and passage encodings are fixed during finetuning, leading to poor domain adaptation. Re-encoding and re-indexing a large knowledge base synchronously during training is computationally expensive and inefficient.","Developing RAG systems for Open-Domain Question Answering (ODQA) that need to perform effectively in specialized, domain-specific knowledge bases, rather than just general Wikipedia-based knowledge. The system requires dynamic adaptation of both its retrieval and generation components to new domains.","Implement a joint training mechanism for RAG where both the neural retriever (Dense Passage Retrieval's question and passage encoders) and the generator (BART seq2seq model) are finetuned simultaneously for the end QA task. Crucially, the external knowledge base's passage encodings and index are updated asynchronously in parallel processes (re-encoding on dedicated GPUs, re-indexing with FAISS on CPUs) to avoid stalling the main training loop. This allows the entire RAG architecture, including its knowledge base, to adapt to the new domain.","Significantly improved domain adaptation performance (Exact Match, F1, Top-k retrieval accuracy) across various specialized domains (COVID-19, News, Conversations) compared to the original RAG. The approach demonstrates superior retriever adaptation compared to standalone retriever finetuning and can be used to train neural retrievers for retrieval-only applications.",['Statement Reconstruction (Auxiliary Training Signal)'],"Open-Domain Question Answering (ODQA) in specialized domains, improving the adaptability of RAG-like models, training domain-specific neural retrievers, reducing the need for gold-standard passages for retriever training.",0,
Statement Reconstruction (Auxiliary Training Signal),"RAG models, even with end-to-end training, can benefit from additional domain-specific knowledge injection to further enhance their understanding and generation capabilities, especially when explicit QA pairs are scarce or insufficient to fully capture domain nuances.","Training Retrieval Augmented Generation (RAG) models for domain adaptation, where the goal is to improve the model's ability to synthesize information from retrieved documents and generate factual, domain-relevant text. This is particularly useful when the model needs to learn to reconstruct or summarize information based on its knowledge base.","Introduce an auxiliary training objective where the RAG model is tasked with reconstructing a given 'statement' (e.g., abstract sentences, news summaries, conversation summaries) by first retrieving relevant passages from its external knowledge base and then generating the statement. A unique control token (e.g., 'p') is prepended to the input to differentiate this task from the primary QA task. The input statements are carefully selected to not be directly present in the knowledge base to prevent simple memorization.","Further improves both the retriever's ability to find relevant information and the generator's accuracy in producing answers, leading to higher overall performance (EM, F1, Top-k retrieval accuracy). It effectively injects more domain-specific knowledge into the model, enabling it to generate statements close to the input based on retrieved context.",['End-to-End Retrieval Augmented Generation for Domain Adaptation (RAGend2end)'],"Enhancing domain adaptation for RAG models, injecting domain-specific knowledge, improving factual consistency and potentially reducing hallucinations in generative models, training better retrievers by forcing them to find information for reconstruction.",0,
Grounded Language Models (GLAM),"Large Language Models (LLMs) possess abstract knowledge but often suffer from a lack of grounding, leading to misalignment with specific environments and limited functional competence in decision-making tasks.","An agent needs to solve decision-making problems in interactive textual environments, using an LLM as its policy. The environment provides observations and sparse, task-conditioned rewards.","Use an LLM as the agent's policy and progressively update it through online Reinforcement Learning (specifically, the PPO algorithm) as the agent interacts with the environment. This involves: 1) using the LLM's language modeling heads to compute conditional probabilities of token sequences for each possible action, and 2) finetuning the LLM (and an added value head) using rewards collected from environmental interactions to achieve functional grounding.","Achieves functional grounding, drastically improving performance, sample efficiency, and generalization abilities for various RL tasks by aligning the LLM's internal knowledge with external environmental dynamics.","['LLM as High-Level Planner', 'Finetuning LLMs on Expert Trajectories (Behavioral Cloning for LLMs)', 'Online Decision Transformer (Pre-training with Offline RL + Online Finetuning)', 'Reinforcement Learning from Human Feedback (RLHF)', 'Distributed LLM Policies (Lamorel)']","Interactive textual environments, spatial and navigation tasks, embodied agents, robotics (implied by related work).",0,
LLM as High-Level Planner,"LLMs can suggest abstract plans but lack direct grounding for low-level actions and real-time interaction with the environment, limiting their functional competence in embodied tasks.","Robotics setups or embodied tasks where LLMs can provide high-level strategic guidance or sequences of actions, but direct control over low-level motor commands or fine-grained environmental interaction is required.","Employ LLMs to generate high-level plans or action sequences. These plans are then executed by a separate, grounded low-level policy or refined through external mechanisms such as affordance functions, a dedicated actor agent, or closed-loop feedback with an environmental 'reporter'. The LLM itself does not directly control low-level actions or learn from direct environmental interaction in this setup.","Leverages the LLM's extensive prior knowledge for complex, long-horizon tasks. However, it requires additional components for grounding and low-level control, as the LLM itself remains ungrounded through direct interaction.",['Grounded Language Models (GLAM)'],"Robotics, embodied reasoning, suggesting action plans in interactive environments.",0,
Finetuning LLMs on Expert Trajectories (Behavioral Cloning for LLMs),"Leveraging existing expert demonstrations to initialize or pre-train LLM-based policies, while acknowledging that purely offline learning may not achieve full functional grounding or optimal performance in interactive environments.","Preparing LLMs to act as policies in interactive environments by utilizing datasets of successful past interactions or expert behavior, often as a starting point before online interaction.","Pre-train or finetune LLMs using Behavioral Cloning (BC) or offline Reinforcement Learning on a dataset of expert trajectories. This teaches the LLM to imitate observed successful actions given states, effectively learning a policy from demonstrations.","Provides a strong initial policy that benefits from expert knowledge, potentially boosting initial performance and sample efficiency. However, it often performs worse than methods that incorporate direct online environmental interaction and grounding, as it lacks the ability to learn from trial-and-error.","['Grounded Language Models (GLAM)', 'Online Decision Transformer (Pre-training with Offline RL + Online Finetuning)']","Initializing LLM policies, leveraging expert demonstrations, pre-training for interactive agents, offline RL for LLMs.",0,
Online Decision Transformer (Pre-training with Offline RL + Online Finetuning),"Combining the advantages of learning from large offline datasets (e.g., leveraging diverse experiences, sample efficiency) with the benefits of online interaction (e.g., adaptation to specific environments, functional grounding) for transformer-based decision-making policies.","Developing robust and adaptable transformer policies for sequential decision-making tasks, where both extensive prior data and real-time environmental interaction are valuable for optimal performance.","First, pre-train a transformer model using offline Reinforcement Learning on a dataset of expert or diverse trajectories. Subsequently, finetune this pre-trained model with online Reinforcement Learning, allowing it to adapt and learn from direct, real-time interactions with the environment.","Creates agents that benefit from both broad offline knowledge and specific online adaptation, potentially leading to more sample-efficient and robust learning compared to purely offline or online approaches.","['Finetuning LLMs on Expert Trajectories (Behavioral Cloning for LLMs)', 'Grounded Language Models (GLAM)']","General decision-making with transformers, combining offline and online learning paradigms, developing adaptable policies.",0,
Reinforcement Learning from Human Feedback (RLHF),"Aligning the outputs and behavior of Large Language Models with complex, often subjective, and evolving human preferences and values, especially in open-ended text generation tasks.","LLMs are used for natural language generation, and the goal is to produce text that is helpful, harmless, and honest, or otherwise aligned with human instructions and expectations. Text generation is framed as a sequential decision-making problem where each token is an action.","Employ Reinforcement Learning (typically Proximal Policy Optimization - PPO) to finetune the LLM. The reward signal for this RL process is provided by a reward model, which itself is trained on a dataset of human preferences (e.g., human rankings of different LLM outputs). This 'human feedback' guides the LLM's learning to produce more desirable outputs.","Produces LLMs that generate text more aligned with human preferences and instructions, often leading to models that are more useful and safer, even with fewer parameters than larger models without such alignment.",['Grounded Language Models (GLAM)'],"Natural language generation, improving LLM alignment with human values/preferences, instruction following, conversational AI, content moderation.",0,
Distributed LLM Policies (Lamorel),"The significant computational cost and time required for online Reinforcement Learning finetuning of large LLMs, particularly when computing action probabilities across multiple parallel environments, making the process intractable.","Scaling the training and inference of LLM-based agent policies in online RL settings, where fast and frequent interactions with multiple environments are necessary to collect sufficient data for learning.","Implement a distributed system using multiple LLM workers running in parallel. A client-server architecture manages the distribution of inference requests (each worker scoring a subset of actions) and aggregates results. This system also supports distributed training, where gradient computations for minibatches are parallelized across the LLM instances and then gathered for model updates.","Overcomes computational bottlenecks, enabling a near-linear reduction in training time with the number of deployed LLM instances. This makes online RL finetuning of large LLMs feasible and scalable, allowing for more extensive experimentation and training.",['Grounded Language Models (GLAM)'],"Scaling online RL finetuning of large LLM policies, high-throughput inference for LLM-powered agents, MLOps for LLM-based systems.",0,
InContext Retrieval-Augmented Language Model (InContext RALM),"Large Language Models (LLMs) inherently lack access to external, up-to-date, or domain-specific knowledge, leading to factual inaccuracies, hallucinations, and an inability to provide source attribution. Existing Retrieval-Augmented Language Model (RALM) approaches often require significant modifications to the LM architecture and dedicated retraining, hindering their widespread adoption and deployment, especially when LMs are accessed via API.","Building or deploying LLM-based applications where factual accuracy, up-to-date information, and source attribution are critical, but direct modification or extensive retraining of the base LLM is impractical, costly, or impossible (e.g., using proprietary LLMs via API). The base LM is frozen and off-the-shelf.","Augment a frozen, off-the-shelf Language Model by dynamically prepending relevant retrieved documents to its input context. This 'document reading' mechanism involves simple concatenation of retrieved text with the input prefix, without altering the LM's architecture or weights. The 'document selection' can initially use general-purpose retrievers (e.g., BM25) and can be further optimized by techniques like reranking. Key parameters like retrieval stride (how often to retrieve) and retrieval query length (how much context to use for the query) are optimized for performance and cost.","Substantial improvements in language modeling performance (e.g., perplexity), mitigation of factual inaccuracies, and provision of a natural source attribution mechanism. Enables the use of retrieval augmentation with pre-trained LMs, even via API access, significantly simplifying deployment and increasing the prevalence of LM grounding.","['Zero-Shot Reranking', 'Predictive Reranking']","Language modeling, open-domain question answering (ODQA), factual text generation, scenarios requiring up-to-date information, applications where LLM fine-tuning is not feasible or desirable.",0,
Zero-Shot Reranking,"Initial document retrieval mechanisms (e.g., lexical search like BM25 or general-purpose dense retrievers) may not optimally rank documents for the specific task of language model generation. They might lack semantic understanding or fail to prioritize documents most relevant to the *upcoming* text, leading to suboptimal grounding for the LLM.","An InContext RALM (or similar retrieval-augmented system) is already in place, providing a set of top-k candidate documents from an initial retriever. The goal is to improve the relevance of the single document (or small set of documents) presented to the LLM, without requiring additional training data or complex model training for the reranker. The generation LM's log probabilities are accessible, or a smaller proxy LM can be used.","Utilize an existing Language Model (either the generation LM itself or a smaller, faster LM) to perform zero-shot reranking of the top-k candidate documents retrieved by an initial retriever. The LM scores each candidate document by evaluating the probability of a short segment of the *target text* (or a proxy, like the immediate prefix) given the document and the current context. The document yielding the highest probability is selected for augmentation.","Improved LM performance by providing more semantically relevant grounding documents compared to using only the top-1 document from the initial retriever. Enables better document selection without dedicated reranker training, making it suitable for scenarios where training data is scarce, computational resources are limited, or when the generation LM is only accessible via API.","['InContext Retrieval-Augmented Language Model (InContext RALM)', 'Predictive Reranking']","Enhancing document selection in RALM systems, improving factual accuracy and coherence of generated text, optimizing retrieval for specific LM tasks, API-constrained LLM environments.",0,
Predictive Reranking,"While zero-shot reranking improves document selection, it might not be fully optimized for the specific nuances of the target LM task and corpus. A more specialized reranker could further enhance the relevance of retrieved documents for LM generation by learning directly from the LM's signal.","An InContext RALM (or similar retrieval-augmented system) is in place, providing candidate documents. Training data from the target corpus is available, allowing for supervised learning. The goal is to train a reranker that is highly specialized in selecting documents that maximize the LM's ability to predict upcoming text.","Train a dedicated, bidirectional reranker (e.g., a fine-tuned transformer-based classifier like RoBERTa) to score the relevance of candidate documents. This reranker is trained in a self-supervised manner, using the generation LM's own probabilities of predicting upcoming text (given a document and prefix) as the target signal. The reranker learns to predict which document will best 'help' the LM, effectively optimizing document selection for the specific LM task and corpus.","Achieves significant additional gains in LM performance compared to off-the-shelf retrievers or zero-shot reranking, by providing highly optimized document selection. This approach leverages domain-specific data to tailor the retrieval process precisely to the LM's needs, leading to lower perplexity and improved generation quality.","['InContext Retrieval-Augmented Language Model (InContext RALM)', 'Zero-Shot Reranking']","Maximizing LM performance in RALM systems, fine-tuning document selection for specific domains or tasks, scenarios where high-quality training data is available for reranker training, improving factual consistency and coherence of generated text.",0,
Retriever-Aware Training (RAT),"Large Language Models (LLMs) struggle to effectively utilize retrieved documentation, especially when it's imperfect or irrelevant, leading to distraction and poor performance. They also struggle to adapt to test-time changes in API documentation (e.g., version updates, argument changes).","Training LLMs for tool usage, specifically API invocation, where documentation is dynamic, vast, and retrievers have imperfect recall.","Augment the instruction-tuned dataset by appending potentially incorrect or irrelevant retrieved documentation to the user prompt, while providing the accurate ground truth in the LLM response. This teaches the LLM to judge the relevance and accuracy of retrieved documents at inference time, using relevant information and ignoring irrelevant context.","Improves LLM accuracy in API invocation, substantially mitigates hallucination, and enables the model to adapt dynamically to test-time changes in API documentation.","['Self-Instruct Finetuning for API Calls', 'Constraint-Aware API Invocation']","Enhancing LLM performance and adaptability in tool-use scenarios, particularly with frequently updated API documentation; building robust LLM agents that interact with external systems.",0,
Self-Instruct Finetuning for API Calls,"Manually creating a large, diverse, and high-quality dataset of natural language instructions and corresponding API calls for finetuning LLMs is labor-intensive and time-consuming.",Developing LLMs capable of accurately generating API calls from natural language prompts across a wide range of functionalities and libraries.,"Leverage a powerful, pre-trained LLM (e.g., GPT-4) to automatically generate synthetic instruction-API pairs. This involves providing a few hand-crafted in-context examples and reference API documentation, then instructing the LLM to generate real-world use cases that invoke specific APIs, ensuring the generated instructions do not contain API names or hints.","Efficiently curates a comprehensive and diverse dataset of instruction-API pairs, enabling effective instruction finetuning of LLMs for accurate API selection and generation.","['Retriever-Aware Training (RAT)', 'Constraint-Aware API Invocation']","Training LLMs for program synthesis, tool invocation, and API generation tasks; dataset generation for specialized LLM applications.",0,
Constraint-Aware API Invocation,"Large Language Models (LLMs) often fail to interpret and respond to user requests that include specific non-functional constraints (e.g., performance, resource usage, accuracy, cost, latency) when selecting or invoking APIs.",Users need LLMs to choose APIs that not only fulfill a functional requirement but also adhere to specific quantitative or qualitative trade-offs and limitations.,"Incorporate instructions containing explicit constraints (e.g., 'model with less than 10M parameters,' 'accuracy of at least 70%') into the LLM's training dataset. This trains the model to comprehend both the functional description and the embedded constraint parameters, enabling it to reason about and categorize API calls accordingly.","Enables LLMs to make more nuanced and appropriate API selections by considering and respecting user-defined constraints, leading to more tailored and practical outputs.","['Retriever-Aware Training (RAT)', 'Self-Instruct Finetuning for API Calls']",Building agentic LLMs that can make informed decisions in complex environments; personalized tool recommendations; resource-optimized task execution; intelligent API gateways.,0,
AST-based API Verification and Hallucination Metric,Evaluating the functional correctness of LLM-generated API calls is challenging due to the existence of multiple functionally equivalent solutions and the impracticality of executing every generated code. Identifying and quantifying hallucination (imagined API calls) is also difficult.,"Assessing the accuracy, correctness, and reliability of LLMs in generating code or API calls for program synthesis and tool-use tasks.","Utilize Abstract Syntax Tree (AST) subtree matching to compare the structure of LLM-generated API calls against a curated database of known, correct API calls. Functional correctness is determined by whether the generated API's AST is a subtree of a reference API's AST, accounting for optional arguments. Hallucination is specifically defined and measured as an API call whose AST is not a subtree of *any* API in the database, indicating an entirely imagined or non-existent tool.","Provides a robust, scalable, and offline evaluation metric that accurately measures both functional correctness and the rate of hallucination in LLM-generated API calls, showing strong correlation with human evaluation.",[],"Benchmarking LLMs for code generation, API invocation, and program synthesis; developing more reliable LLM-powered coding assistants; automated code review for LLM-generated code.",0,
ThinkonGraph (ToG),"Large Language Models (LLMs) struggle with deep, responsible, and multi-hop knowledge reasoning, often leading to hallucinations or an inability to answer questions requiring specialized or up-to-date knowledge. Existing loose-coupling LLM-KG integration paradigms (e.g., RAG) treat LLMs as mere translators for KG queries, limiting their direct participation in graph reasoning and heavily relying on the KG's completeness and quality.","AI systems requiring LLMs to perform complex, knowledge-intensive tasks that necessitate active exploration and reasoning over structured external knowledge sources like Knowledge Graphs (KGs), where explainability and dynamic decision-making are crucial.","The LLM acts as an agent to interactively explore and reason over a Knowledge Graph (KG). It iteratively performs a beam search on the KG to discover and refine reasoning paths. The process involves:
1.  **Initialization:** The LLM identifies initial topic entities from the input question.
2.  **Iterative Exploration:** In each step, the LLM performs a two-step exploration:
    *   **Relation Exploration (Search & Prune):** The LLM searches for candidate relations linked to the current tail entities and then prunes them to select the top-N most relevant relations.
    *   **Entity Exploration (Search & Prune):** The LLM searches for candidate entities connected by the selected relations and prunes them to select the top-N most relevant entities.
3.  **Iterative Reasoning:** After each exploration step, the LLM evaluates if the current top-N reasoning paths are sufficient to answer the question. If so, it generates the answer. If not, it continues the exploration.
4.  **Fallback:** If the maximum search depth is reached without a conclusive answer, the LLM generates an answer based solely on its inherent knowledge.","Significantly enhances LLMs' deep and responsible reasoning capabilities for knowledge-intensive tasks by extracting diverse and multi-hop reasoning paths. Mitigates hallucination issues. Provides explicit, editable reasoning paths, improving explainability and enabling knowledge traceability and correctability. Offers a flexible, plug-and-play framework for various LLMs and KGs, and can enable smaller LLMs to achieve performance competitive with larger models, reducing deployment costs.","['Relation-based ThinkonGraph (ToGR)', 'Knowledge Traceability and Correctability', 'LLM-KG (Loose Coupling)', 'Lightweight Pruning']","Knowledge Base Question Answering (KBQA), Open-domain Question Answering, Slot Filling, Fact Checking.",0,
Relation-based ThinkonGraph (ToGR),"The full ThinkonGraph (ToG) approach, which uses LLM-constrained pruning for both relations and entities, can incur high computational costs and reasoning time due to numerous LLM calls. Additionally, the literal information of intermediate entities might be missing or unfamiliar to the LLM, potentially leading to misguided reasoning.","Scenarios similar to ToG, but where efficiency is a higher priority, or when the quality/completeness of literal entity information in the KG is a concern.","A variant of the ThinkonGraph (ToG) pattern that focuses on exploring top-N *relation chains* (e.g., `e0 -> r1 -> r2 -> ... -> rD`) rather than full triple-based reasoning paths. It follows the same iterative structure as ToG for relation search and pruning. However, for entity pruning, it employs a 'random prune' strategy, randomly sampling N entities from the candidate set, instead of using the LLM for selection.","Reduces overall computational cost and reasoning time by eliminating the need for LLM calls during entity pruning. Enhances robustness by primarily emphasizing the literal information of relations, thereby mitigating the risk of misguided reasoning when intermediate entity literal information is missing or unfamiliar to the LLM.","['ThinkonGraph (ToG)', 'Lightweight Pruning']","Knowledge Base Question Answering (KBQA), Open-domain Question Answering, Slot Filling, Fact Checking, especially in cost-sensitive or data-sparse environments.",0,
Knowledge Traceability and Correctability,"LLM reasoning processes often lack transparency, explainability, and a direct mechanism for identifying and correcting errors or updating outdated knowledge within the underlying knowledge base. This leads to issues like hallucination, reduced user trust, and challenges in maintaining the quality and currency of external knowledge sources.","AI systems, particularly those integrating LLMs with Knowledge Graphs (KGs), where explainability, user trust, continuous improvement of knowledge, and the ability to debug reasoning paths are critical.","The system generates and displays explicit reasoning paths (e.g., sequences of triples or relation chains) used by the LLM to derive an answer. If human users, experts, or even other LLMs identify potential errors, uncertainties, or outdated information in the system's output, the explicit paths allow for:
1.  **Tracing:** Pinpointing the exact triples or knowledge segments that led to the erroneous conclusion.
2.  **Correction:** Facilitating the direct correction of suspicious or incorrect knowledge within the KG. This process can also lead to 'knowledge infusion,' where LLM's inherent knowledge or expert feedback improves the KG.","Improves the explainability, transparency, and responsibility of LLM reasoning. Enables a human-in-the-loop mechanism for debugging and improving AI system outputs. Facilitates the continuous improvement and maintenance of Knowledge Graph quality, reducing the cost of KG construction and correction. Enhances user trust and mitigates hallucination by providing verifiable reasoning.",['ThinkonGraph (ToG)'],"Debugging and auditing LLM-powered systems, Knowledge Graph curation and maintenance, Human-in-the-loop AI, Fact-checking applications, enhancing trust in AI outputs.",0,
LLM-KG (Loose Coupling),"Large Language Models (LLMs) often struggle with knowledge-intensive tasks, exhibiting limitations such as hallucination, an inability to access specialized or out-of-date knowledge, and difficulties with long logic chains or multi-hop reasoning.","LLM applications that require access to external, structured, and explicit knowledge beyond what is contained in their pre-training data, typically from Knowledge Graphs (KGs).","Integrate external Knowledge Graphs (KGs) with LLMs by following a fixed pipeline:
1.  **Retrieve:** Information relevant to the input question is retrieved from the KG.
2.  **Augment:** The retrieved knowledge is translated into a textual format and used to augment the LLM's input prompt.
3.  **Generate:** The LLM then generates a response based on the augmented prompt. In this paradigm, the LLM primarily acts as a translator, converting natural language questions into machine-understandable commands for KG searching, but does not directly participate in the graph reasoning process.","Mitigates hallucination and improves LLM reasoning by providing external, factual context. Offers a complementary strategy to address LLM limitations regarding specialized or current knowledge.",['ThinkonGraph (ToG)'],"Knowledge Base Question Answering (KBQA), fact-checking, information retrieval, general knowledge-intensive NLP tasks.",0,
Lightweight Pruning,"AI systems that involve iterative search and pruning steps, especially those leveraging Large Language Models (LLMs) for decision-making (like pruning reasoning paths in a KG), can incur significant computational costs and inference time due to frequent LLM calls.","Deploying iterative AI reasoning frameworks (e.g., ThinkonGraph) in environments where computational resources are limited, inference speed is critical, or cost-efficiency is a primary concern, and a slight trade-off in accuracy is acceptable.","In the pruning steps of an iterative search process, replace the computationally expensive LLM calls with more lightweight, faster models (e.g., BM25, SentenceBERT). These lightweight models are used to evaluate and select the top-N candidates (e.g., entities or relations) based on criteria like literal similarity to the input query, instead of relying on the LLM's more sophisticated reasoning for pruning.","Drastically reduces the number of LLM calls and overall computational complexity (e.g., from O(ND) to O(D) in ToG), leading to faster inference and lower operational costs. While it may lead to some performance degradation compared to LLM-based pruning, this can sometimes be partially offset by increasing the beam width of the search.","['ThinkonGraph (ToG)', 'Relation-based ThinkonGraph (ToGR)']","Optimizing the efficiency of iterative LLM-agentic systems, cost-sensitive deployments, real-time applications, scenarios where a balance between accuracy and speed is desired.",0,
Retrieval Augmented Generation (RAG),"Large Language Models (LLMs) often suffer from knowledge cutoffs, hallucinate factual information, and lack access to real-time or private domain-specific data, limiting their utility in knowledge-intensive tasks.","Applications requiring LLMs to provide accurate, up-to-date, or domain-specific answers based on external, verifiable knowledge sources, rather than solely relying on their pre-trained internal knowledge.","Integrate a pre-trained LLM with an external retrieval system. When a query is received, the retrieval system first fetches relevant documents or passages from a knowledge base (e.g., vector database, search index). These retrieved documents are then provided as additional context to the LLM, which uses this context to generate a more informed and accurate response.","Reduces factual errors and hallucinations, enables LLMs to access and incorporate external, up-to-date, and domain-specific information, and improves the overall factual accuracy and relevance of generated responses.",Retrieval Augmented Fine-Tuning (RAFT),"Question Answering systems, chatbots for specific domains (e.g., customer support, legal, medical), information retrieval, knowledge-intensive NLP tasks, enterprise search.",0,
Retrieval Augmented Fine-Tuning (RAFT),"Pretrained LLMs, even when used with RAG, struggle to effectively utilize retrieved documents in specialized domains, especially when retrieval is imperfect (containing distractor documents). Existing finetuning methods often fail to account for the open-book nature of RAG at test time or the presence of irrelevant context.","Adapting LLMs for domain-specific RAG applications (e.g., medical QA, enterprise documents, API documentation) where maximizing accuracy based on a given set of documents is critical, and the model needs to be robust to noisy or irrelevant retrieved information.","A specialized finetuning recipe for LLMs that explicitly trains the model to leverage retrieved documents and handle distractors. The training data is constructed as follows:
1.  **Contextualized QA Pairs**: Each training instance includes a question, a set of documents (comprising both 'golden' relevant documents and 'distractor' irrelevant documents), and a Chain-of-Thought (CoT) style answer.
2.  **Chain-of-Thought with Citations**: The answers are generated with detailed reasoning steps and explicitly cite verbatim sequences from the relevant 'golden' documents. This teaches the model to reason and attribute information.
3.  **Distractor Document Training**: The model is trained with distractor documents present in the context, compelling it to learn to identify and ignore irrelevant information.
4.  **Negative Context Sampling (Varying Golden Document Presence)**: For a certain proportion (P-fraction) of the training data, the 'golden' document is intentionally omitted, leaving only distractor documents. This encourages the model to be more robust, potentially memorizing some answers or relying on its internal knowledge when the perfect context isn't available.","Significantly improves the LLM's ability to answer questions accurately in domain-specific RAG settings, enhances robustness against distractor documents, improves context comprehension, and prevents overfitting to concise answers. The model learns to better read, extract, and reason from provided documents.",Retrieval Augmented Generation (RAG),"Domain-specific Question Answering, specialized code generation from documentation, legal document analysis, medical information retrieval, any application requiring high-accuracy LLM responses from a specific, potentially noisy, document collection.",0,
Knowledge-Grounded Generation,"Large Language Models (LLMs) tend to hallucinate and lack access to up-to-date or domain-specific external knowledge, leading to ungrounded or inaccurate responses, especially for mission-critical applications.","Building AI systems that require LLMs to generate factually accurate and reliable responses based on external, dynamic, or proprietary information, particularly when fine-tuning the LLM is prohibitively expensive or impossible (black-box LLMs).","Augment the LLM's input prompt with consolidated evidence retrieved from external knowledge sources. This involves a modular 'Knowledge Consolidator' component that:
1.  **Retrieves:** Generates search queries based on user input and dialog history, then calls APIs (e.g., web search, task-specific databases) to fetch raw evidence.
2.  **Links:** Enriches raw evidence by identifying and linking entities to related contextual information (e.g., Wikipedia descriptions).
3.  **Chains:** Prunes irrelevant information and forms coherent 'evidence chains' that are most relevant to the query.
This consolidated evidence is then explicitly included in the prompt provided to the LLM.","LLM generates responses that are factually grounded in external knowledge, significantly reducing hallucinations and improving the accuracy and trustworthiness of its outputs without requiring model fine-tuning.","['Automated Feedback Loop for LLM Refinement', 'Agentic Iterative Self-Refinement']","Open-domain question answering, information-seeking dialog, customer service, factual content generation, any application where LLM responses must be verifiable and accurate.",0,
Automated Feedback Loop for LLM Refinement,"Initial LLM-generated responses may not consistently meet desired quality criteria (e.g., factuality, coherence, task-specific alignment, safety) and require iterative improvement without direct human intervention for every turn.","Developing robust AI agents or systems that leverage black-box LLMs where direct model parameter modification is not feasible, and continuous self-correction is needed to align outputs with specific requirements.","Implement a 'Utility' module that evaluates candidate LLM responses and generates actionable feedback. This module:
1.  **Evaluates:** Assigns a utility score to a candidate response based on a set of task-specific utility functions (e.g., model-based functions trained on human preferences for fluency, informativeness, factuality; or rule-based functions checking compliance).
2.  **Generates Feedback:** If the response's utility score falls below a threshold, a verbalized feedback message is generated (e.g., 'The response is inconsistent with the knowledge. Please generate again.'). This feedback can be generated by a text generation model or rule-based natural language generator.
This feedback is then used to revise the prompt for the next LLM generation attempt.","Enables the LLM system to self-correct and iteratively refine its responses, leading to higher quality, better-aligned, and more reliable outputs by guiding the LLM towards desired characteristics.","['Knowledge-Grounded Generation', 'Agentic Iterative Self-Refinement']","Reducing hallucination, improving factual consistency, aligning responses with conversational goals, enhancing safety and compliance, self-criticism for LLMs.",0,
Agentic Iterative Self-Refinement,"Complex, multi-step tasks or dynamic conversational scenarios often require an AI system to adapt, learn, and refine its actions and responses over time, especially when dealing with the inherent variability and potential for errors in LLM outputs.","Designing AI agents that can engage in long-horizon interactions, perform multi-hop reasoning, or handle tasks requiring continuous adaptation and improvement, often by orchestrating multiple AI components and external tools.","Structure the AI system as an agent operating within a Markov Decision Process (MDP) framework, employing a continuous loop of observation, action, and refinement. Key components include:
1.  **Working Memory:** Maintains the current dialog state, including user queries, retrieved evidence, candidate responses, utility scores, and feedback.
2.  **Policy:** Selects the next optimal action (e.g., acquire evidence, query LLM, send final response) based on the current state. This policy can be rule-based or learned (e.g., via reinforcement learning).
3.  **Action Executor:** Carries out the selected action, which may involve:
    -   **Knowledge Consolidator:** Retrieving and processing external knowledge.
    -   **Prompt Engine:** Constructing prompts for the LLM, incorporating context, knowledge, and feedback.
4.  **Utility Module:** Evaluates LLM responses and generates feedback.
The agent iteratively revises its prompts and re-queries the LLM based on the feedback and consolidated knowledge until a satisfactory response is achieved or a stopping condition is met.","Creates a robust and adaptive AI agent capable of handling complex tasks, performing multi-hop reasoning, and significantly improving the quality and groundedness of LLM responses through continuous self-correction and external knowledge integration.","['Knowledge-Grounded Generation', 'Automated Feedback Loop for LLM Refinement']","Complex conversational AI, multi-step task execution, autonomous agents, systems requiring dynamic adaptation and self-improvement.",0,
Multi-Agent Conversation,"Enhancing LLM capabilities for complex tasks, encouraging divergent thinking, improving factuality/reasoning, providing guardrails, combining broad LLM capabilities, facilitating task partitioning and integration.","LLM applications spanning a broad spectrum of domains and complexities, where a single LLM or agent is insufficient.",Employ multiple cooperating agents that converse with each other or humans to accomplish tasks. Leverage chat-optimized LLMs' ability to incorporate feedback and combine capabilities modularly.,"Enhanced agent capabilities, improved factuality and reasoning, modular combination of LLM capabilities, intuitive task partitioning and integration, and guardrails.","['Customizable Conversable Agent', 'Automated Agent Chat (Autoreply Mechanism)', 'Two-Agent Chat', 'Sequential Chat', 'Nested Chat', 'Group Chat', 'Dynamic Conversation Flow', 'Multi-Agent Coordination (Commander-Subordinate)', 'Role-Based Agent Specialization', 'Adversarial Agent Interaction']","Mathematics, coding, question-answering, supply chain optimization, online decision-making, entertainment",0,
Customizable Conversable Agent,"Designing individual agents that are capable, reusable, customizable, and effective in multi-agent collaboration, adapting to diverse application needs.","Building LLM applications where agents need specific roles, responsibilities, and capabilities (LLMs, human inputs, tools).","Design agents as conversable entities with specific roles, capable of sending/receiving messages, maintaining internal context, and configured with a mix of capabilities (LLMs, human inputs, tools). Developers can reuse or extend built-in agents.","Flexible agent behaviors, reusability, modular combination of LLM capabilities, ability to hold multi-turn conversations autonomously or with humans in the loop.","['Multi-Agent Conversation', 'Human-in-the-Loop Agent', 'Tool-Augmented Agent', 'Role-Play Prompting']","Math problem solving, retrieval-augmented QA, decision making in embodied agents, supply chain optimization, conversational chess",0,
Human-in-the-Loop Agent,"Many LLM applications require human involvement for feedback, oversight, or to solve challenging problems that LLMs cannot solve autonomously.","LLM applications where human input is desired or essential, such as math problem solving, interactive retrieval, or dynamic task solving.","Configure a conversable agent (e.g., UserProxyAgent) to solicit human inputs at certain rounds of a conversation, with configurable involvement levels and patterns (e.g., frequency, conditions for requesting input, option to skip).","Effective incorporation of human feedback, ability to solve challenging problems, enhanced user experience, improved safety and alignment.","['Customizable Conversable Agent', 'Multi-Agent Conversation']","Math problem solving (autonomous, human-in-the-loop, multi-user), interactive retrieval in QA, dynamic task solving, conversational chess",0,
Tool-Augmented Agent,"LLMs have limitations in accessing real-time information, performing complex computations, or interacting with external environments.","Tasks requiring code execution, function calls, or interaction with external systems (e.g., databases, web APIs, simulated environments).","Equip a conversable agent with the capability to execute tools via code execution or function execution, allowing it to perform actions suggested by LLMs.","Extends LLM capabilities beyond their training data, enables interaction with external environments, facilitates complex task completion.","['Customizable Conversable Agent', 'LLM as a Planner (with Executor)', 'Retrieval Augmentation']","Math problem solving (code interpreter), retrieval-augmented QA (vector database), decision making in embodied agents (executor agent), supply chain optimization (Python execution), web interaction tasks",0,
Automated Agent Chat (Autoreply Mechanism),"Managing complex multi-agent conversation flows without explicit, centralized control, and streamlining the development of intricate applications.","Multi-agent systems where agents need to interact autonomously and dynamically, and developers desire a decentralized workflow.","Agents have unified conversation interfaces (send/receive, generate_reply, register_reply) and automatically invoke generate_reply upon receiving a message, sending a reply back unless a termination condition is met. Custom reply functions can be registered to customize behavior.","Decentralized, modular, and unified workflow definition; natural induction of conversation flow; reduced need for an extra control plane; streamlined development.","['Multi-Agent Conversation', 'Dynamic Conversation Flow', 'Two-Agent Chat', 'Sequential Chat', 'Nested Chat', 'Group Chat']","General multi-agent application development in AutoGen, implementing various conversation patterns",0,
Hybrid Control (Natural Language & Code),"Providing flexible and powerful control over agent conversation flow and behavior, accommodating different levels of abstraction for developers.","Developers need to program agent interactions, sometimes preferring natural language for high-level guidance and code for precise logic, and requiring seamless transitions between them.","Control conversation flow by prompting LLM-backed agents with natural language (e.g., system messages, instructions) and/or using programming language (e.g., Python code for termination conditions, human input mode, tool execution logic, custom reply functions). Supports flexible transition between the two.","Flexible control flow management, ability to guide agents with high-level instructions or precise logic, enhanced programmability, and adaptability.","['Customizable Conversable Agent', 'Automated Agent Chat (Autoreply Mechanism)', 'Role-Play Prompting', 'Grounding Prompting']","Configuring AssistantAgent system messages, defining termination conditions, custom reply functions, LLM-proposed function calls",0,
Two-Agent Chat,"Simple, direct collaboration between two agents for task completion, often as a foundational interaction pattern.","Tasks that can be effectively solved through a back-and-forth dialogue between two specialized agents, or as a building block for more complex patterns.","Configure two conversable agents to initiate and respond to messages from each other, typically with one acting as an assistant and the other as a user proxy or executor.","Straightforward implementation for many problems, effective for tasks like math problem solving or QA, and easily extensible.","['Multi-Agent Conversation', 'Automated Agent Chat (Autoreply Mechanism)', 'Sequential Chat', 'Nested Chat', 'Group Chat']","Math problem solving, retrieval-augmented QA, decision making in embodied agents",0,
Sequential Chat,"Tasks requiring a sequence of interdependent multi-agent conversations, where the output of one chat informs the next.","Complex tasks that can be broken down into a series of sub-tasks, each handled by a two-agent chat, executed in a predefined order.","Orchestrate a sequence of two-agent chats, where each chat is initiated after the completion of the previous one, allowing for a structured progression through multi-step problems.","Enables execution of complex tasks through coordinated chat sequences, beneficial for multi-step problems, and provides a clear workflow.","['Multi-Agent Conversation', 'Two-Agent Chat', 'Automated Agent Chat (Autoreply Mechanism)']",Multi-step problem solving,0,
Nested Chat,"An agent needs to perform an internal sub-task, self-reflection, or 'inner monologue' by consulting other agents before replying to its original sender.","An agent requires additional information, critique, or a complex sub-computation that can be delegated to other agents without interrupting the main conversation flow.","A receiver agent, upon receiving a message, invokes a new, internal conversation (nested chat) with other agents. This nested chat completes its task, and its result informs the receiver agent's reply to the original sender. This is often achieved by registering a custom reply function.","Allows agents to create an 'inner monologue,' enables self-reflection, facilitates recursive composition of agents, and handles complex sub-tasks efficiently.","['Multi-Agent Conversation', 'Automated Agent Chat (Autoreply Mechanism)', 'Inner Monologue (via Nested Chat)']","Self-reflection, complex agent composition, critique, internal sub-computations",0,
Group Chat,"Dynamic task solving where the exact workflow cannot be predetermined, requiring flexible collaboration among multiple agents with shared context.","Scenarios where collaboration without strict communication order is beneficial, such as coding, web scraping, or general dynamic task solving, involving more than two agents.","Participating agents share a common context and converse dynamically. A GroupChatManager agent serves as a conductor, dynamically selecting a speaker, collecting responses, and broadcasting messages. Speaker selection can be guided by role-play prompts.","Enables dynamic task solving, fosters flexible collaboration, often leading to higher success rates and fewer LLM calls compared to fixed workflows.","['Multi-Agent Conversation', 'Dynamic Conversation Flow', 'Automated Agent Chat (Autoreply Mechanism)', 'Role-Play Prompting']","Dynamic task solving (e.g., coding, web scraping), multi-user problem solving",0,
Dynamic Conversation Flow,"Predefined conversation orders are insufficient for complex, unpredictable tasks where the next step or speaker depends on the ongoing conversation status and context.","Multi-agent systems requiring adaptive workflows, such as group chats or scenarios where agents need to make real-time decisions about who to interact with next.","Implement custom reply functions and triggers (e.g., for nested chat or group chat speaker transitions) or use LLM-driven function calls to dynamically decide the conversation flow based on the current conversation status and context.","Enables flexible and adaptive multi-agent workflows, suitable for dynamic task solving, and improves efficiency by guiding interactions.","['Group Chat', 'Nested Chat', 'Automated Agent Chat (Autoreply Mechanism)']","Dynamic task solving, group chat speaker selection, LLM-driven function calls",0,
Retrieval Augmentation,"LLMs have intrinsic limitations regarding factual accuracy, access to up-to-date information, or knowledge of private/domain-specific data.","Question-answering, code generation, or any task requiring external, up-to-date, or proprietary knowledge that is not part of the LLM's training data.","Integrate a context retriever (e.g., vector database) with an LLM-backed agent. The agent retrieves relevant documents/chunks based on the query and uses them as context for generating responses.","Mitigates LLM limitations, improves factuality, enables use of external/private knowledge, boosts performance on knowledge-intensive tasks.","['Tool-Augmented Agent', 'Interactive Retrieval']","Question-answering (Natural Questions dataset), code generation based on specific codebases",0,
Interactive Retrieval,"Initial retrieval attempts might not provide sufficient or relevant context, leading to incomplete or incorrect LLM responses, and the system needs a mechanism to request more information.",Retrieval-augmented systems where the LLM needs to signal when more context is required to answer a question or complete a task.,"The LLM-based assistant is instructed to reply with a specific phrase (e.g., 'UPDATE CONTEXT') if it cannot find information in the current context. This triggers the user proxy agent to retrieve more relevant chunks and update the context for the assistant, initiating further retrieval attempts.","Improves performance on QA tasks, enables more robust retrieval augmentation, reduces 'I don't know' responses when information is available but not initially retrieved, and allows for dynamic context updates.","['Retrieval Augmentation', 'Human-in-the-Loop Agent']",Question-answering,0,
LLM as a Planner (with Executor),"Embodied agents or systems need to decompose high-level goals into actionable steps and execute them in an environment, often requiring iterative planning and feedback.","Interactive decision-making tasks in simulated or real-world environments (e.g., ALFWorld, MiniWoB) where an agent needs to generate and execute a sequence of actions.","An LLM-backed assistant agent generates plans and action decisions (e.g., using ReAct prompting), which are then executed by a separate executor agent. The executor agent interacts with the environment, performs actions, and reports results/observations as feedback back to the assistant.","Enables LLMs to tackle complex, multi-step tasks in interactive environments, facilitates task decomposition and execution, and allows for iterative refinement of plans based on environmental feedback.","['Tool-Augmented Agent', 'Grounding Agent']","ALFWorld (household environments), MiniWoB (web interaction tasks)",0,
Grounding Agent,"LLM-based agents may lack commonsense knowledge, struggle with physical world constraints, or get stuck in repetitive errors, especially in interactive or rule-bound environments.","Decision-making tasks where agents need to adhere to physical laws, game rules, or commonsense knowledge to avoid flawed plans or error loops.","Introduce a specialized agent that supplies crucial commonsense knowledge or rule reminders to the decision-making agent, particularly when early signs of recurring errors are detected or at the start of a task.","Significantly enhances the system's ability to avoid error loops, improves adherence to rules and commonsense, boosts task success rate, and provides a modular way to inject external knowledge.","['LLM as a Planner (with Executor)', 'Game-Playing Agent (with Rule Enforcement)']","ALFWorld (commonsense knowledge), Conversational Chess (game rules)",0,
Multi-Agent Coordination (Commander-Subordinate),"Orchestrating complex workflows involving multiple specialized agents to achieve a common goal, managing communication flow, and maintaining context/memory across interactions.","Applications requiring a division of labor among agents, where one agent oversees and directs others, and user interactions need to be managed (e.g., supply chain optimization).","Design a 'Commander' agent responsible for receiving user questions, coordinating with specialized 'subordinate' agents (e.g., Writer, Safeguard), directing communication flow, and handling memory tied to user interactions. The Commander executes code and requests interpretation.","Streamlined complex workflows, improved productivity, effective division of labor, context-aware decision-making, and reduced manual intervention.","['Role-Based Agent Specialization', 'Adversarial Agent Interaction', 'Multi-Agent Conversation']",Supply chain optimization (OptiGuide),0,
Role-Based Agent Specialization,"Complex tasks benefit from dividing responsibilities among agents with distinct expertise, improving efficiency and focus.","Multi-agent systems where different aspects of a task (e.g., code generation, safety checking, interpretation) can be handled by dedicated agents.","Assign specific roles and responsibilities to individual agents (e.g., Writer for crafting code and interpretation, Safeguard for checking code safety), allowing them to focus on their area of expertise and contribute to a larger workflow.","Improved performance, modularity, reduced code complexity, better handling of specialized sub-tasks, and clear accountability for different parts of the workflow.","['Multi-Agent Coordination (Commander-Subordinate)', 'Adversarial Agent Interaction']","Supply chain optimization (Writer, Safeguard agents)",0,
Adversarial Agent Interaction,"Ensuring quality, safety, or correctness of outputs by having agents critically evaluate each other's work, preventing errors or malicious actions.","Tasks where generated outputs (e.g., code, plans) need to be validated against specific criteria, security concerns, or potential risks before execution or finalization.","Design an agent (e.g., Safeguard) to act as a 'virtual adversarial checker,' critically screening the outputs of other agents (e.g., generated code) and raising issues if criteria are not met (e.g., security red flags, execution failures).","Improved safety, quality assurance, prevention of errors or malicious outputs, and a robust validation mechanism within the multi-agent system.","['Role-Based Agent Specialization', 'Multi-Agent Coordination (Commander-Subordinate)']",Supply chain optimization (Safeguard checking code safety),2,
Game-Playing Agent (with Rule Enforcement),"Developing interactive game applications where agents (human or AI) play by specific rules, and moves need validation to maintain game integrity.","Conversational games (e.g., chess) where players communicate in natural language, and a system needs to parse moves, apply game rules, and provide feedback.","Implement player agents (human or LLM-powered) and a separate 'board agent' that acts as a third party. The board agent provides game information, parses natural language moves into structured formats (e.g., UCI), validates moves against standard rules, and responds with errors if moves are illegal, prompting players to re-propose.","Natural, flexible, and engaging game dynamics; maintained game integrity; reduced hallucination of invalid moves by player agents; simplified development through modular design.","['Grounding Agent', 'Human-in-the-Loop Agent', 'Customizable Conversable Agent']",Conversational Chess,0,
Inner Monologue (via Nested Chat),"An agent needs to perform internal reasoning, self-reflection, or complex sub-computations without directly exposing these steps to the main conversation, mimicking human thought processes.",Enhancing an agent's reasoning and problem-solving capabilities by allowing it to 'think aloud' or consult internal sub-agents for complex decisions or critiques.,"Utilize the Nested Chat pattern where one agent holds the current conversation while invoking conversations with other agents as its 'inner monologue' to accomplish sub-tasks, reflect, or generate critiques, before formulating and sending its reply to the original sender.","Realizes self-reflection, improves LLM reasoning and problem-solving capability, allows for more complex internal processing and decision-making, and aligns with the 'Society of Mind' concept.",['Nested Chat'],"Self-reflection, complex internal reasoning, critique generation",0,
Role-Play Prompting,"Guiding LLM-backed agents to adopt specific personas or behaviors for effective collaboration, task execution, or dynamic interaction management.","Multi-agent conversations where agents need to adhere to defined roles (e.g., Assistant, Critic, Engineer) or for dynamic speaker selection in group chats to ensure appropriate contributions.","Incorporate natural language instructions in the agent's system message or prompts to explicitly define its role, responsibilities, and expected behavior within the conversation.","Agents exhibit consistent and appropriate behavior for their assigned roles, improves collaboration, enhances dynamic speaker selection, and leads to more effective consideration of conversation context and role alignment.","['Customizable Conversable Agent', 'Group Chat', 'Hybrid Control (Natural Language & Code)']","AssistantAgent system message, dynamic speaker selection in group chat",0,
Grounding Prompting,"Ensuring LLM outputs adhere to specific constraints, facts, or external information, and preventing hallucinations or irrelevant responses.","LLM-backed agents generating code, making decisions, or providing information where accuracy, adherence to rules, or specific output formats are critical.","Include natural language instructions in the prompt (e.g., system message) to guide the LLM to confine its outputs, verify answers, include verifiable evidence, or adhere to specific formats, making it easier for other agents to consume.","Reduces hallucinations, improves factual accuracy, ensures outputs are consumable by other agents or systems, and helps maintain consistency and reliability.","['Hybrid Control (Natural Language & Code)', 'Customizable Conversable Agent']","AssistantAgent system message (e.g., 'confine LLM outputs,' 'include verifiable evidence'), guiding agents to make legal moves in games (though a dedicated agent is often more robust)",0,
Retrieval-Augmented Generation (RAG),"Large Language Models (LLMs) often generate factually incorrect answers (hallucinations) and their knowledge is limited to their parametric memory, which can become outdated.","Enhancing the accuracy and factual consistency of LLM responses, particularly in knowledge-intensive tasks like Question Answering (QA).","Integrate non-parametric knowledge from external knowledge bases into LLMs. An additional retrieval module accesses a knowledge base to find information relevant to the given input, and this retrieved information is then incorporated into the LLM's input for generation.","Improves response accuracy, reduces hallucinations, and keeps LLMs current with world knowledge by providing supplementary context.","['Single-step RAG', 'Multi-step RAG', 'Adaptive Retrieval-Augmented Generation (AdaptiveRAG)']","['Question Answering (QA)', 'Fact-checking', 'Information synthesis']",0,
Single-step RAG,"LLMs may struggle with queries that require external knowledge beyond their internal parametric memory, but which can be resolved with a single, direct information lookup.","Question Answering tasks where queries are of moderate complexity, requiring external knowledge but not extensive multi-hop reasoning or iterative information gathering.","A retrieval model first identifies and retrieves relevant documents from an external knowledge source based on the input query. This retrieved information is then directly augmented into the LLM's input, allowing the LLM to generate an answer in a single retrieval-and-generation pass.","Offers significant improvements in accuracy for queries requiring external knowledge compared to non-retrieval methods, while maintaining relative efficiency for simpler queries.","['Retrieval-Augmented Generation (RAG)', 'Adaptive Retrieval-Augmented Generation (AdaptiveRAG)']","['Open-domain QA for moderate complexity queries', 'Information extraction from a single document']",0,
Multi-step RAG,"Complex queries necessitate synthesizing information from multiple source documents and performing multi-hop reasoning, which cannot be adequately addressed by a single retrieval-and-response step.",Multi-hop Question Answering (QA) and other complex reasoning tasks where answers depend on interconnected pieces of information spread across several documents.,"The LLM interacts iteratively with a retrieval module over several rounds. In each step, new documents are retrieved based on the current query and accumulated context (including previous documents and intermediate answers). This iterative process allows the LLM to progressively refine its understanding and build a comprehensive foundation to formulate a final answer. This can involve query decomposition or interleaving Chain-of-Thought reasoning with retrieval.","Effectively handles complex multi-hop queries and multi-reasoning tasks, providing more comprehensive and accurate answers for challenging questions.","['Retrieval-Augmented Generation (RAG)', 'Chain-of-Thought Reasoning', 'Adaptive Retrieval-Augmented Generation (AdaptiveRAG)']","['Multi-hop QA', 'Complex reasoning tasks', 'Iterative information synthesis']",0,
Adaptive Retrieval-Augmented Generation (AdaptiveRAG),"Existing one-size-fits-all Retrieval-Augmented Generation (RAG) approaches are inefficient for simple queries (e.g., using multi-step RAG for a straightforward question) and insufficient for complex queries (e.g., using single-step RAG for a multi-hop question). Real-world user queries exhibit a wide range of complexities.","Question Answering systems built with RAG, where the system needs to dynamically balance computational efficiency and response accuracy across diverse query complexities.","Implement a framework that dynamically selects the most suitable RAG strategy (non-retrieval, Single-step RAG, or Multi-step RAG) for an incoming query. This selection is driven by a 'Query Complexity Classifier' (a smaller Language Model) that predicts the complexity level of the query. The system adapts its operational behavior without changing internal model architecture or parameters during adaptation.","Significantly enhances the overall efficiency and accuracy of QA systems by allocating appropriate computational resources based on query complexity, providing a robust middle ground between minimalist and maximalist approaches.","['Retrieval-Augmented Generation (RAG)', 'Single-step RAG', 'Multi-step RAG', 'Query Complexity Classifier']","['Open-domain Question Answering', 'Dynamic resource management in LLM applications', 'Personalized information retrieval']",0,
Query Complexity Classifier,"To enable dynamic adaptation of LLM strategies (e.g., in RAG systems), the system needs to accurately determine the complexity level of an incoming user query. However, pre-annotated datasets for query-complexity pairs are typically unavailable.","As a component within an adaptive LLM framework (like AdaptiveRAG), where different processing strategies are optimal for different query complexities.","Train a smaller Language Model (Classifier) to predict one of several predefined complexity levels (e.g., straightforward, moderate, complex) for a given query. The training dataset for this classifier is automatically constructed using two strategies: 1) **Outcome-based Labeling:** Assign labels based on which LLM strategy (e.g., non-retrieval, single-step RAG, multi-step RAG) successfully answers the query, prioritizing simpler successful strategies. 2) **Inductive Bias Labeling:** For queries not covered by outcome-based labeling, leverage inherent biases in existing datasets (e.g., single-hop datasets for moderate, multi-hop datasets for complex queries).","Provides an effective mechanism for pre-determining query complexity, which is instrumental in dynamically selecting the most fitting LLM strategy, thereby improving overall system efficiency and accuracy.",['Adaptive Retrieval-Augmented Generation (AdaptiveRAG)'],"['Dynamic strategy selection in LLM applications', 'Query routing', 'Resource optimization in AI systems']",0,
Chain-of-Thought Reasoning,"Large Language Models (LLMs) often struggle with complex reasoning tasks, providing direct answers that may be incorrect or lack transparency, especially for multi-step problems.","Improving the reasoning capabilities and explainability of LLMs, particularly when they need to perform multi-step logical deductions or interact with external tools/modules iteratively.","Instead of directly asking for the final answer, prompt the LLM to generate a sequence of intermediate thoughts, steps, or justifications before arriving at the final conclusion. This 'chain of thought' guides the LLM through a logical progression of reasoning.","Elicits emergent reasoning abilities in LLMs, leading to improved performance on complex tasks, better explainability of the LLM's decision-making process, and enhanced capability for multi-step problem-solving.",['Multi-step RAG'],"['Multi-hop Question Answering', 'Complex problem-solving', 'Agentic AI planning', 'Mathematical reasoning']",0,
Interpretable Global Surrogate Model,"Complex, black-box AI models lack global interpretability, making it hard to understand their overall decision logic.","When a high-performing but opaque AI model is used, and there's a need for a holistic understanding of its behavior across the entire dataset.","Train a simpler, inherently interpretable model (the 'surrogate') on the predictions of the complex black-box model. This surrogate aims to mimic the global behavior of the black-box model.","A more transparent, understandable model that approximates the global decision-making of the original black-box system.",[],"Global model understanding, model validation, debugging.",3,
Local Surrogate Explanation (LIME-like),Understanding why a black-box AI model made a specific individual prediction is difficult due to its opacity.,"When individual predictions of a black-box model need to be explained in a human-understandable way, focusing on the local factors influencing that specific decision.","For a given instance, generate perturbed samples in its local neighborhood. Train a simple, interpretable model (e.g., linear model) on these perturbed samples, weighted by their proximity to the instance, to locally approximate the black-box model's behavior.",A feature importance vector indicating which features locally contributed to the individual prediction.,"['Interpretable Global Surrogate Model', 'Game-Theoretic Feature Attribution (SHAP-like)', 'Local Rule-Based Explanation (Anchor/LORE-like)', 'LACE (Local Pattern-Based Explanation with Prediction Difference)']","Local prediction explanation, debugging individual decisions, building trust.",3,
Game-Theoretic Feature Attribution (SHAP-like),"Quantifying the contribution of each feature to an individual black-box model's prediction in a fair, consistent, and theoretically sound manner.","When a quantitative, fair, and consistent attribution of feature importance is required for individual predictions of black-box models, often for fairness analysis or detailed debugging.","Apply concepts from cooperative game theory, specifically Shapley values, to attribute the prediction outcome to each input feature. This involves calculating the marginal contribution of each feature across all possible coalitions of features.","A set of Shapley values, one for each feature, representing its average marginal contribution to the prediction.","['Local Surrogate Explanation (LIME-like)', 'LACE (Local Pattern-Based Explanation with Prediction Difference)', 'Subgroup Divergence Analysis (DivExplorer Algorithm)']","Local prediction explanation, fairness analysis, bias detection, quantitative debugging.",3,
Local Rule-Based Explanation (Anchor/LORE-like),"Providing human-understandable, qualitative explanations for individual black-box model predictions in a rule-based format.","When users prefer rule-based explanations for individual predictions, which can be easier to interpret than feature importance scores, especially for structured data.","Extract a set of local rules (e.g., 'anchors' or decision rules from a local surrogate tree) that describe the conditions under which the black-box model makes a specific prediction in the neighborhood of the instance.","A set of IF-THEN rules that explain the individual prediction, providing qualitative insights into the decision logic.","['Local Surrogate Explanation (LIME-like)', 'LACE (Local Pattern-Based Explanation with Prediction Difference)']","Local prediction explanation, qualitative debugging, human-in-the-loop validation.",3,
Counterfactual Explanation,Understanding what minimal changes to an input instance would cause a black-box AI model to change its prediction.,"When users need actionable insights into how to alter an input to achieve a desired outcome from a black-box model, or to understand the model's decision boundaries.","Find the closest possible instance to the original input (in terms of feature values) that results in a different, desired prediction from the black-box model.",A 'what-if' scenario showing the smallest perturbation to an input that flips the model's decision.,[],"Actionable explanations, fairness analysis (e.g., how to change an outcome), debugging, understanding model sensitivity.",3,
LACE (Local Pattern-Based Explanation with Prediction Difference),"Explaining individual black-box classifier predictions by highlighting both individual and interacting feature contributions qualitatively (rules) and quantitatively (prediction difference), while addressing computational complexity.","Structured data, black-box classification models, need for model-agnostic, local, and comprehensive explanations (qualitative and quantitative).","Captures locality via K-nearest neighbors, trains an associative classifier (e.g., L3) on these labeled neighbors to derive local rules (patterns), estimates prediction difference for individual attribute values and relevant patterns (from rules) via marginalization, and automatically tunes K based on locality approximation.","A qualitative explanation (local rules/patterns) and a quantitative explanation (prediction difference for individual features and patterns), visualized as a bar plot.","['Local Surrogate Explanation (LIME-like)', 'Game-Theoretic Feature Attribution (SHAP-like)', 'Local Rule-Based Explanation (Anchor/LORE-like)', 'Interactive Human-in-the-Loop Explanation Tool (xPlain)']","XAI, debugging, model validation, human-in-the-loop inspection.",3,
Interactive Human-in-the-Loop Explanation Tool (xPlain),"Facilitating human understanding, debugging, and trust in black-box AI models through interactive exploration of individual predictions.","Black-box classification models, need for interactive XAI, model validation, debugging, and comparison.","Integrates a local explanation method (LACE) into an interactive UI, allowing users to inspect, compare, perform 'what-if' analysis, define custom rules, and view aggregated explanation metadata (attribute view, item view, local rule view).","Enhanced human understanding, trust, and ability to debug and compare AI models.",['LACE (Local Pattern-Based Explanation with Prediction Difference)'],"XAI, model validation, debugging, model comparison, ethical AI.",3,
Subgroup Divergence Analysis (DivExplorer Algorithm),"Identifying and characterizing data subgroups where a black-box classification model behaves differently (divergently) from its overall behavior, especially for fairness and error analysis.","Black-box classification models, structured data, need for model-agnostic subgroup analysis, fairness assessment, error analysis, and model debugging.","Defines a 'divergence' metric for itemsets, uses frequent pattern mining to extract all frequent itemsets, assesses statistical significance, quantifies item contributions to divergence using Shapley values, generalizes Shapley values for 'global item divergence', identifies 'corrective items', and prunes redundant itemsets.","Identification of critical data subgroups, characterization of their divergent behavior, insights into contributing factors (items), and detection of corrective items.","['Game-Theoretic Feature Attribution (SHAP-like)', 'Interactive Subgroup Divergence Exploration Tool (DivExplorer System)']","Model validation, testing, error analysis, fairness assessment, bias identification, responsible AI.",3,
Interactive Subgroup Divergence Exploration Tool (DivExplorer System),Enabling interactive exploration and inspection of divergent subgroups identified by the DivExplorer algorithm.,"Black-box classification models, need for interactive subgroup analysis, model debugging, and bias identification.","Integrates the DivExplorer algorithm into an interactive UI, allowing users to view/sort divergent itemsets, drill down into item contributions, visualize the itemset lattice, search for supersets, and examine global item influence.","Facilitates human understanding of subgroup-specific model behaviors, aids in debugging, and supports bias identification.",['Subgroup Divergence Analysis (DivExplorer Algorithm)'],"Model validation, debugging, fairness assessment, responsible AI.",3,
Prefix-Aware Knowledge Tree Caching with Hierarchical Memory Management,"In Retrieval-Augmented Generation (RAG), Large Language Model (LLM) attention mechanisms are sensitive to the order of retrieved documents, meaning Key-Value (KV) tensors for the same document vary based on preceding tokens. This makes traditional caching difficult. Additionally, GPU memory is limited, requiring a multi-level caching strategy for long RAG sequences.","RAG systems where retrieved documents are injected into LLM prompts, and their order significantly impacts KV tensor computation and generation quality. The system needs to efficiently manage and share KV cache across fast (GPU) and slower (host) memory tiers.","Organize document KV tensors in a 'knowledge tree' (a prefix tree based on document IDs), where each path represents a specific sequence of documents and nodes hold the KV tensors. This structure allows sharing common prefixes across requests and efficient retrieval while respecting document order. Nodes are dynamically placed in a GPU and host memory hierarchy, with more frequently accessed documents in faster GPU memory. A Prefix-aware GreedyDualSizeFrequency (PGDSF) replacement policy manages node placement and eviction, calculating priority based on access frequency, KV tensor size, last access time, and a prefix-aware recomputation cost (considering the cost of computing KV tensors for non-cached tokens and the impact of preceding documents). A 'swap-out-only-once' strategy minimizes GPU-host data transfer.","Minimizes cache miss rate, ensures the most valuable KV tensors are retained in appropriate memory tiers, adapts to memory hierarchy and prefix sensitivity, and significantly reduces redundant computation for RAG, leading to improved Time-To-First-Token (TTFT) and throughput.","['Dynamic Speculative Pipelining for RAG', 'Cache-Aware Request Reordering for RAG']","Efficient KV cache management for Retrieval-Augmented Generation (RAG) systems, optimizing LLM inference for knowledge-intensive tasks, RAGCache system.",0,
Dynamic Speculative Pipelining for RAG,"The retrieval (often CPU-bound) and LLM generation (GPU-bound) steps in RAG are typically executed sequentially, leading to idle GPU resources during retrieval and increased end-to-end latency, especially when retrieval latency is substantial.","RAG systems where retrieval and generation are distinct, sequential phases, and there's an opportunity to overlap these operations to reduce overall latency and improve resource utilization.","Dynamically overlap knowledge retrieval and LLM inference. The vector search continuously produces 'candidate documents' (e.g., top-k documents in its queue) in stages. The LLM engine initiates 'speculative generation' early using these candidates. If subsequent candidate documents from the vector search differ from the previous ones, the current speculative generation is terminated, and a new one begins. If the candidates match the final retrieved documents, the speculative generation results are returned directly. This pipelining is dynamically enabled based on system load (e.g., only if the number of pending LLM requests falls below a predetermined maximum batch size for prefill iteration).","Significantly reduces end-to-end latency by minimizing the non-overlapping time between retrieval and generation, improves resource utilization (CPU and GPU), and controls the overhead of incorrect speculative generations under varying system loads, contributing to lower TTFT.","['Prefix-Aware Knowledge Tree Caching with Hierarchical Memory Management', 'Cache-Aware Request Reordering for RAG']","Reducing latency in RAG systems, optimizing resource utilization in hybrid CPU/GPU ML workflows, RAGCache system.",0,
Cache-Aware Request Reordering for RAG,"Unpredictable arrival patterns of user requests in RAG systems can lead to inefficient cache utilization and 'cache thrashing' (frequent swapping of cached items), especially when requests referring to the same documents are not processed consecutively.","RAG serving systems that maintain a shared Key-Value (KV) cache for retrieved documents, where the order of processing incoming requests can significantly impact cache hit rates and overall system throughput.","Implement a priority-based request scheduling mechanism. Incoming requests are managed in a priority queue and reordered for processing based on an 'OrderPriority' metric: `OrderPriority = Cached Length / Computation Length`. This metric prioritizes requests that are likely to enhance cache efficiency by having a larger portion of their required context already cached relative to the computation needed for the uncached part. To prevent 'starvation' of lower-priority requests, a reordering window size is set, ensuring all requests are processed within a defined timeframe.","Improves the cache hit rate, reduces total computation time by maximizing KV cache reuse, optimizes resource utilization, and mitigates cache volatility, leading to higher throughput and lower TTFT under high request rates.","['Prefix-Aware Knowledge Tree Caching with Hierarchical Memory Management', 'Dynamic Speculative Pipelining for RAG']","Optimizing cache efficiency in RAG serving, improving throughput and latency in LLM inference systems with shared caches, RAGCache system.",0,
Memory Augmentation for LLM Agents,"Large Language Models (LLMs) have limited context windows (short-term memory) and lack persistent, external knowledge (long-term memory), hindering their ability to acquire, process, and retain extensive information for complex tasks.","LLM-powered language agents needing to manage and retain information beyond their immediate context or inherent parametric memory, especially for long-horizon tasks or when facing context accumulation limits.","Employ techniques like memory summarization (for managing working memory and short-term context) and retrieval (for accessing external, long-term knowledge bases). The use of a 'NotebookWrite' tool is mentioned as an implementation to record necessary information, manage working memory, and prevent maximum token limits.","Substantial improvement in the general abilities of language agents by allowing them to acquire, process, and retain more information, and effectively manage context accumulation.",Tool-Augmented LLM Agents,"Complex task decomposition, reasoning, long-horizon planning, information collection, agentic AI.",0,
Tool-Augmented LLM Agents,"LLMs have inherent limitations in accessing real-time information, performing specific computations, or interacting with dynamic external environments, restricting their ability to perform real-world tasks.","LLM-powered language agents needing to expand their capabilities beyond their inherent knowledge and reasoning to perform tasks that require external data, specific actions, or interaction with the environment.","Equip language agents with the ability to interact with external tools (e.g., search APIs, databases, specialized functions) to acquire necessary information, perform specific actions, or access external functionalities. This 'tool-augmentation paradigm' allows agents to proactively acquire information from partially observable environments.","Significantly expands the potential capabilities of language agents, enabling them to tackle tasks requiring external information or actions, and to operate more robustly in unconstrained settings.","ReAct (Reasoning and Acting), Memory Augmentation for LLM Agents","Complex planning (e.g., travel planning), information collection, real-world interaction, web agents, embodied agents, agentic AI.",0,
Tree/Graph-based Search for LLM Planning,"Optimizing solution searches in complex planning tasks for language agents, especially when exploring multiple possibilities or needing to find optimal paths efficiently.","Language agents performing planning tasks that involve exploring a search space of possible actions or states, aiming for efficient and effective solution discovery.","Employ classical data structures like trees and graphs to represent the search space and guide the language agent's exploration. This allows for more structured and efficient search, helping to decompose tasks and optimize solution searches in fewer steps.",Enhances the planning capabilities of language agents by providing a structured approach to explore possibilities and optimize solution searches.,"Tree of Thoughts (ToT), Graph of Thoughts (GoT)","Complex planning, task decomposition, multi-step reasoning, agentic AI.",0,
Environmental Feedback Loop for LLM Agents,"Language agents may make errors, get trapped in dead loops, or fail to dynamically adjust their plans without external validation or guidance from the environment.","LLM-powered agents performing tasks in dynamic or partially observable environments where their actions yield observable outcomes, and their plans require continuous refinement and error correction.","Incorporate a feedback mechanism where the agent's actions or plans are evaluated against the environment (or a simulated environment). This feedback (e.g., observations, costs, error messages, null results) is then used to adjust or refine subsequent reasoning and actions.","Improves agent performance by allowing dynamic adjustment of plans, rectification of errors, and adaptation to environmental changes, preventing persistent errors and dead loops.","ReAct (Reasoning and Acting), Reflexion (Verbal Reinforcement Learning)","Planning, tool use, complex task execution, self-correction, adapting to environment constraints, agentic AI.",0,
ReAct (Reasoning and Acting),"Language models struggle to effectively combine reasoning (generating thoughts) with acting (performing actions) in interactive, multi-step tasks, leading to disjointed behavior and inefficient problem-solving.","LLM-powered agents needing to interact with external environments or tools to solve problems that require both deliberation and action, where the agent needs to dynamically adapt based on observations.","Interleave reasoning steps ('Thought') with action steps ('Action') and observation steps ('Observation'). The agent first reasons about the current situation, then decides on an action, executes it, and observes the outcome, using this observation to inform the next thought.","Synergizes reasoning and acting, enabling agents to dynamically plan, execute, and adapt based on environmental feedback, leading to more effective iteration with tools and improved task performance.","Tool-Augmented LLM Agents, Environmental Feedback Loop for LLM Agents, Reflexion (Verbal Reinforcement Learning)","Information collection, planning, interactive problem-solving, web agents, embodied agents, agentic AI, prompt design.",0,
Reflexion (Verbal Reinforcement Learning),"Language agents may get stuck in dead loops, make persistent errors, or fail to learn from past mistakes without a mechanism for self-correction and improvement over multiple attempts.","LLM-powered agents performing multi-step tasks where errors can occur, and there's a need to improve performance over time or across attempts by analyzing failures and generating corrective insights.","Utilize a 'reflection model' (often another LLM call) to analyze past erroneous attempts, generate high-level insights or 'verbal reinforcement' (e.g., identifying failure modes like argument errors, dead loops, hallucinations), and use this feedback to guide future reasoning and actions.","Enhances the agent's ability to identify and correct flawed reasoning, leading to improved task completion, reduced errors, and better adaptation to complex scenarios.","Environmental Feedback Loop for LLM Agents, ReAct (Reasoning and Acting)","Complex planning, error rectification, self-correction, long-horizon tasks, improving robustness, agentic AI, MLOps.",0,
Chain of Thought (CoT) Prompting,"LLMs often struggle with complex reasoning tasks, providing direct answers without showing intermediate steps, which can lead to errors and a lack of transparency in their decision-making process.","Using LLMs for tasks requiring multi-step reasoning, problem-solving, or complex decision-making where the process of arriving at the answer is important for accuracy and interpretability.","Prompt the LLM to generate a series of intermediate reasoning steps before providing the final answer (e.g., by adding 'Let's think step by step'). This encourages the model to break down the problem into smaller, manageable parts.","Elicits and improves reasoning capabilities in LLMs, leading to more accurate and coherent solutions for complex tasks and providing transparency into the model's thought process.",,"Mathematical problem-solving, logical reasoning, planning, complex question answering, prompt design, knowledge & reasoning.",0,
"Multi-Constraint, Long-Horizon Planning for LLM Agents","LLMs' autoregressive nature and limited cognitive capacity make it difficult for them to perform global planning, holistically consider multiple interdependent constraints (explicit and implicit), and anticipate future implications for long-horizon tasks, often leading to local optima or constraint violations.","LLM-powered agents tasked with complex, multi-step planning problems (like travel planning) where decisions at one stage significantly impact future stages, and multiple diverse constraints must be satisfied simultaneously over an extended period.","Employ strategies that enable the agent to look beyond immediate steps and manage the entire plan. This involves explicit mechanisms for tracking and balancing multiple constraints, using backtracking to adjust previous decisions, or employing heuristic methods for forward-looking planning to anticipate future implications and costs. The paper highlights the *need* for such sophisticated strategies.","Aims to improve the agent's ability to handle long-horizon tasks and satisfy multiple global constraints by enabling more holistic and strategic planning, moving beyond single-objective optimization.","Tree/Graph-based Search for LLM Planning, Environmental Feedback Loop for LLM Agents","Travel planning, complex task execution, resource allocation, multi-objective optimization, agentic AI, planning.",0,
Retrieval-Augmented Generation (RAG),"Large Language Models (LLMs) lack explicit, up-to-date external knowledge, are prone to hallucination, and struggle with factual accuracy.","LLMs are used for tasks requiring access to specific, verifiable, or dynamic external information, such as open-domain question answering or fact-checking.","Integrate a retrieval mechanism (e.g., sparse or dense retrieval) to extract relevant knowledge from an external corpus (e.g., text, databases) and provide this retrieved information to the LLM as additional context.","Enhances LLMs with explicit external knowledge, reduces hallucinations, and improves factual accuracy and timeliness of generated responses.","['Code-Based Tool Augmentation', 'LLM as a Planner (for Tool Use)', 'Tool Description & Few-Shot Prompting for Tool Use', 'Long-Term Memory / Experience-Based Learning (for Agents)']","Open-domain Question Answering, Fact-checking, Timely information retrieval, Knowledge-intensive NLP tasks.",0,
Code-Based Tool Augmentation,LLMs exhibit weaknesses in numerical reasoning and struggle with complex tabular or mathematical tasks that require precise computation or structured data manipulation.,"LLMs are applied to tasks requiring accurate calculations, data querying, or logical operations on structured data (e.g., tables, databases).","Augment LLMs with code interpreters (e.g., Python Interpreter, SQL Interpreter) or specialized mathematical tools (e.g., WolframAlpha Calculator) to execute computations and interact with structured data programmatically.","Improves LLMs' numerical reasoning, enables accurate handling of tabular data, and facilitates complex mathematical problem-solving by offloading computational tasks to reliable external tools.","['Retrieval-Augmented Generation (RAG)', 'LLM as a Planner (for Tool Use)', 'Tool Description & Few-Shot Prompting for Tool Use', 'Emergent Tool Composition / Innovation']","Math word problems, Tabular Question Answering, Data analysis, Scientific computing, Code generation for structured prediction.",0,
LLM as a Planner (for Tool Use),"LLMs need to solve complex, multi-step tasks that require orchestrating and composing multiple external tools in a logical sequence.","An LLM is provided with a set of available tools and a complex goal, needing to determine a sequence of actions (tool calls) to achieve it.","Leverage the LLM's reasoning capabilities to act as a controller, autonomously breaking down complex tasks into intermediate reasoning steps and generating a sequence of tool calls (a 'tool chain') to achieve the overall goal.","Enables LLMs to solve complex problems by effectively composing and orchestrating multiple tools, extending their capabilities beyond single-step responses.","['Decomposed Planning', 'Self-Reflection / Feedback-Driven Planning (ReAct)', 'Tool Description & Few-Shot Prompting for Tool Use', 'Emergent Tool Composition / Innovation', 'Long-Term Memory / Experience-Based Learning (for Agents)']","Multi-tool question answering, Complex task automation, Agentic behavior, API integration.",0,
Decomposed Planning,"LLMs struggle with complex, long-horizon tasks that cannot be solved in a single, direct step, often leading to errors or incomplete solutions.","LLMs are faced with a complex problem requiring multiple logical steps, sub-tasks, or intermediate reasoning to reach a solution.","Enable LLMs to autonomously break down the complex task into a sequence of intermediate, manageable reasoning steps or sub-goals. This can be facilitated through prompting techniques like Chain-of-Thought.","Improves LLM's ability to tackle complex, multi-step problems by simplifying the reasoning process, making it more tractable, and allowing for a structured approach to problem-solving.","['LLM as a Planner (for Tool Use)', 'Self-Reflection / Feedback-Driven Planning (ReAct)']","Complex Question Answering, Multi-tool orchestration, Long-horizon task completion, Mathematical reasoning.",0,
Self-Reflection / Feedback-Driven Planning (ReAct),"LLMs may make suboptimal decisions, generate infeasible actions, or omit arguments during multi-step tasks, especially when composing tools, without a mechanism for correction.","LLMs are performing multi-step tasks, often involving external tools, where the outcome or observation of each action can be obtained and evaluated.","Prompt LLMs to generate interleaved verbal reasoning traces (Thought) and tool calls (Action), and then use the observations/feedback from tool execution to self-reflect on previous decisions and iteratively refine their plans or actions.","Improves decision-making, reduces errors (e.g., argument errors, infeasible actions), and enhances the LLM's ability to refine its tool-use chain for better success rates and more robust problem-solving.","['LLM as a Planner (for Tool Use)', 'Decomposed Planning', 'Long-Term Memory / Experience-Based Learning (for Agents)']","Multi-tool question answering, Complex reasoning tasks, Iterative problem-solving, Agentic control with environmental interaction.",0,
Tool Description & Few-Shot Prompting for Tool Use,"LLMs struggle to understand how to use external tools effectively, call them with correct arguments, or compose them for complex tasks, especially when introduced to new tools.","LLMs are augmented with a set of external tools, and their usage needs to be guided within the LLM's limited context window to ensure correct and efficient interaction.","Include clear, concise descriptions of each tool's functionality, its inputs, and outputs, and provide few-shot examples demonstrating correct usage and composition of these tools directly within the prompt.","Improves LLM's ability to correctly call tools, reduces argument errors and infeasible actions, and guides the LLM in composing tools for multi-step problems by providing an 'in-context' tutorial.","['LLM as a Planner (for Tool Use)', 'Emergent Tool Composition / Innovation']","Initializing LLMs for tool use, Reducing common tool-calling errors, Guiding complex tool chains, API integration.",0,
Emergent Tool Composition / Innovation,"LLMs may struggle to compose tools in novel ways not explicitly shown in few-shot examples, limiting their problem-solving scope for challenging tasks that require creative combinations.","LLMs are provided with a set of tools and few-shot examples, but complex tasks require creative or novel combinations of these tools that go beyond direct exemplars provided in the prompt.","Design prompts and potentially fine-tuning strategies that encourage LLMs to infer and apply logical relationships between tools, enabling them to compose tools in innovative ways beyond explicit exemplars, often relying on their inherent code understanding and reasoning abilities.","Enhances LLM's ability to solve challenging tasks requiring novel tool combinations, extending their problem-solving capabilities, though this 'innovation' can sometimes be accompanied by hallucinations.","['LLM as a Planner (for Tool Use)', 'Tool Description & Few-Shot Prompting for Tool Use']","Complex multi-tool tasks, Open-ended problem solving, Situations where few-shot examples are insufficient for all possible compositions.",0,
Long-Term Memory / Experience-Based Learning (for Agents),"LLMs have limited context windows and cannot learn or adapt based on past experiences (successes or failures) beyond their current prompt, leading to repetitive errors or inefficient behavior over time.","LLMs are used in agentic systems or for tasks requiring continuous interaction, adaptation, and learning from historical data or past interactions.","Provide LLMs with external memory capabilities to store and retrieve past experiences, observations, and learned strategies, allowing them to learn and adapt based on historical interactions and accumulated knowledge.","Enables LLMs to improve performance over time by leveraging past knowledge, avoiding repetitive mistakes, and adapting to new situations or user preferences, fostering more robust and intelligent agent behavior.","['Self-Reflection / Feedback-Driven Planning (ReAct)', 'LLM as a Planner (for Tool Use)']","Continuous learning agents, Personalized interactions, Multi-session tasks, Complex agentic workflows, Adaptive systems.",0,
Tool-Integrated Reasoning,"Large Language Models (LLMs) struggle with complex mathematical problems that require both abstract, semantic analysis and precise, rigorous computation or symbolic manipulation. Existing approaches (pure natural language reasoning or pure program synthesis) have complementary weaknesses: natural language excels at planning and abstract reasoning but fails at precise computation, while programs excel at computation but lack nuanced reasoning and error handling.","Designing LLM-based agents for tasks demanding a combination of high-level strategic reasoning (e.g., problem decomposition, semantic analysis) and low-level, accurate execution of operations (e.g., equation solving, symbolic manipulation, complex arithmetic) that are beyond the LLM's inherent capabilities.","The agent interleaves natural language rationales with program-based tool use. The LLM generates a natural language rationale to analyze the problem, plan, or explain a step. When a sub-task requires precise computation or external capabilities, the LLM generates a program based on the preceding rationale. This program is executed by an external tool (e.g., computation library, symbolic solver), yielding an output. The execution output is fed back to the LLM, which then generates the next natural language rationale to continue reasoning, make adjustments, or finalize the answer. This process repeats until the problem is solved.","Synergistically combines the analytical prowess of language with the computational efficiency of tools, significantly improving performance on complex quantitative tasks, reducing the gap with closed-source models, and demonstrating superior generalization.",['Output Space Shaping'],"Mathematical problem-solving, scientific reasoning, complex quantitative tasks, any domain where LLMs need to combine abstract reasoning with precise external computation.",0,
Output Space Shaping,"When training LLMs for complex, multi-step tasks involving tool use (e.g., via imitation learning), relying solely on a limited set of high-quality, human-curated trajectories can restrict the model's output space. This leads to inflexibility in exploring diverse, plausible reasoning paths during inference and can result in improper tool-use behavior.","Fine-tuning LLMs for agentic behavior or tool-use, especially when the initial training data, while high-quality, lacks sufficient diversity to cover all valid reasoning trajectories or potential error scenarios. The goal is to improve robustness, generalization, and the model's ability to self-correct or explore alternative solutions.","To encourage diversity and mitigate improper behavior, the training process is augmented with a two-pronged approach: 1) **Sampling Diverse Trajectories:** The model (after initial imitation learning) is used to sample multiple diverse tool-use trajectories for each training problem. Valid trajectories (those leading to correct answers without errors) are retained. 2) **Correcting Invalid Trajectories:** Invalid trajectories (those with wrong answers or tool-use errors) are identified. A more capable 'teacher model' (e.g., a larger LLM or a previously trained, stronger version) is then used to correct the subsequent portions of these invalid trajectories, effectively turning them into valid ones. The model is then retrained on a combined dataset consisting of the original high-quality trajectories, the newly sampled valid trajectories, and the teacher-corrected invalid trajectories.","Significantly boosts reasoning accuracy, encourages the exploration of diverse plausible reasoning steps, and reduces improper tool-use behavior. It provides greater benefits for smaller models and difficult problems, improving generalization and robustness.",['Tool-Integrated Reasoning'],"Improving the robustness and generalization of LLM agents, data augmentation for complex reasoning and tool-use tasks, fine-tuning models for self-correction and error recovery in agentic workflows.",0,
Iterative Task Solving,"LLMs struggle with complex, long-horizon tasks that require dynamic adjustment, error correction, and adaptation to intermediate results or tool feedback, as a one-step, pre-defined plan might fail or become outdated.","Complex, multi-step tasks for LLM-based agents where intermediate results or tool feedback are crucial for refining the plan and achieving the objective.","Instead of committing to a complete task plan upfront, the LLM iteratively interacts with tools, adjusting subtasks progressively based on the feedback received from tool executions. This enables the LLM to address the problem step-by-step, continuously refining its plan in response to tool outputs.","Improves problem-solving capabilities, enhances adaptability, and increases robustness by allowing LLMs to correct errors, adjust to new information, and handle unforeseen circumstances during task execution.","['ReACT', 'Error Handling Mechanisms (for Tool Calling)', 'Multi-Agent Collaboration (for Tool Learning)', 'Self-Verification (for Tool Selection/Calling)', 'Information Integration for Response Generation']","Agentic systems, complex problem-solving, real-world applications requiring dynamic interaction, robotics, vision-and-language navigation.",0,
One-step Task Solving,"Decomposing a complex user query into a complete, pre-defined sequence of subtasks and tool calls without the ability to incorporate intermediate feedback or adapt to execution outcomes.","Tasks where the entire plan can be reliably determined upfront, or where real-time feedback loops are not feasible, necessary, or implemented in the system design.","Upon receiving a user question, the LLM analyzes the user's intent and immediately plans all the subtasks needed to solve the problem. The LLM then directly generates a response based on the results returned by the selected tools, without considering the possibility of errors during the process or altering the plan based on tool feedback.","Provides a simpler and potentially faster execution flow for well-defined tasks, but is less robust to errors or unexpected tool outputs compared to iterative approaches.","['Chain-of-Thought (CoT) Planning', 'Retriever-Augmented Tool Selection', 'LLM-Guided Tool Selection', 'Direct Insertion for Response Generation']","Simpler, well-defined tasks, initial tool learning paradigms, scenarios where latency is critical and task complexity is low.",0,
Chain-of-Thought (CoT) Planning,"LLMs often struggle with complex reasoning and multi-step task decomposition, especially in zero-shot or few-shot settings, leading to superficial or incorrect plans.","LLMs need to break down a complex user query into a sequence of simpler, solvable subquestions or steps, and to articulate the reasoning behind the plan.","Leverage the LLM's innate abilities through strategic prompting (e.g., by incorporating directives like 'let's think step by step') or by providing few-shot examples. This guides the LLM to explicitly generate intermediate reasoning steps, which facilitates the decomposition of complex tasks into simpler subtasks and the outlining of their dependencies and execution sequence.","Improves the LLM's logical analysis capabilities, enables more structured and accurate planning for multi-step tasks, and enhances interpretability by revealing the decision-making process.","['ReACT', 'Tool Graph Planning', 'LLM-Guided Tool Selection']","General task decomposition, complex query resolution, reasoning tasks, enhancing interpretability of LLM plans.",0,
Tool Graph Planning,Efficiently identifying an optimal sequence of tool calls to solve a complex task when there are a large number of tools and intricate dependencies between their functionalities.,"A system with a vast and potentially interconnected set of available tools, where each tool has defined functionalities, input requirements, and output types, forming a graph structure.","Construct the entire action space as a decision tree or a tool graph where nodes represent potential API function calls. Leverage graph traversal algorithms (e.g., Depth-First Search) or Graph Neural Networks (GNNs) to identify optimal solutions or sequences of tool calls, considering dependencies and execution order.","Enables more efficient and accurate subtask selection, structured planning, and navigation through complex tool dependencies, leading to better task completion.","['Chain-of-Thought (CoT) Planning', 'Multi-Agent Collaboration (for Tool Learning)', 'Retriever-Augmented Tool Selection']","Complex API orchestration, multi-tool task execution, agentic planning, systems with large and interconnected tool libraries.",0,
Adaptive Tool Documentation,"LLMs' comprehension and tool-using capabilities are often hampered by static, verbose, or suboptimal tool documentation, leading to errors, inefficient usage, or difficulty in parameter extraction.","LLMs interacting with external tools, where the quality and clarity of tool descriptions, parameter requirements, and usage guidelines are critical for effective tool integration.","Dynamically adjust and optimize tool documentation based on interaction feedback between LLMs and external tools. This can involve prompting LLMs to rewrite tool descriptions to be more concise, incorporating explicit guidelines for functionality, or compressing lengthy documentation into summary sequences while preserving key information.","Improves LLMs' comprehension of tool functions and parameter requirements, leading to more efficient, accurate, and robust tool usage with minimal performance loss.","['Error Handling Mechanisms (for Tool Calling)', 'Context Compression (for Tool Outputs)', 'Parameter Extraction and Formatting']","MLOps for tool learning, improving tool integration, prompt engineering for tool descriptions, enhancing LLM's ability to learn new tools.",0,
Retriever-Augmented Tool Selection,"Real-world systems often incorporate a vast number of tools, making it impractical to input descriptions of all tools into LLMs simultaneously due to context length limitations and high latency.","A large pool of available tools where only a subset is relevant to a given user query or subquestion, and the LLM needs to efficiently narrow down the options.","Employ an efficient tool retrieval system as an initial step. This system uses methods like term-based (e.g., TFIDF, BM25) or semantic-based (e.g., SentenceBert, ANCE) retrieval to identify and filter the top-K most suitable tools from the vast set. These pre-selected tools are then presented to the LLM for final consideration.","Bridges the gap between broad LLM capabilities and practical input limitations, enabling efficient and effective tool selection from large tool libraries, reducing context window pressure, and improving latency.","['LLM-Guided Tool Selection', 'Tool Reranking', 'Tool Graph Planning']","Large-scale tool integration, reducing LLM context window pressure, improving latency in tool selection, enhancing the scalability of tool-augmented LLMs.",0,
LLM-Guided Tool Selection,"After initial retrieval or with a limited set of tools, the LLM needs to make the final, nuanced decision on which specific tool to use, often considering sequential dependencies and the current task state.",A refined list of candidate tools (either from a retrieval phase or a small initial set) and a specific subquestion that requires an external tool for resolution.,"Provide the LLM with the descriptions and parameter lists of the candidate tools within its input context, along with the user query. The LLM then uses its reasoning capabilities (e.g., Chain-of-Thought, ReACT) to select the optimal tool, considering current information, the information needed to be acquired, and the potential order of invocation for serial tool calling.","Enables accurate and context-aware tool selection, handles complex reasoning for tool invocation order, and improves overall task resolution by making informed choices.","['Retriever-Augmented Tool Selection', 'Chain-of-Thought (CoT) Planning', 'ReACT', 'Tool Reranking', 'Self-Verification (for Tool Selection/Calling)']","Fine-grained tool choice, complex multi-tool tasks, agentic decision-making, improving the precision of tool usage.",0,
Tool Reranking,"Initial tool retrieval methods might provide relevant tools but not in the optimal order, or might overlook nuances such as the hierarchical structure of tools or the distinction between seen and unseen tools, affecting selection efficiency and accuracy.","A list of candidate tools has been retrieved (e.g., by a retriever-based method), but their relevance, completeness, or optimal order for the task needs further refinement before presentation to the LLM or for final selection.","Apply an adaptive and hierarchy-aware reranking method to the retrieved tools. This process considers factors like the hierarchical structure of the tool library, the differences between familiar and novel tools, and the completeness of the tool set to optimize the order and relevance of the presented tools.","Improves the precision, completeness, and overall effectiveness of tool selection, leading to more accurate and efficient tool invocation by the LLM.","['Retriever-Augmented Tool Selection', 'LLM-Guided Tool Selection']","Enhancing tool retrieval systems, optimizing tool selection for complex or evolving tool libraries, improving the quality of input to LLM-guided selection.",0,
Parameter Extraction and Formatting,LLMs need to accurately extract required parameters from user queries and format them precisely according to tool specifications to prevent calling errors and ensure successful tool invocation.,"A specific tool has been selected, and its documentation outlines required parameters (names, types, descriptions). The user query contains the necessary information for these parameters.","Leverage LLMs (either through tuning-free methods like few-shot demonstrations or rule-based approaches, or through tuning-based methods) to parse the tool description, identify critical information, and accurately extract and format the parameters (content and format) strictly adhering to the prescribed output format, avoiding superfluous sentences.","Ensures successful tool invocation by providing correctly structured and content-accurate parameters, minimizing tool calling failures.","['Adaptive Tool Documentation', 'Error Handling Mechanisms (for Tool Calling)', 'Tool-Augmented Fine-tuning']","Automating API calls, integrating LLMs with external systems, agentic execution, ensuring data integrity for tool inputs.",0,
Error Handling Mechanisms (for Tool Calling),"Tool calling frequently encounters various errors (e.g., incorrect formatting of input parameters, parameters exceeding acceptable ranges, tool server errors), which can disrupt the task flow and lead to task failure.","LLMs are attempting to call external tools, and the tool server returns an error message or an unexpected output.","Integrate mechanisms designed to refine the LLM's action based on the error messages returned upon calling failure. This involves parsing error feedback, understanding the nature of the error, and adjusting subsequent tool calls, parameter extraction, or even the overall task plan.","Creates a more resilient and adaptive system, ensuring continuity and efficiency in tool learning even in the face of operational disruptions, and improving the robustness of LLM-tool interactions.","['Iterative Task Solving', 'Adaptive Tool Documentation', 'Parameter Extraction and Formatting', 'Self-Verification (for Tool Selection/Calling)']","Robust agentic systems, MLOps for tool learning, improving reliability of LLM-tool interactions, enhancing user trust.",0,
Information Integration for Response Generation,"Tool outputs are diverse (text, numbers, code, images), complex, often lengthy, and may not be directly user-friendly. LLMs need to synthesize this information with their internal knowledge to create a comprehensive and coherent response.","LLMs have received outputs from one or more external tools and need to formulate a comprehensive, accurate, and user-friendly response to the original user query.","Incorporating the output of tools into the LLM's context as input. The LLM then synthesizes this information with its internal knowledge to craft a superior reply. This may involve simplifying lengthy results (e.g., using pre-created schemas), truncating output (though potentially losing information), or employing dedicated compressor models to condense information.","Provides comprehensive, accurate, and contextually relevant responses, enhancing user experience and leveraging the full capabilities of both LLMs and external tools.","['Context Compression (for Tool Outputs)', 'Tool Learning with MultiModal Inputs', 'Iterative Task Solving']","User-facing LLM applications, complex query answering, data synthesis, generating explanations based on tool results.",0,
Direct Insertion for Response Generation,"For simple, straightforward tool outputs, the overhead of complex information integration might be unnecessary, but direct insertion can lead to a poor user experience if the tool outputs are unpredictable or raw.","LLMs have received a simple, atomic, and easily interpretable output from a tool that can be directly presented to the user.","Directly embed the output of tools into the generated response. For instance, if a user asks 'How is the weather today?', the LLM might produce a response template like 'It's [Weather]', which is then directly replaced with the result returned by the tool (e.g., 'It's rainy').","Offers a simple and straightforward approach to response generation for basic tool outputs, suitable for scenarios where minimal post-processing is required.",['Information Integration for Response Generation'],"Simple tool interactions, early tool learning paradigms, debugging tool outputs.",0,
ReACT (Reasoning and Acting),"LLMs need to dynamically interleave explicit reasoning (planning, self-reflection) with executing actions (tool use) to solve complex tasks, adapt to environmental feedback, and improve their decision-making.","Agentic LLM systems where the model needs to make decisions, execute actions, observe outcomes, and learn from the environment in a dynamic, interactive manner.","A framework that integrates reasoning with action. The LLM generates 'Thoughts' (reasoning steps) to plan, reflect, and justify its actions, followed by 'Actions' (tool calls). It then observes the 'Observations' (tool outputs or environmental feedback) and refines its reasoning processes based on this feedback, repeating the Thought-Action-Observation cycle.","Enhances adaptability, decision-making capabilities, and robustness by fostering a dynamic interaction between reasoning and action, leading to more effective and interpretable task completion.","['Iterative Task Solving', 'Chain-of-Thought (CoT) Planning', 'LLM-Guided Tool Selection', 'Error Handling Mechanisms (for Tool Calling)', 'Self-Verification (for Tool Selection/Calling)', 'Multi-Agent Collaboration (for Tool Learning)']","Embodied agents, complex problem-solving, interactive systems, improving transparency and reliability of LLM agents.",0,
Tool-Augmented Fine-tuning,"Base LLMs lack inherent awareness and capability to effectively utilize external tools, requiring explicit instruction or complex prompting, which can be inefficient or unreliable.","Improving the foundational ability of LLMs to seamlessly interact with and leverage external tools, especially for specific domains or a large set of APIs.","Finetune LLMs on carefully curated datasets specifically designed to teach tool usage. This can involve training the model to predict API calls (Toolformer), using instruction-solution pairs derived from expert demonstrations (ToolLLaMA), or leveraging reinforcement learning from human/execution feedback (TaskMatrix.AI, TRICE). Special tokens (toolkens) can be used to seamlessly integrate tool calls into the generation process.","Enhances the LLM's inherent awareness and capability to utilize tools effectively, making tool integration more seamless, robust, and efficient, especially for domain-specific tasks.","['Parameter Extraction and Formatting', 'LLM-Guided Tool Selection', 'Iterative Task Solving', 'Adaptive Tool Documentation']","Developing specialized tool-using LLMs, improving efficiency of tool integration, creating domain-specific agents, enhancing generalization to unseen tools.",0,
Context Compression (for Tool Outputs),"Lengthy tool outputs (e.g., search results, database query responses, API logs) can easily exceed the limited context window of LLMs, leading to loss of critical information or an inability to process the full output.",Receiving extensive textual or data-rich outputs from external tools that need to be processed and integrated by an LLM for subsequent reasoning or response generation.,"Employ methods to condense lengthy information into a more succinct format while preserving key information relevant to the user query or task. This can involve using pre-created schemas to simplify results, truncating output (though potentially losing information), or developing dedicated compressor models (e.g., ReCOMP) to extract and summarize the most useful parts.","Enables LLMs to effectively process and integrate large tool outputs within their context window, preventing information loss, improving the efficiency of processing, and facilitating better response generation.","['Information Integration for Response Generation', 'Adaptive Tool Documentation']","Processing large search results, database queries, complex API responses, managing LLM context window limitations.",0,
Multi-Agent Collaboration (for Tool Learning),"Complex tool learning workflows often involve multiple distinct subtasks (e.g., task planning, tool selection, parameter extraction, tool calling, execution, response generation) that can benefit from specialized expertise and distributed responsibilities.","Designing robust, modular, and efficient tool-augmented LLM systems, especially for complex, multi-step tasks that require different types of intelligence or processing.","Implement a framework where different specialized AI agents collaborate to achieve a common goal. Each agent is tasked with a specific role, such as a planning agent for task decomposition, an observing agent for output extraction, or an execution agent for parameter extraction and tool calling. These agents interact and pass information to each other.","Improves efficiency, accuracy, and modularity in tool learning by distributing responsibilities among specialized components, enhancing the system's ability to handle complex tasks.","['Iterative Task Solving', 'Tool Graph Planning', 'Parameter Extraction and Formatting', 'ReACT']","Complex agentic systems, automated workflows, robust tool integration, distributed AI systems.",0,
Self-Verification (for Tool Selection/Calling),"LLMs can make errors in tool selection or parameter extraction, leading to incorrect tool calls, suboptimal task execution, or a lack of confidence in their decisions.","Situations where the LLM needs to make a critical decision about tool usage or parameter values, and there are close candidate tools, potential ambiguities, or a need to ensure accuracy before committing to an action.","Introduce a self-verification mechanism where the LLM actively checks its own decisions. This can involve distinguishing between close candidate tools by self-asking contrastive questions during tool selection, or by validating parameter values against tool specifications before making a call.","Enhances the accuracy, reliability, and robustness of tool selection and calling, reducing errors and improving overall task performance and trustworthiness of the LLM agent.","['Error Handling Mechanisms (for Tool Calling)', 'LLM-Guided Tool Selection', 'ReACT', 'Iterative Task Solving']","High-stakes applications, improving robustness of agentic systems, enhancing decision-making quality.",0,
Tool Learning with MultiModal Inputs,"Existing tool learning often focuses on text-based queries, limiting LLMs' ability to fully understand diverse user intents expressed through multiple modalities (e.g., images, audio, video).","User queries that encompass visual, auditory, or other non-textual information, requiring LLMs to process and respond to a broader spectrum of inputs to accurately discern user intent.","Integrate multimodal tools (e.g., speech recognition, image analysis, 3D processing) and multimodal encoders with LLMs. This enables LLMs to be aware of and process multimodal input instructions, subsequently selecting correctly matched tools or generating multimodal responses.","Significantly enhances LLMs' perceptual capabilities, improves understanding of complex user intent, and broadens application scenarios to include rich, multimodal interactions.","['Information Integration for Response Generation', 'Multi-Agent Collaboration (for Tool Learning)']","Multimodal assistants, vision-and-language navigation, robotics, interactive systems, augmented reality applications.",0,
Distinguish Business Logic from ML Models,"ML application systems are complex due to regularly retrained, non-deterministic ML components, and evolving business requirements and ML algorithms. There's a need to isolate failures and changes between business logic and ML models.","Any ML application system where outputs depend on ML techniques, and where ML components and business logic are subject to frequent changes and require independent management.","Define clear APIs between traditional and ML components. Place business and ML components with different responsibilities into distinct layers (e.g., Data Layer, Logic Layer, Presentation Layer), separating business logic and inference engine. Divide data flows into three (Business Logic Data Flow, ML Runtime Data Flow, ML Development Data Flow).","Decoupling traditional business and ML components allows ML components to be monitored and adjusted independently to meet user requirements and changing inputs, and helps developers debug ML application systems easily.","['ClosedLoop Intelligence', 'DataAlgorithmServingEvaluator']","Chatbot systems, any ML application system with ML-dependent outputs.",4,
DataAlgorithmServingEvaluator,Prediction systems need to connect various data processing pipeline pieces into one coherent system and support prototyping predictive models.,"Prediction systems that require a structured approach to integrate data sources, algorithms, serving, and evaluation components.","Separate components like MVC for ML: data (data source and data preparator), algorithms (serving and evaluator).",Creates a coherent system for prediction and facilitates prototyping predictive models.,"['ClosedLoop Intelligence', 'Distinguish Business Logic from ML Models']",Prediction systems.,5,
Event-driven ML Microservices,"Frequent prototyping of ML models and constant changes necessitate agile development teams to build, deploy, and maintain complex data pipelines.","Environments where ML models are frequently prototyped and changed, leading to complex data pipelines that require agility in development and maintenance.","Construct pipelines by chaining together multiple microservices, where each service listens for data arrival and performs its designated task.","Enables agility in building, deploying, and maintaining complex ML data pipelines.",[],"Building, deploying, and maintaining complex ML data pipelines.",6,
ParameterServer Abstraction,Lack of widely accepted abstractions for distributed learning.,"Distributed learning environments where data and workloads need to be distributed across worker nodes, and global parameters need to be maintained.","Distribute both data and workloads over worker nodes, while server nodes maintain globally shared parameters (represented as vectors and matrices).",Provides a structured abstraction for managing parameters and workloads in distributed ML training.,[],Distributed learning.,7,
ClosedLoop Intelligence,"Addressing big, open-ended, time-changing, or intrinsically hard problems with ML.","ML systems designed to tackle complex, evolving, or difficult problems that benefit from continuous feedback and interaction.",Connect machine learning to the user and close the loop. Design clear interactions along with implicit and direct outputs.,Improves the ability of ML systems to address complex problems by incorporating user feedback and interaction.,"['Distinguish Business Logic from ML Models', 'DataAlgorithmServingEvaluator']","Systems addressing big, open-ended, time-changing, or intrinsically hard problems.",8,
Federated Learning,"Standard machine learning approaches require centralizing training data, which is often not feasible due to privacy concerns or data locality (e.g., mobile devices).","Scenarios where training data cannot be centralized on one machine or in a datacenter, such as mobile devices or sensitive enterprise data.","Employ Federated Learning, which enables devices (e.g., mobile phones) to collaboratively learn a shared prediction model while keeping all training data on the device.",Enables collaborative model learning and deployment while preserving data privacy and reducing data transfer.,['Secure Aggregation'],"Mobile phones, privacy-sensitive data, distributed learning without data centralization.",9,
ML Versioning,"ML models and their multiple versions can change the behavior of overall ML applications, making reproducibility difficult.","ML application development where model changes, data changes, or training system changes can impact application behavior and require reproducibility.","Record the ML model structure, training data, and training system to ensure a reproducible training process.",Ensures a reproducible training process and helps manage the impact of ML model changes on application behavior.,[],"Managing ML model changes, ensuring reproducibility of training processes.",10,
Handshake,"An ML system depends on inputs delivered outside of the normal release process, leading to potential issues if these inputs change unexpectedly.",ML systems that rely on external data sources or inputs that are not managed within the standard software release cycle.,"Create a handshake normalization process, regularly check for significant changes in external inputs, and send ALERTS.","Provides a mechanism to monitor and manage external data dependencies, preventing unexpected behavior due to input changes.",[],"Managing external data dependencies for ML systems, ensuring input stability.",11,
Isolate and Validate Output of Model,"Machine learning models are known to be unstable, vulnerable to adversarial attacks, and susceptible to noise in data and data drift over time.","Deploying ML models in production where robustness, security, and reliability are critical, given the inherent vulnerabilities of ML models.",Encapsulate ML models within rule-based safeguards and use redundant and diverse architecture that mitigates and absorbs the low robustness of ML models.,"Mitigates instability, vulnerability to attacks, and low robustness of ML models, improving their reliability and security in production.",[],"Improving robustness, security, and reliability of ML models in production.",12,
Canary Model,A surrogate ML model that approximates the behavior of the best ML model is needed to provide explainability and monitor performance.,"Situations where understanding model behavior, detecting performance degradation, or providing explanations for predictions is crucial.",Run the canary inference pipeline in parallel with the primary inference pipeline to monitor prediction differences.,"Provides a mechanism for model monitoring, detecting prediction differences, and contributing to explainability.",[],"Model monitoring, explainability, detecting performance degradation.",13,
Decouple Training Pipeline from Production Pipeline,It is necessary to separate and quickly change the ML data workload and stabilize the training workload to maximize efficiency.,"ML development and deployment environments where training and production workloads have different requirements (e.g., training needs flexibility and experimentation, production needs stability and efficiency).",Physically isolate different workloads to different machines. Then optimize the machine configurations and network usage for each.,"Maximizes efficiency by allowing independent optimization and management of ML training and production workloads, and enables quicker changes to data workloads.",[],"Managing ML training and production environments, optimizing resource allocation.",14,
Descriptive Data Type for Rich Information,"The rich information used and produced by ML systems is often encoded with plain data types (e.g., raw floats, integers), losing semantic meaning and making systems less robust or interpretable.","Designing ML systems where interpretability, robustness, and clear understanding of model parameters and predictions are important.","Design a robust system where model parameters (e.g., logodds multiplier, decision threshold) and predictions carry explicit information about their meaning and origin.",Creates a more robust and interpretable system by embedding rich semantic information into ML-related data types.,[],"Improving interpretability, robustness, and clarity of ML model parameters and predictions.",15,
Design Holistically about Data Collection and Feature Extraction,"The system to prepare data in an ML-friendly format can become a 'pipeline jungle,' making management difficult and costly.","Developing ML systems that involve complex data preparation, cleaning, and feature engineering processes.",Avoid 'pipeline jungles' by adopting a holistic approach to data collection and feature extraction from the outset.,Dramatically reduces ongoing costs and complexity associated with managing ML data pipelines.,[],Designing ML data collection and feature extraction processes to prevent complexity.,16,
Reuse Code between Training Pipeline and Serving Pipeline,Training-serving skew can occur due to discrepancies in how data is handled between the training and serving pipelines.,ML systems where consistency between the data processing logic used during model training and model inference is critical to avoid performance degradation.,"Reuse code between the training pipeline and serving pipeline, for example, by preparing objects that store results in an understandable way for humans.","Prevents training-serving skew, ensuring consistent data handling and model performance across training and serving environments.",[],Ensuring consistency and preventing skew between ML training and serving pipelines.,17,
Secure Aggregation,"In distributed learning, the system needs to communicate and aggregate model updates in a secure, efficient, scalable, and fault-tolerant way.","Distributed machine learning, particularly in Federated Learning, where individual device data must remain private while contributing to a global model.",Encrypt data from each mobile device in Federated Learning and calculate totals and averages without individual examination.,"Enables secure, efficient, scalable, and fault-tolerant aggregation of model updates while preserving data privacy.",['Federated Learning'],Securely aggregating model updates in Federated Learning and other distributed ML scenarios.,9,
Workflow Pipeline,Creating an end-to-end reproducible training and deployment pipeline for a machine learning component is difficult. Data science notebooks can run a whole pipeline but they do not scale.,"Developing and deploying machine learning components where reproducibility, scalability, and maintainability of the pipeline steps are crucial.",Make each pipeline step a separate containerized service. Services are orchestrated and chained together to form pipelines that can be run via REST API calls.,"The portability, scalability, and maintainability of the individual pipeline steps is improved at the cost of an overall more complex solution.",[],Presented at AWS Blog.,18,
Distinguish Business Logic from ML Model,"Machine Learning (ML) systems are complex because their ML components must be retrained regularly and have an intrinsic non-deterministic behavior. Similar to other systems, the business requirements for these systems as well as ML algorithms change over time.","Designing systems that integrate machine learning components with traditional business logic, where ML models require frequent updates and exhibit non-deterministic behavior.",Define clear APIs between traditional and ML components. Place business and ML components with different responsibilities into three layers. Divide data flows into three.,"Improved management of complexity, retraining, and changing requirements in ML systems by clearly separating concerns.",[],[],4,
Encapsulating ML Models within Rule-based Safeguards,"It is impossible to guarantee the correctness of ML model predictions, so they should not be directly used for safety or security-related functions. Furthermore, ML models can be unstable and vulnerable to adversarial attacks, data noise, and drift.","Deploying ML models in safety or security-critical applications, or in environments where model robustness, reliability, and resistance to adversarial attacks are paramount.","Introduce a deterministic rule-based mechanism that decides what to do with the prediction results, e.g., based on additional quality checks.",Reduced risk for negative impacts of incorrect predictions but a more complex architecture.,[],[],12,
AI Pipeline,Complex prediction or synthesis use cases are often difficult to accomplish with a single AI tool or model.,"Scenarios requiring the combination of multiple AI capabilities or models to achieve a complex, multi-step AI task during inference.","Divide the problem into smaller consecutive steps, then combine several existing AI tools or custom models into an inference-time AI pipeline where each specialized tool or model is responsible for a single step.","More tools and models need to be integrated, but the provided result is of higher quality. Each step can be optimized individually.",[],Typical computer vision inference pipelines.,19,
Two-Phase Predictions,"Executing large, complex models can be time-consuming and costly, especially if lightweight clients like mobile or IoT devices are involved.","Deploying ML models to resource-constrained or latency-sensitive clients, where a full, complex model inference is not always necessary or feasible.","Split the prediction into two phases. A simple, fast model is executed first on the client. Afterwards, a large, complex model is optionally executed in the cloud for deeper insights.","Prediction response time is reduced for some cases. The number of large, expensive predictions is reduced. The client has a fallback model when there is no Internet connection.",[],Voice activation in AI assistants like Alexa or Google Assistant.,20,
Ethics Credentials,"Responsible AI requirements are either omitted or mostly stated as high-level objectives and not specified explicitly in a verifiable way as expected system outputs. Because of this, users may trust an AI system less or even refrain from using it.","Developing AI systems where building user trust, ensuring ethical compliance, and demonstrating responsible AI practices are critical.","Provide verifiable ethics credentials for your AI system or component. Using publicly accessible and trusted data infrastructure, the credentials can be verified as proof of ethical compliance. Additionally, users may also have to verify their credentials before getting access to the AI system.","Trust and system acceptance increases and awareness of ethical issues is raised. However, a trusted public data infrastructure is needed and credentials need to be maintained and potentially refreshed from time to time.",[],[],21,
Deploy Canary Model,"You trained a new model with assumed better prediction quality, but it's not certain if this will carry over to production. Additionally, there could be other quality issues with the new model that should not affect all users in production at once.","Deploying new or updated ML models to a production environment, where the risk of introducing regressions or performance degradation needs to be minimized.","Deploy the new model in addition to the existing ones and route a small number of requests to it to evaluate its performance. If this test is successful, all existing models can be replaced. If not, the new model needs to be improved.",Only a small number of users are subjected to potential bugs or low-quality predictions. Additional serving and monitoring infrastructure is required.,[],[],13,
Agent Profiling,Autonomous agents need to assume specific roles and characteristics to perform tasks effectively and influence the behavior of Large Language Models (LLMs) in a consistent and meaningful way.,"Designing LLM-based autonomous agents for specific tasks, simulations, or multi-agent systems where distinct roles, personalities, or social information are required to guide agent behavior.","Define agent profiles that encompass basic information (e.g., age, gender, career), psychological information (e.g., personalities), and social information (e.g., relationships between agents). These profiles are typically incorporated into the LLM's prompt to influence its responses and actions. Profiles can be generated through: 
- **Handcrafting:** Manually specifying agent characteristics (e.g., 'you are an outgoing person').
- **LLM-generation:** Automatically generating profiles based on predefined rules and seed examples using LLMs.
- **Dataset Alignment:** Obtaining profiles from real-world datasets (e.g., demographic backgrounds) to ensure agents reflect attributes of a real population.","Agents exhibit behaviors consistent with their assigned roles, personalities, or social information, leading to more realistic simulations, effective collaboration in multi-agent systems, and task accomplishment aligned with specific personas.","['Agent Memory Management', 'Agent Planning', 'Agent Action Execution', 'Prompt and Mechanism Engineering for Agent Capability Acquisition']","Social simulations (e.g., Generative Agent, AgentSims), multi-agent collaboration (e.g., MetaGPT, ChatDev, Self-collaboration for software development), personality trait exploration (e.g., PTLLM), toxicity studies of LLM output, user behavior simulation (e.g., RecAgent).",0,
Agent Memory Management,"LLM-based autonomous agents need to effectively store, retrieve, and process past information (perceptions, behaviors, thoughts) to accumulate experiences, self-evolve, and behave in a consistent, reasonable, and effective manner, overcoming the inherent context window limitations of LLMs.","LLM-based autonomous agents operating in continuous and dynamic environments that require long-range reasoning, learning from past interactions, and maintaining behavioral consistency over extended periods.","Implement a memory module inspired by human cognitive processes, typically combining short-term and long-term memory components, and defining formats and operations for memory interaction.
- **Memory Structures:**
  - **Unified Memory (Short-term):** Information is directly written into the LLM's context window for recent perceptions and in-context learning.
  - **Hybrid Memory (Short-term + Long-term):** Combines the context window for immediate information with external vector storage (e.g., vector databases) for consolidating and retrieving important, older information based on relevance.
- **Memory Formats:** Information can be stored as natural language, embeddings (for efficient retrieval), structured lists (e.g., hierarchical trees for subgoals, triplet phrases), or in databases (allowing SQL-like manipulation).
- **Memory Operations:**
  - **Memory Reading:** Extract relevant information based on criteria like recency, relevance, and importance scores.
  - **Memory Writing:** Store new perceptions, with strategies to handle memory duplication (e.g., condensing similar records) and overflow (e.g., FIFO, explicit deletion).
  - **Memory Reflection:** Agents independently summarize past experiences into more abstract, high-level insights, which can then be used to guide future actions or generate new insights hierarchically.","Agents gain the ability to accumulate experiences, maintain long-term consistency, perform long-range reasoning, and adapt their behavior based on past interactions, leading to more human-like and effective task accomplishment in complex environments.","['Agent Profiling', 'Agent Planning', 'Agent Action Execution', 'Prompt and Mechanism Engineering for Agent Capability Acquisition']","Conversation agents (e.g., RLP, SCM, MemorySandbox), embodied agents (e.g., SayPlan, DEPS, Generative Agent, AgentSims, GITM, Reflexion, SimplyRetrieve), knowledge management (e.g., MemoryBank, ChatDB, DBGPT, RETLLM), multi-agent collaboration (e.g., ChatDev, MetaGPT).",0,
Agent Planning,"LLM-based autonomous agents need to effectively decompose complex, long-horizon tasks into simpler subtasks and determine future actions, often in dynamic and unpredictable environments, to achieve goals.","LLM-based autonomous agents operating in environments that require multi-step reasoning, strategic decision-making, and adaptation to changes or failures during task execution.","Implement a planning module that empowers agents with human-like planning capabilities, enabling them to generate and, optionally, revise plans based on feedback.
- **Planning without Feedback:** Agents generate plans in a one-shot manner without iterative refinement based on execution outcomes.
  - **Single-path Reasoning (e.g., Chain of Thought - CoT):** Decomposes tasks into a linear sequence of intermediate steps, guiding the LLM to generate plans step-by-step, often using few-shot examples or trigger phrases.
  - **Multi-path Reasoning (e.g., Tree of Thoughts - ToT):** Explores multiple reasoning paths in a tree-like structure, allowing the agent to consider various options at each step and select the most promising one using search algorithms (e.g., BFS, DFS) or LLM evaluation.
  - **External Planner:** Leverages specialized, efficient external planning algorithms (e.g., PDDL solvers) by translating LLM-generated task descriptions into formal planning languages and then back into natural language actions.
- **Planning with Feedback:** Agents iteratively generate and refine plans based on real-time information received after taking actions.
  - **Environmental Feedback:** Incorporates objective signals from the environment (e.g., task completion status, observations, execution errors, scene graph updates) to adjust subsequent plans.
  - **Human Feedback:** Integrates subjective human input (e.g., corrections, preferences, clarifications) to align agent plans with human values and preferences, and to mitigate issues like hallucination.
  - **Model Feedback (Self-Refinement):** Utilizes internal feedback generated by the agent itself or auxiliary LLMs (e.g., self-critique, evaluation of reasoning steps, detailed verbal feedback) to identify and correct planning errors.","Agents can effectively tackle complex, long-horizon tasks by breaking them down, generating coherent action sequences, adapting to unforeseen circumstances, and continuously improving their planning capabilities, leading to more robust and human-like task accomplishment.","['Agent Profiling', 'Agent Memory Management', 'Agent Action Execution', 'Prompt and Mechanism Engineering for Agent Capability Acquisition']","Task planning for embodied agents (e.g., SayPlan, DEPS, LLMPlanner, Inner Monologue, Voyager, Ghost), general problem-solving (e.g., CoT, ToT, RAP), tool-use (e.g., HuggingGPT), software development (e.g., COLLM), robotics.",0,
Agent Action Execution,"LLM-based autonomous agents need to translate their internal decisions, plans, and retrieved memories into concrete, observable actions that interact with the environment, achieve specific goals, and potentially modify their internal state or trigger subsequent actions.","LLM-based autonomous agents operating in dynamic environments where they must perform physical or virtual actions, communicate, or explore, often requiring capabilities beyond the LLM's inherent knowledge or prone to hallucination.","Implement an action module that defines the goals, production strategies, available action space, and anticipated impacts of agent actions, influenced by the profiling, memory, and planning modules.
- **Action Goals:** Actions can be aimed at task completion (e.g., crafting an item, completing a function), communication (e.g., with other agents or humans for sharing information or collaboration), or environment exploration (e.g., discovering unknown skills or unfamiliar areas).
- **Action Production:** Actions are generated either by recalling relevant information from the agent's memory (Memory Recollection) or by strictly following pre-generated plans from the planning module (Plan Following).
- **Action Space:** Agents can leverage:
  - **External Tools:** To overcome LLM limitations (e.g., lack of expert knowledge, hallucination), agents can call external APIs (e.g., HuggingFace models, Python interpreters, RESTful APIs), query databases or knowledge bases for domain-specific information, or integrate other specialized external models (e.g., for multimodal processing).
  - **Internal Knowledge of LLMs:** Agents can rely on the LLM's inherent capabilities such as planning (decomposing complex tasks), conversation (generating high-quality dialogue), and common sense understanding (making human-like decisions).
- **Action Impact:** Actions can lead to changes in the environment (e.g., moving positions, collecting items, constructing buildings), alterations in the agent's internal states (e.g., updating memories, forming new plans, acquiring novel knowledge), or triggering new actions in a sequence.","Agents can effectively interact with their environment, achieve diverse goals, and adapt their behavior by strategically combining their internal LLM capabilities with external resources, leading to more versatile and capable autonomous systems.","['Agent Profiling', 'Agent Memory Management', 'Agent Planning', 'Prompt and Mechanism Engineering for Agent Capability Acquisition']","Embodied agents in games (e.g., Minecraft: Voyager, DEPS, GITM), software development (e.g., ChatDev, MetaGPT), recommendation systems (e.g., RecAgent), social simulations (e.g., Generative Agent, S3), robotics (e.g., SayCan, TidyBot), general tool-use (e.g., HuggingGPT, ToolFormer, Gorilla, ToolBench, RestGPT, TaskMatrixAI, ChemCrow, MMREACT).",0,
Finetuning for Agent Capability Acquisition,"LLM-based autonomous agents often lack task-specific capabilities, skills, and experiences required for effective performance in particular domains, beyond the general knowledge of the base LLM.","Enhancing the performance and versatility of LLM-based autonomous agents for specific tasks or domains, especially when using open-source LLMs or when a large amount of task-specific knowledge needs to be incorporated into the model's parameters.","Adjust the parameters of the underlying LLM by finetuning it on task-dependent datasets. This approach encodes task-specific knowledge directly into the model.
- **Finetuning with Human Annotated Datasets:** Utilize datasets manually created by human workers, often for aligning with human values, converting natural language to structured formats, or simulating specific human behaviors.
- **Finetuning with LLM Generated Datasets:** Leverage LLMs themselves to generate large-scale annotation data, offering a cost-effective and scalable alternative, though potentially with less perfect quality than human annotation.
- **Finetuning with Real-world Datasets:** Directly use datasets collected from real-world applications and interactions (e.g., web interaction logs, domain-specific text-to-SQL queries) to train agents for practical scenarios.","Agents acquire specialized knowledge, task-specific skills, and improved performance in targeted domains, leading to more accurate, aligned, and effective behavior by adjusting the model's internal parameters.",[],"Enhancing Agent Profiling (e.g., for uncommon roles or psychology characters), improving Agent Memory Management (e.g., for better natural language to structured memory conversion), refining Agent Planning (e.g., for domain-specific planning), boosting Agent Action Execution (e.g., for tool-using capability, web-related tasks, text-to-SQL tasks), aligning agents with human values (e.g., CoH), educational functions (e.g., EduChat), complex interactive reasoning (e.g., SWIFTSAGE).",0,
Prompt and Mechanism Engineering for Agent Capability Acquisition,"LLM-based autonomous agents need to acquire or unleash task-specific capabilities, skills, and experiences without finetuning the underlying LLM, often due to using closed-source models, the desire for flexible and dynamic adaptation, or the need to overcome context window limitations.","Enhancing the performance and versatility of LLM-based autonomous agents for specific tasks or domains, particularly when finetuning is not feasible, when dynamic, adaptive learning is required, or when leveraging existing LLM capabilities through external design.","Employ strategies that involve carefully designing prompts or engineering specialized mechanisms to guide the LLM's behavior and enable learning and evolution without altering its core parameters.
- **Prompt Engineering:** Craft valuable information into prompts (e.g., few-shot examples, trigger sentences like 'think step by step', agent beliefs, reflections on past failures) to influence LLM actions and elicit desired capabilities (e.g., complex task reasoning, self-awareness).
- **Mechanism Engineering:** Develop specialized modules, novel working rules, or other architectural strategies to enhance agent capabilities:
  - **Trial-and-Error:** The agent performs an action, a predefined critic evaluates it, and feedback (e.g., failure reasons, differences from human behavior) is incorporated to revise plans or actions iteratively.
  - **Crowdsourcing (Multi-agent Debate):** Multiple agents provide independent responses to a question; if inconsistent, they are prompted to incorporate others' solutions and iterate towards a consensus, enhancing collective and individual capabilities.
  - **Experience Accumulation:** The agent explores and, upon successful task completion, stores successful action sequences or refined skills (e.g., executable code) in memory for future retrieval and reuse in similar tasks.
  - **Self-driven Evolution:** The agent autonomously sets goals, explores the environment, receives feedback (e.g., from a reward function), and gradually improves its capabilities and knowledge according to its own preferences or through multi-agent interactions (e.g., teacher-student models, dynamic role adjustment).","Agents gain task-specific knowledge, skills, and adaptability without direct model finetuning, allowing for flexible and dynamic learning, improved performance, and the ability to learn and evolve in dynamic environments, especially for both open- and closed-source LLMs.","['Agent Profiling', 'Agent Memory Management', 'Agent Planning', 'Agent Action Execution']","Complex task reasoning (e.g., CoT, CoTSC, ToT), self-awareness in conversation (e.g., SocialAGI), retrospective reflection (e.g., Retroformer), recommender systems (e.g., RAH), multi-robot collaboration (e.g., RoCo), general problem-solving (e.g., PREFER), multi-agent debate (e.g., Du et al. 91), embodied agents in games (e.g., Minecraft: GITM, Voyager), app interaction (e.g., AppAgent), goal-setting (e.g., LMA3), multi-agent adaptation (e.g., SALLMMS), teacher-student learning (e.g., CLMTWA), multi-agent collaboration (e.g., NLSOM).",0,
Context-Driven ML Solution Design,"Designing effective and efficient ML solutions is challenging due to the need to align ML algorithms, data preparation, and evaluation metrics with specific business objectives, user preferences, data characteristics, and non-functional requirements (NFRs).","Developing ML systems for various business problems (e.g., loan approval, fraud detection, task assignment) where the optimal ML approach is highly dependent on the operational environment, available data, and desired system qualities.","Systematically capture and utilize 'Contexts' (User Contexts, Data Contexts, Model Contexts) and 'Softgoals' (NFRs) to guide the selection and configuration of ML algorithms, data preparation steps, and evaluation metrics. This involves:
    - **Algorithm Selection:** Choosing algorithms based on their applicability given User, Data, and Model Contexts, and their known contributions to Softgoals (e.g., 'kNearest Neighbor is applicable when Users desire simplicity', 'Nave Bayes is applicable when Features are independent').
    - **Data Preparation:** Specifying data transformations and cleansing steps that are conditional on Data Contexts and the requirements of chosen algorithms (e.g., 'Perform data normalization on numerical features when using kMeans algorithm').
    - **Evaluation Metric Selection:** Determining appropriate performance indicators based on User Contexts and business priorities (e.g., 'Precision should be used for evaluation when Users desire a low rate of false-positives').","Leads to ML solutions that are better tailored to specific business problems, more robust to varying conditions, and more aligned with user and business needs, reducing development time and improving solution quality.",['Anomaly Detection Strategy Selection'],"Loan approval systems, fraud detection systems, task assignment systems, and any ML application requiring adaptive design based on contextual factors and quality attributes.",22,
Anomaly Detection Strategy Selection,"Effectively detecting anomalies requires choosing the right ML approach (e.g., supervised, semi-supervised, unsupervised) based on the availability and nature of labeled anomaly data.","Building systems for anomaly detection (e.g., fraud detection, intrusion detection) where the dataset might contain either labeled examples of both normal and anomalous behavior, or only examples of normal behavior.","Implement a conditional strategy based on data availability:
    - If labeled examples of both anomalous and non-anomalous data are available, employ a supervised anomaly detection approach.
    - If only labeled examples of non-anomalous data are present (and anomalies are rare or unknown), employ a semi-supervised anomaly detection approach.","Ensures the most appropriate and effective anomaly detection model is selected and trained given the constraints of data labeling, leading to higher detection rates and reduced false positives/negatives.",['Context-Driven ML Solution Design'],"Fraud detection in insurance claims, network intrusion detection, manufacturing quality control, medical anomaly detection.",23,
LLM as an Agent with Tool Use (Web Browsing),"Large Language Models (LLMs) lack real-time, external knowledge and the ability to perform actions beyond text generation, limiting their utility for tasks requiring up-to-date information or interaction with external systems.","Tasks requiring an LLM to gather information from the web, navigate interfaces, or perform multi-step operations to answer questions or complete tasks.","Integrate an LLM with a text-based web browsing environment, allowing it to issue commands (e.g., search, click, scroll, quote) and receive contextual observations (e.g., current page text, search results). The LLM acts as an agent within this environment, making decisions based on its observations and issuing actions.","Enables LLMs to perform complex, long-form question answering, access up-to-date information, and interact with external tools, significantly expanding their capabilities beyond static knowledge.","['Behavior Cloning (for Agent Policy Learning)', 'Reward Modeling (for Preference-based Learning)', 'Reinforcement Learning from Human Feedback (RLHF)', 'Rejection Sampling (Best-of-N)', 'Reference Collection for Verifiability/Fact-checking', 'Retrieval-Augmented Generation (RAG)']","Long-form question answering, web automation, information retrieval, interactive systems, data collection.",0,
Behavior Cloning (for Agent Policy Learning),"Training an AI agent from scratch in a complex environment with a large action space is sample-inefficient and difficult, especially when the agent needs to learn specific interaction protocols (e.g., browser commands).",Initializing an agent's policy or teaching it specific interaction patterns in an environment where human demonstrations of desired behavior are available.,"Collect human demonstrations of the task within the environment (e.g., human experts using the web browser) and use supervised learning (fine-tuning) to train the agent to imitate these actions and commands.","Provides a strong baseline policy, enables the agent to understand the environment's interaction format, and significantly reduces the exploration problem for subsequent reinforcement learning, making the agent functional from the outset.","['LLM as an Agent with Tool Use (Web Browsing)', 'Reward Modeling (for Preference-based Learning)', 'Reinforcement Learning from Human Feedback (RLHF)', 'Rejection Sampling (Best-of-N)']","Agent initialization, learning interaction protocols, supervised fine-tuning of LLMs for interactive tasks, data generation for further training.",0,
Reward Modeling (for Preference-based Learning),"Directly optimizing for subjective qualities (e.g., factual accuracy, coherence, overall usefulness) in AI-generated outputs is hard because these qualities are difficult to quantify with a simple, programmatic reward function.","Optimizing AI systems (especially LLMs or agents) for subjective human preferences where direct human labeling of every output is infeasible, but pairwise comparisons of outputs are collectible.","Train a separate model (the Reward Model) to predict human preferences (e.g., an Elo score) based on comparisons of AI-generated outputs. This model takes an output (e.g., an answer with references) and assigns a scalar reward, which then provides a quantifiable signal for further optimization.","Translates subjective human preferences into a quantifiable reward signal, enabling scalable optimization of AI outputs for complex, human-centric criteria that are otherwise hard to define programmatically.","['Behavior Cloning (for Agent Policy Learning)', 'Reinforcement Learning from Human Feedback (RLHF)', 'Rejection Sampling (Best-of-N)']","AI alignment, preference learning, quality optimization, subjective evaluation, fine-tuning LLMs for human-preferred outputs.",0,
Reinforcement Learning from Human Feedback (RLHF),"Fine-tuning an agent's policy to maximize a complex, subjective reward function (often derived from human preferences) in an interactive environment, especially when direct human feedback is too slow or expensive for every step.","Improving an agent's performance in an environment where a Reward Model can provide a dense reward signal, and the agent needs to learn to explore and exploit the environment to achieve higher rewards.","Use a reinforcement learning algorithm (e.g., Proximal Policy Optimization - PPO) to fine-tune the agent's policy. The Reward Model's output is used as the environment reward, and a KL penalty to the initial policy (e.g., the Behavior Cloning policy) is often added to mitigate over-optimization and mode collapse.","Further optimizes the agent's policy beyond behavior cloning, leading to higher performance according to the learned reward function and better alignment with human preferences, often surpassing human demonstrator performance.","['LLM as an Agent with Tool Use (Web Browsing)', 'Behavior Cloning (for Agent Policy Learning)', 'Reward Modeling (for Preference-based Learning)', 'Rejection Sampling (Best-of-N)']","Agent policy optimization, AI alignment, complex task learning, fine-tuning LLMs for interactive tasks, improving subjective quality of outputs.",0,
Rejection Sampling (Best-of-N),"Improving the quality of AI-generated outputs (e.g., answers, actions) by leveraging a Reward Model without requiring additional training of the generative model, especially when inference-time compute is available.","When a Reward Model is available to score multiple generated outputs, and the goal is to select the highest-quality output from a set of candidates at inference time.","Generate 'N' candidate outputs from the base model (e.g., a Behavior Cloning or RL policy). Use the Reward Model to score each candidate, and then select the candidate with the highest reward score as the final output.","Significantly improves the quality of outputs according to the Reward Model, often outperforming direct generation from the base policy, by effectively 'filtering' for better samples. This method requires more inference-time compute but no additional training.","['Reward Modeling (for Preference-based Learning)', 'Behavior Cloning (for Agent Policy Learning)', 'Reinforcement Learning from Human Feedback (RLHF)']","Quality improvement, output selection, leveraging reward models at inference time, fine-tuning LLMs for better output quality without further training.",24,
Reference Collection for Verifiability/Fact-checking,"AI-generated answers, especially from LLMs, can suffer from hallucinations or factual inaccuracies, making it difficult for humans to trust or verify the information provided.","AI systems generating factual information or answers to questions where accuracy, transparency, and verifiability are critical for user trust and evaluation.","Design the AI system (e.g., an agent browsing the web) to explicitly identify and collect supporting passages (references) from its information sources while generating an answer. These references are then presented alongside the answer, often with citations.","Enhances the factual accuracy of answers, makes AI outputs more transparent, allows human evaluators to more easily assess factual accuracy, and builds user trust by providing traceable sources for claims.","['LLM as an Agent with Tool Use (Web Browsing)', 'Retrieval-Augmented Generation (RAG)', 'AI Debate (for Fact-checking/Robustness)']","Factual question answering, summarization, content generation, truthfulness, academic research assistance.",0,
Retrieval-Augmented Generation (RAG),"Large Language Models (LLMs), despite vast pre-training, can struggle with up-to-date information, domain-specific knowledge, or factual accuracy, leading to 'hallucinations' or outdated information.","Generating answers or text that requires access to external, up-to-date, or specific knowledge bases beyond the LLM's training data.","Integrate a retrieval mechanism (e.g., a search engine, a document database, or an active browsing agent) that fetches relevant information based on the input query. The LLM then uses this retrieved information as additional context to generate a more informed, accurate, and current response.","Reduces hallucinations, improves factual accuracy, provides access to current and specific information, and enhances the overall quality and trustworthiness of generated text by grounding it in external data.","['LLM as an Agent with Tool Use (Web Browsing)', 'Reference Collection for Verifiability/Fact-checking']","Open-domain question answering, knowledge-intensive NLP tasks, summarization, content creation, chatbots.",0,
AI Debate (for Fact-checking/Robustness),"Evaluating the factual accuracy and robustness of AI-generated claims, especially in complex or subjective domains, is challenging and prone to human bias or oversight, and simple reference checking might be insufficient (e.g., cherry-picking references).","When high-stakes factual accuracy or robust reasoning is required from AI systems, and a single AI's output or a simple evaluation process is not sufficient to ensure reliability.","Train multiple AI models to 'debate' a claim, with some models arguing for and others against, by finding supporting and refuting evidence. A human or another AI then judges the debate to determine the truthfulness or robustness of the claim, considering all presented evidence.","Improves the reliability and trustworthiness of AI systems by forcing them to consider counter-evidence and present a more balanced assessment, mitigating issues like cherry-picking references and enhancing the overall truthfulness of AI outputs.","['Reference Collection for Verifiability/Fact-checking', 'Recursive Reward Modeling / Iterated Amplification']","Fact-checking, truthfulness, robustness, AI alignment, complex reasoning, safety-critical AI applications.",0,
Recursive Reward Modeling / Iterated Amplification,"Scaling human oversight and evaluation to increasingly complex AI systems, where humans might struggle to fully understand or evaluate the AI's internal workings or complex outputs due to cognitive limitations or time constraints.","Developing highly capable AI systems where direct human evaluation of the final output becomes too difficult or time-consuming, and human evaluators need assistance.","Train an AI to assist humans in evaluating other AI systems or its own outputs. This can involve the AI breaking down complex tasks into simpler sub-tasks, finding evidence, or explaining its reasoning, which humans can then more easily evaluate. This process can be recursive, with AIs helping to evaluate other AIs that are helping humans.","Enables more scalable and accurate human oversight of complex AI systems, potentially leading to better alignment, safety, and the ability to train AIs on tasks that are too complex for direct human supervision.",['AI Debate (for Fact-checking/Robustness)'],"AI alignment, scalable oversight, complex task evaluation, AI safety research, advanced human-AI collaboration.",0,
ReAct (Reasoning and Acting),"Large Language Models (LLMs) struggle with complex tasks requiring both dynamic reasoning and interaction with external environments. Traditional approaches often separate reasoning (e.g., Chain-of-Thought) and acting, leading to issues like hallucination, error propagation in reasoning, or a lack of abstract reasoning and working memory in acting.","LLMs applied to tasks requiring both language-based reasoning and interaction with external environments or tools (e.g., APIs, simulated worlds).","Augment the LLM's action space to include both domain-specific actions and freeform language 'thoughts' (reasoning traces). These thoughts are interleaved with actions and observations, allowing the model to dynamically plan, track progress, handle exceptions, and incorporate external information (act to reason) while also using reasoning to guide actions (reason to act).","Improved performance, interpretability, trustworthiness, and robustness across diverse language reasoning and decision-making tasks, overcoming limitations of reasoning-only or acting-only approaches. It enables dynamic planning, self-correction, and grounding in external facts.","['Chain-of-Thought (CoT) Prompting', 'Self-Consistency (CoT-SC)', 'Combining Internal and External Knowledge', 'Inner Monologue (IM)', 'SayCan']","Question Answering (HotpotQA), Fact Verification (Fever), Text-based Games (ALFWorld), Web Navigation (WebShop), Human-in-the-loop behavior correction, general task solving for agentic LLMs.",0,
Chain-of-Thought (CoT) Prompting,"Large Language Models (LLMs) often struggle with complex reasoning tasks, producing incorrect direct answers without showing their intermediate steps, making their reasoning opaque and prone to errors.","LLMs used for tasks requiring multi-step reasoning, such as arithmetic, commonsense reasoning, symbolic reasoning, or complex question answering.",Prompt the LLM to generate a series of intermediate reasoning steps (a 'chain of thought') before providing the final answer. This is typically achieved by providing a few in-context examples that demonstrate this step-by-step reasoning process.,"Elicits emergent reasoning capabilities in LLMs, significantly improving performance on complex reasoning tasks and making the model's decision-making process more interpretable. However, it can suffer from hallucination and lack of grounding in external facts.","['ReAct (Reasoning and Acting)', 'Self-Consistency (CoT-SC)', 'Least-to-Most Prompting', 'Zero-shot Chain-of-Thought (Zero-shot CoT)', 'Selection-Inference', 'STaR (Self-Taught Reasoner)', 'Faithful Reasoning', 'Scratchpads', 'Combining Internal and External Knowledge']","Arithmetic, commonsense reasoning, symbolic reasoning, question answering, fact verification.",0,
Self-Consistency (CoT-SC),"Chain-of-Thought (CoT) reasoning can be brittle, and a single reasoning path generated by an LLM might lead to an incorrect answer due to minor errors or suboptimal choices.",Improving the robustness and accuracy of Chain-of-Thought reasoning in LLMs for complex tasks.,"Generate multiple diverse Chain-of-Thought reasoning paths by sampling from the LLM (e.g., using a higher decoding temperature). Then, select the most consistent answer by taking a majority vote among the final answers derived from these multiple reasoning paths.","Significantly boosts performance over vanilla CoT by leveraging the diversity of reasoning paths and reducing reliance on a single, potentially flawed, trace, leading to more robust and accurate results.","['Chain-of-Thought (CoT) Prompting', 'ReAct (Reasoning and Acting)', 'Combining Internal and External Knowledge']","Reasoning tasks where CoT is applied, question answering, fact verification.",0,
Combining Internal and External Knowledge,"LLMs possess strong internal knowledge and reasoning capabilities (e.g., CoT) but can hallucinate or provide outdated information. Conversely, external tools (e.g., ReAct's actions with APIs) provide factual, up-to-date information but might lack flexible reasoning structure or suffer from search errors.","Knowledge-intensive tasks where both the LLM's internal knowledge and access to external, up-to-date information are crucial for accurate and robust problem-solving.","Integrate approaches that leverage internal LLM knowledge (like Self-Consistency) with approaches that interact with external tools (like ReAct). The model dynamically switches between these methods based on heuristics, such as backing off to internal reasoning if external tool interaction fails, or consulting external tools if internal reasoning is not confident.","Achieves superior overall performance by synergizing the strengths of both internal and external knowledge sources, leading to more factual, grounded, and robust reasoning while mitigating individual weaknesses.","['ReAct (Reasoning and Acting)', 'Self-Consistency (CoT-SC)', 'Chain-of-Thought (CoT) Prompting']","Question Answering (HotpotQA), Fact Verification (Fever).",0,
Inner Monologue (IM),"Embodied agents need to reason about their actions and environment state to achieve goals, but simple action prediction might lack sufficient context, high-level planning, or a mechanism for internal state tracking.","Embodied agents operating in interactive environments, requiring internal state tracking and goal-oriented behavior.","Inject feedback from the environment as an 'inner monologue' into the agent's input. This monologue is typically limited to observations of the environment state and what needs to be completed for the goal, providing a form of internal reflection to motivate actions.","Motivates actions and provides some form of internal state representation, improving performance in embodied tasks. However, it can be less flexible and comprehensive than more advanced reasoning patterns like ReAct, as it's often limited to reactive feedback rather than proactive, diverse reasoning.","['ReAct (Reasoning and Acting)', 'SayCan']","Robotic action planning, embodied agents in interactive environments.",0,
Least-to-Most Prompting,"Complex reasoning tasks are difficult for LLMs to solve directly in a single step or with simple Chain-of-Thought, especially when they involve many interdependent subproblems.","LLMs tackling multi-step, complicated reasoning problems that can be naturally decomposed into a sequence of simpler subproblems.","Decompose the main problem into a series of simpler, dependent subproblems. The LLM first solves the initial subproblem, then uses its solution to help formulate and solve the next subproblem, and so on, until the final solution is reached. This sequential problem-solving leverages the LLM's ability to build upon previous results.","Enables LLMs to tackle more complex reasoning tasks by breaking them down into manageable steps, significantly improving accuracy and capability on problems that are otherwise intractable.",['Chain-of-Thought (CoT) Prompting'],"Complicated reasoning tasks, multi-step problem solving.",0,
Zero-shot Chain-of-Thought (Zero-shot CoT),"Standard Chain-of-Thought (CoT) prompting typically requires a few in-context examples to demonstrate the reasoning process, which can be costly to create or unavailable in certain scenarios.",Applying Chain-of-Thought reasoning with LLMs when no specific in-context examples are provided or practical to include in the prompt.,"Elicit Chain-of-Thought reasoning by simply adding a generic phrase like 'Let's think step by step' to the prompt, without providing any example reasoning traces. This phrase acts as an instruction to the LLM to generate intermediate steps.","Enables CoT reasoning in a zero-shot setting, making the technique more broadly applicable and reducing the need for example engineering, while still eliciting reasoning capabilities.",['Chain-of-Thought (CoT) Prompting'],Any task where CoT is applicable but few-shot examples are not practical or available.,0,
Selection-Inference,"Multi-step logical reasoning with LLMs can be prone to errors and lack transparency, making it hard to verify the reasoning process and ensure faithfulness to the given premises.","LLMs performing interpretable and accurate logical reasoning, especially in multi-step scenarios where the derivation of conclusions needs to be clear.","Decompose the reasoning process into two distinct steps: a 'selection' step where the LLM identifies and extracts only the relevant information or premises from a larger context, and an 'inference' step where it draws conclusions based *only* on the previously selected information.","Improves interpretability and accuracy for logical reasoning by structuring the process and making the intermediate steps explicit and verifiable, enhancing the faithfulness of the reasoning.","['Chain-of-Thought (CoT) Prompting', 'Faithful Reasoning']","Interpretable logical reasoning, multi-step reasoning.",0,
STaR (Self-Taught Reasoner),"Acquiring large datasets of high-quality, human-annotated reasoning rationales for fine-tuning LLMs is expensive and time-consuming, limiting the scalability of reasoning improvements.","Improving LLM reasoning capabilities through fine-tuning, especially when human-annotated rationales are scarce or difficult to obtain at scale.","Bootstrap the reasoning process by having the LLM generate its own rationales for problem-solving. These self-generated rationales are then filtered (e.g., by checking if they lead to correct answers) and used to fine-tune the model, iteratively improving its reasoning abilities without extensive human annotation.","Enables LLMs to learn and improve their reasoning capabilities with less reliance on expensive human-annotated rationales, making the process more scalable and efficient for enhancing reasoning.",['Chain-of-Thought (CoT) Prompting'],"Improving reasoning in LLMs, generating rationales for fine-tuning.",25,
Faithful Reasoning,"Ensuring that LLM-generated multi-step reasoning is faithful to the premises and does not introduce ungrounded or hallucinated facts, which is a common challenge in complex reasoning.","Multi-step reasoning tasks where faithfulness, verifiability, and adherence to given information at each step are critical.","Decompose the multi-step reasoning process into several distinct steps, with each step potentially performed by a dedicated or specialized LLM. This modular approach allows for more control and verification at each stage, ensuring that the reasoning remains faithful to the input and intermediate derivations.","Improves the faithfulness and accuracy of multi-step reasoning by enforcing a structured and verifiable process, reducing the likelihood of hallucination and ungrounded conclusions.","['Chain-of-Thought (CoT) Prompting', 'Selection-Inference']",Multi-step reasoning requiring high faithfulness and verifiability.,0,
Scratchpads,"LLMs often struggle with multi-step computation problems, making errors in intermediate steps or failing to track complex calculations accurately.","LLMs performing tasks that involve explicit multi-step computations, arithmetic, or symbolic manipulation where intermediate steps are crucial.","Fine-tune the LLM to explicitly generate and use intermediate computation steps, much like a human uses a scratchpad. This involves training the model to output these intermediate steps alongside the final answer, making the computation process transparent and learnable.","Improves performance on multi-step computation problems by making the intermediate steps explicit and learnable, enhancing the model's ability to track and execute complex calculations accurately.",['Chain-of-Thought (CoT) Prompting'],"Multi-step computation, arithmetic, symbolic reasoning.",0,
SayCan,"LLMs can generate high-level plans for embodied agents, but these plans often lack grounding in the physical environment, leading to unexecutable, unsafe, or inefficient actions.","Using LLMs for robotic action planning in real-world or simulated embodied environments where physical constraints, affordances (what actions are possible), and visual grounding are important.","Combine LLM-generated action predictions with an 'affordance model' that is grounded in the visual or physical environment. The LLM suggests possible actions based on high-level goals, and the affordance model then re-ranks or filters these actions based on their physical feasibility and likelihood of success in the current environment state.","Enables LLMs to generate grounded, executable, and safe plans for robots, bridging the gap between high-level language instructions and low-level physical actions by incorporating real-world constraints.","['Inner Monologue (IM)', 'ReAct (Reasoning and Acting)']","Robotic action planning, embodied AI, grounding language in physical actions.",0,
Retrieval-Augmented Generation (RAG),"Large Language Models (LLMs) often lack access to up-to-date, domain-specific, or proprietary knowledge, leading to factual inaccuracies, 'hallucinations,' or an inability to answer questions requiring external information. Modifying model weights for every new piece of knowledge is computationally expensive and impractical.","Building intelligent systems that require LLMs to provide accurate, grounded, and comprehensive answers based on external, dynamic, or specialized information sources, rather than solely relying on their pre-trained parametric knowledge.","Integrate an external information retrieval system with an LLM. When a query is received, a retriever first fetches relevant documents, passages, or contexts from a large corpus or database. These retrieved contexts are then provided to the LLM as additional input, alongside the original query, enabling the LLM to generate an answer that is grounded in the provided external information.","LLMs can access and incorporate up-to-date and domain-specific knowledge, significantly reducing factual errors and hallucinations. This approach allows LLMs to adapt to new information and domains without requiring costly re-training or fine-tuning of their core weights.",['Unified Reranker-Generator LLM (RankRAG)'],"Knowledge-intensive NLP tasks, open-domain question answering, conversational AI, fact verification, domain-specific information retrieval, content generation requiring external validation.",0,"['Generative AI', 'LLM-specific', 'Knowledge & Reasoning']"
Unified Reranker-Generator LLM (RankRAG),"Traditional Retrieval-Augmented Generation (RAG) pipelines face several limitations: 1) They often rely on separate, less capable models for initial retrieval and subsequent reranking, leading to suboptimal context selection. 2) LLMs struggle with processing a large number of retrieved contexts due to efficiency concerns and the introduction of irrelevant or noisy information, which can degrade generation accuracy. 3) Dedicated ranking models often exhibit limited zero-shot generalization capabilities compared to versatile LLMs. 4) There's a persistent trade-off in selecting the optimal number of top-k contexts: too few can compromise recall, while too many introduce noise and distract the LLM.","Improving the effectiveness, efficiency, and generalization of RAG systems by optimizing the context selection and utilization process. This pattern is particularly relevant when aiming to leverage the inherent capabilities of LLMs for both understanding relevance and generating coherent text, and when addressing the limitations of multi-stage RAG architectures that use distinct models for each step.","Instruction-tune a *single Large Language Model* to perform both context ranking (reranking) and answer generation within a unified RAG framework. The training process involves a specialized instruction-tuning blend that incorporates: 
1.  **Context-rich QA data:** To enhance the LLM's ability to use context for generation.
2.  **Retrieval-augmented QA data:** Including hard negative contexts (irrelevant retrieved passages) to improve the LLM's robustness against noisy information.
3.  **Context ranking data:** Explicitly training the LLM to assess query-passage relevance (e.g., True/False judgments).
4.  **Retrieval-augmented ranking data:** Training the LLM to identify multiple relevant passages from a given set of retrieved contexts.
All these diverse tasks are cast into a standardized `(question, context, answer)` format to enable mutual enhancement and effective knowledge transfer between ranking and generation capabilities. 
At inference, the system follows a 'Retrieve-Rerank-Generate' pipeline: 
1.  An initial retriever fetches a broader set of top-N contexts.
2.  The instruction-tuned LLM then acts as a reranker, calculating relevance scores for these N contexts and selecting a refined top-k (where k < N).
3.  The *same* instruction-tuned LLM then uses these refined top-k contexts to generate the final answer.","Significantly improved performance on knowledge-intensive NLP tasks compared to existing RAG methods and dedicated ranking models, often with a smaller amount of ranking data. Enhanced robustness to irrelevant contexts during generation and superior zero-shot generalization capability to new domains (e.g., biomedical) without additional domain-specific fine-tuning. This approach effectively balances recall and precision by dynamically selecting the most relevant contexts, leading to more accurate and grounded answers.",['Retrieval-Augmented Generation (RAG)'],"Open-domain Question Answering (OpenQA), Fact Verification, Conversational Question Answering (ConvQA), and various domain-specific RAG applications (e.g., biomedical QA), where precise context selection and robust generation are critical.",0,"['Generative AI', 'LLM-specific', 'Knowledge & Reasoning', 'MLOps']"
Chain-of-Thought (CoT) Prompting,"Large Language Models (LLMs) struggle with complex multi-step reasoning tasks, often producing incorrect or non-transparent answers.","Tasks requiring sequential logical steps, arithmetic, or symbolic manipulation, where intermediate thoughts can guide the final answer.",Prompt the LLM to generate a series of intermediate reasoning steps or 'thoughts' before providing the final answer. This makes the LLM's reasoning process explicit.,"Improves the LLM's ability to perform complex reasoning, enhances transparency, and often leads to more accurate results.","['Tree of Thoughts / Graph of Thoughts', 'Plan-and-Solve Prompting / Decomposed Prompting']","Complex question answering, mathematical problem solving, logical deduction.",0,
Tree of Thoughts / Graph of Thoughts,"Linear Chain-of-Thought reasoning can be limited, potentially getting stuck on incorrect paths or failing to explore diverse reasoning trajectories.","Highly complex problems where multiple reasoning paths might exist, or where intermediate steps have non-linear dependencies and require aggregation.","Expands the reasoning process into a non-linear structure. Tree of Thoughts allows the LLM to explore multiple reasoning branches, evaluate intermediate thoughts, and backtrack. Graph of Thoughts further models reasoning as a graph, enabling more flexible connections between thoughts, aggregation of information from multiple paths, and handling of complex dependencies.","Enhances the robustness and comprehensiveness of LLM reasoning by allowing for broader exploration, self-correction, and synergistic combination of ideas.","['Chain-of-Thought (CoT) Prompting', 'Plan-and-Solve Prompting / Decomposed Prompting']","Strategic planning, complex problem-solving, multi-faceted decision making.",0,
Plan-and-Solve Prompting / Decomposed Prompting,"LLMs often struggle with complex, multi-faceted tasks that require a structured approach, leading to errors or incomplete solutions.","Tasks that can be naturally broken down into a sequence of smaller, more manageable subtasks or require a strategic approach before execution.","Prompt the LLM to first generate a high-level plan or decompose the complex task into a series of subtasks. Then, the LLM executes each subtask or step of the plan sequentially.","Improves the LLM's ability to tackle complex problems by providing a structured framework, reducing cognitive load, and making the problem-solving process more systematic.","['Chain-of-Thought (CoT) Prompting', 'Tree of Thoughts / Graph of Thoughts']","Multi-step instruction following, complex code generation, long-horizon task execution.",0,
Agentic LLM with Tool Use,"Large Language Models (LLMs) are limited by their training data cutoff, lack real-time information, and cannot interact with external environments or perform actions.","Tasks requiring up-to-date factual knowledge, interaction with external APIs/databases, environment manipulation, or dynamic information retrieval.","Treat the LLM as an autonomous agent that can both reason (generate thoughts) and act (execute external tools or interact with an environment). The LLM observes the environment, decides on an action, executes it, and then incorporates the observation into its next reasoning step.","Extends LLM capabilities beyond static knowledge, enabling real-time information access, dynamic problem-solving, and interaction with the real world.","['Retrieval-Augmented Reasoning', 'Reasoning on Graphs']","Web browsing, API interaction, robotics, complex data analysis, knowledge graph querying.",0,
Reasoning Verification (Entailer),"LLMs can generate plausible but incorrect or unfaithful reasoning steps, leading to unreliable outputs, especially in critical applications.","Scenarios where the truthfulness and faithfulness of each reasoning step are paramount, and errors can have significant consequences.","Introduce a separate verification mechanism (e.g., another LLM, a rule-based system, or a factual knowledge base checker) to validate the logical consistency and factual accuracy of the reasoning steps generated by the primary LLM.",Enhances the trustworthiness and reliability of LLM reasoning by identifying and potentially correcting erroneous or hallucinated steps.,['Monte-Carlo Planning for Faithful Reasoning'],"High-stakes decision support, legal reasoning, medical diagnosis, scientific discovery.",0,
Monte-Carlo Planning for Faithful Reasoning (FAME),"Generating reasoning steps that are consistently faithful to the underlying facts or logical rules, especially in complex, multi-step scenarios.","Tasks where the faithfulness of the reasoning process is critical, and exploring multiple potential reasoning paths can help identify the most reliable one.",Employ Monte-Carlo planning techniques to explore a search space of possible reasoning steps. This involves simulating different reasoning trajectories and evaluating their faithfulness or correctness to guide the generation of more reliable steps.,"Improves the faithfulness of LLM-generated reasoning by systematically exploring and validating potential paths, reducing the likelihood of unfaithful deductions.","['Reasoning Verification', 'Tree of Thoughts / Graph of Thoughts']","Complex logical inference, knowledge graph reasoning, multi-hop question answering.",0,
Retrieval-Augmented Reasoning,"LLMs suffer from a lack of up-to-date knowledge, are prone to factual hallucinations, and have limited capacity for very long contexts.","Knowledge-intensive tasks, factual question answering, or scenarios where LLMs need to reason over specific, external, or dynamic information.","Integrate a retrieval mechanism that fetches relevant external knowledge (e.g., documents, knowledge graph triples, database entries) based on the input query or intermediate reasoning steps. This retrieved knowledge is then provided to the LLM as context to inform its reasoning and generation.","Significantly improves factual accuracy, reduces hallucinations, enables access to real-time or proprietary knowledge, and enhances the faithfulness of LLM reasoning.","['Agentic LLM with Tool Use', 'Reasoning on Graphs']","Open-domain question answering, factual summarization, knowledge graph question answering.",0,
LLM as a Semantic Parser,"Translating natural language questions or commands into precise, executable structured queries (e.g., SPARQL, SQL) for knowledge bases or databases.","Question answering systems that need to leverage structured data for accurate and interpretable answers, where direct natural language processing is insufficient.",Use an LLM to generate a structured query from a natural language input. This query is then executed against a knowledge graph or database to retrieve the answer.,"Enables LLMs to access and reason over structured knowledge with high precision, providing interpretable results by showing the executed query. However, it relies on the executability and correctness of the generated query.","['Agentic LLM with Tool Use', 'Retrieval-Augmented Reasoning']","Knowledge Graph Question Answering (KGQA), database querying, structured data analysis.",0,
Reasoning on Graphs (RoG),"LLMs lack up-to-date knowledge and hallucinate during reasoning on complex Knowledge Graph Question Answering (KGQA) tasks, and existing KG-LLM methods often overlook the crucial structural information of KGs.","Knowledge Graph Question Answering (KGQA) tasks requiring faithful, interpretable, and structurally-aware reasoning over large and dynamic knowledge graphs.","A planning-retrieval-reasoning framework that synergizes LLMs with KGs:
1.  **Planning Module:** LLMs generate 'relation paths' (sequences of relations grounded by KGs) as faithful plans.
2.  **Retrieval Module:** These relation paths are used to retrieve valid 'reasoning paths' (instances of relation paths with specific entities) from the KGs.
3.  **Reasoning Module:** The LLM then conducts faithful reasoning based on the retrieved reasoning paths and generates answers with interpretable explanations.
The system is optimized through planning optimization (distilling KG knowledge into LLMs for path generation) and retrieval-reasoning optimization (enabling LLMs to reason based on retrieved paths). It's also designed to be plug-and-play with different LLMs during inference.","Achieves state-of-the-art performance on KG reasoning tasks, generates faithful and interpretable reasoning results by leveraging KG structure, and can seamlessly integrate with and improve the performance of arbitrary LLMs.","['Retrieval-Augmented Reasoning', 'Plan-and-Solve Prompting / Decomposed Prompting', 'Agentic LLM with Tool Use', 'Monte-Carlo Planning for Faithful Reasoning']","Knowledge Graph Question Answering, complex factual reasoning, interpretable AI.",0,
