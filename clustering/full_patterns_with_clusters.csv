,Pattern Name,Problem,Context,Solution,Result,Related Patterns,Uses,Category,Cluster Number,AI Design Pattern Category
0,LLMs as Knowledge Base,"Traditional knowledge graphs (KGs) for recommender systems are limited, sparse, expensive to construct/complete, and lack extensive factual and cross-domain knowledge, leading to suboptimal recommendation performance.","Recommender systems that can benefit from explicit knowledge to understand user-item relationships, enhance explainability, and provide cross-domain recommendations.","Leverage Large Language Models (LLMs) to retrieve factual and commonsense knowledge. LLMs are used for knowledge graph completion (predicting missing facts, entities, relations) and knowledge graph construction (entity discovery, coreference resolution, relation extraction, end-to-end KG building from raw text). This includes distilling commonsense facts from LLMs.","More extensive and up-to-date knowledge bases, improved understanding of entity relations, enhanced recommendation accuracy, relevance, and personalization, particularly for cross-domain scenarios. Automation of knowledge graph construction.","['LLMs as Content Interpreter', 'LLMs as Explainer']","Enhancing Click-Through Rate (CTR) prediction models, graph augmentation strategies, determining complementary relationships between entities in industrial recommenders.","['Knowledge & Reasoning Patterns', 'LLM-specific Patterns']",13,
1,LLMs as Content Interpreter,"Content-based recommender systems often suffer from sparse content features and traditional methods struggle to capture deep semantic information, long-context dependencies, and generalize effectively from textual content.","Recommender systems that rely on textual attributes and characteristics of items (e.g., news articles, product descriptions, reviews) to understand item properties and match user preferences.","Employ powerful pretrained language models (PLMs) like BERT and GPT as advanced content interpreters. This involves fine-tuning PLMs for recommendation-specific objectives, initializing news encoders with PLMs, using PLMs to generate universal continuous representations from item descriptions for cold-start and cross-domain recommendations, and instruction tuning to formulate recommendation as an instruction-following task.","Enhanced understanding and interpretation of textual content, improved recommendations, better capture of user interests, alleviation of cold-start problems, facilitation of cross-domain recommendations, and strong generalization abilities.","['LLMs as Knowledge Base', 'Prompt Design Patterns']","News recommendation, tag recommendation, tweet representations, code example recommendation, zero-shot/few-shot recommendation, rating prediction, sequential recommendation.","['LLM-specific Patterns', 'Personalization Pattern']",13,
2,LLMs as Explainer,"Most recommender systems are black boxes, leading to a lack of transparency and diminished user trust. Traditional explanation methods are often formulaic, lack diversity, and are tightly coupled with specific models, limiting generalizability.","Recommender systems where users desire comprehensible, persuasive, and transparent justifications for recommendations, and where system errors need to be identifiable.","Leverage LLMs' generative ability, extensive training data, and in-context learning capabilities (zero-shot, few-shot, Chain-of-Thought prompting) to craft customized, precise, natural, and adaptable explanations. LLMs can also provide model-agnostic interpretations of complex deep learning recommendation models.","Improved user trust, transparency, and satisfaction; more diverse and coherent explanations; real-time feedback integration; a versatile and scalable interpretation framework applicable across various recommendation models.","['Prompt Design Patterns', 'AI–Human Interaction Patterns', 'Generative AI Patterns']","Generating justifications for drug recommendations, product recommendations, and general explainable recommendations.","['AI–Human Interaction Patterns', 'LLM-specific Patterns', 'Generative AI Patterns', 'Prompt Design Patterns']",13,
3,LLMs as Common System Reasoner (Direct Recommendation),"Traditional recommendation models typically require explicit training or fine-tuning for new tasks or domains, making adaptation to new cases without extensive data and training challenging.","Recommender systems aiming to provide recommendations (e.g., rating prediction, ranking prediction) with minimal or no explicit training, especially when operating on open-domain datasets.","Utilize LLMs' emergent abilities of in-context learning (zero-shot/few-shot) and step-by-step reasoning (Chain-of-Thought). By providing natural language instructions and/or a few input-output demonstrations in the prompt, LLMs can generate recommendations without explicit training. Chain-of-Thought prompting guides LLMs to break down complex tasks into intermediate reasoning steps.","Ability to make predictions on new recommendation cases with minimal or no training, improved recommendation performance through multi-step reasoning, particularly effective in open-domain scenarios.","['Prompt Design Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns']","Rating prediction, ranking prediction, sequential recommendation, and reranking tasks in open-domain datasets like MovieLens and Amazon Books.","['LLM-specific Patterns', 'Prompt Design Patterns', 'Knowledge & Reasoning Patterns', 'Personalization Pattern']",13,
4,LLMs as Common System Reasoner (Automated Selection for AutoML),"Automated Machine Learning (AutoML) in recommender systems involves complex, large search spaces (e.g., embedding size, features, interactions, model architecture) and costly manual setup or time-consuming traditional search strategies.","The design and optimization of recommender systems, particularly in areas like Neural Architecture Search (NAS) and feature engineering, where automation is desired to reduce manual effort and improve efficiency.","Leverage LLMs' memorization and reasoning capabilities to automate aspects of AutoML. LLMs can generate network architectures, act as black-box agents to propose better-performing architectures based on past trials, or be integrated into existing search algorithms (e.g., genetic algorithms) to guide the search process and generate candidate crossovers/mutations.","Reduced manual effort and improved efficiency in AutoML for recommender systems, generation of reasonable or better-performing architectures, and enhanced search performance when LLMs are integrated with other optimization algorithms.","['Planning Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns']","Neural Architecture Search (NAS) for recommender systems, feature interaction search.","['MLOps Patterns', 'Planning Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns']",13,
5,LLMs as Conversational Agent,"Traditional conversational recommender systems (CRSs) face challenges with scalability, lack of synergy between components, and a high demand for supervised training data. They also struggle to maintain context and memory over long conversations due to token limits.","Personalized assistance and recommendation systems that interact with users through natural language dialogues to uncover preferences, provide real-time recommendations, and adapt strategies based on user feedback.","Employ LLMs as the core of CRSs due to their inherent conversational abilities, intelligence, and knowledge. This involves: 1) Leveraging LLMs' pre-trained conversational capabilities. 2) Fine-tuning LLMs with private, domain-specific dialogue data, potentially generated by user simulators. 3) Utilizing tool learning, where LLMs act as controllers to integrate and use traditional recommendation models or external data sources. 4) Implementing memory augmentation techniques to store and retrieve user profiles and facts from long dialogue contexts.","Real-time understanding of user intents, adaptive and personalized recommendations, reduced data dependency for generative dialogue models, improved handling of private domain data, and enhanced ability to manage long conversational contexts.","['Tools Integration Patterns', 'Prompt Design Patterns', 'LLM-specific Patterns', 'AI–Human Interaction Patterns']","Building enterprise-level CRSs, open-domain recommendations (movies, music, news, games), personalized assistants.","['Agentic AI Patterns', 'AI–Human Interaction Patterns', 'LLM-specific Patterns', 'Tools Integration Patterns', 'Personalization Pattern']",13,
6,LLM-based Tool Learning,"LLMs have limitations in accessing knowledge beyond their training data, performing complex computations, or interacting with external systems. They may also struggle to break down complex tasks into executable subtasks autonomously.",Any AI system where foundational models (LLMs) need to enhance their task-solving capabilities by combining with specialized external tools to overcome inherent limitations.,"Apply LLMs as intelligent controllers to select, manage, and interact with various existing AI models or specialized tools. LLMs comprehend problem statements, decompose tasks into subtasks, convert them into executable instructions for tools, and then aggregate the results. This can involve LLMs learning to use tools through self-supervised methods or acting as autonomous agents.","Enhanced task-solving capabilities beyond the LLM's intrinsic knowledge, more accurate and efficient solutions for complex problems, access to external, real-time knowledge and functionalities, and improved generalization across diverse tasks.","['Agentic AI Patterns', 'Prompt Design Patterns', 'LLM-specific Patterns']","Solving general AI tasks (e.g., HuggingGPT), complex visual tasks (Visual ChatGPT), web browsing, mathematical reasoning, robotic tasks, and general downstream tasks.","['Tools Integration Patterns', 'Agentic AI Patterns', 'LLM-specific Patterns']",11,
7,LLM-augmented Recommendation Engine,"LLMs struggle to memorize domain-specific knowledge (e.g., item corpora, user profiles) and are susceptible to temporal generalization problems (external knowledge evolving), leading to inaccurate or non-grounded recommendations in personalized systems.","Personalization scenarios where LLMs act as recommendation agents but require grounding in specific, up-to-date, or private domain item corpora and detailed user data.","Equip LLMs with external tools, specifically dedicated recommendation engines, search engines, and databases. The LLM acts as an orchestrator, using search engines for external knowledge, a multi-stage recommendation engine (retrieve and rerank) to ground recommendations in item corpora, and databases (e.g., vector databases, user profile modules) to handle cold-start items or store/retrieve user-specific facts.","Recommendations grounded in specific item corpora, ability to handle private/domain-specific knowledge, alleviation of cold-start and temporal generalization problems, more factual and personalized responses, and improved understanding of user intent.","['LLM-based Tool Learning', 'LLMs as Conversational Agent', 'Personalization Pattern']","Music recommendation, content recommendation, and general personalized recommendations in conversational settings.","['Tools Integration Patterns', 'Personalization Pattern', 'LLM-specific Patterns']",13,
8,LLMs as Personalized Content Creator,"Traditional recommender systems primarily suggest existing items. There is a growing need to generate appealing, customized digital content that directly matches individual user interests, especially in areas like online advertising, where existing methods often use predefined templates or struggle with sparse feedback.","Online advertising, e-commerce, customer service, and other scenarios where personalized, engaging digital content (text, images, multi-modal) needs to be generated to enhance user experience and drive business growth.","Integrate AI-Generated Content (AIGC) powered by LLMs into recommender systems. LLMs provide enhanced reasoning for user personalized intent and interest, even with few-shot prompting. Reinforcement Learning from Human Feedback (RLHF) can be applied to fine-tune models to better capture user intent. LLMs' powerful generative abilities and cross-modal knowledge bases enable realistic creation of text, images, and other multi-modal content. Interactive generation processes with feedback loops allow for better capture of explicit user preferences.","More appealing, customized, and realistic content generated for users; improved understanding of explicit user preferences; enhanced user experiences; and increased business growth through personalized content.","['Generative AI Patterns', 'Personalization Pattern', 'AI–Human Interaction Patterns']","Text ad generation (ad titles, descriptions), image generation, chatbots for personalized product recommendations, automated customer service responses, and FAQs.","['Generative AI Patterns', 'Personalization Pattern', 'LLM-specific Patterns', 'AI–Human Interaction Patterns']",13,
9,Agent-Computer Interface (ACI),"Language Model (LM) agents struggle to interact effectively with complex digital environments (e.g., Linux shell, human-designed IDEs) due to mismatched abilities, verbose feedback, and unoptimized action spaces, leading to poor performance and error propagation.","Designing or deploying LM agents to automate complex tasks in digital environments (like software engineering) that were originally designed for human users or have granular, low-level interfaces. LMs have limitations such as lacking visual understanding and sensitivity to context window size.","Introduce an abstraction layer, the Agent-Computer Interface (ACI), between the LM agent and the computer environment. This interface provides a curated set of high-level, LM-friendly commands, structured and concise environment feedback, and built-in guardrails tailored to the LM's strengths and weaknesses. It also manages interaction history and formats input for the LM.","Substantially improved LM agent performance, enhanced ability to interact with the environment (e.g., file creation, editing, navigation, test execution), and increased task success rates compared to using raw, human-centric interfaces. Achieved state-of-the-art performance on benchmarks like SWEbench.","['Agentic AI Patterns', 'LLM-specific Patterns', 'Tools Integration Patterns', 'AI–Human Interaction Patterns']","['Automating software engineering tasks (SWEagent)', 'General computer control', 'Web navigation']",,7,
10,Simple and Understandable Actions,"Language Model (LM) agents struggle to reliably use complex commands with many options and verbose documentation (e.g., many bash commands), leading to misinterpretations, incorrect usage, and an increased need for demonstrations or fine-tuning.","Designing the action space for LM agents interacting with a digital environment, especially when the underlying system offers highly configurable but complex commands.","Design commands with a small number of clear, intuitive options and concise, easy-to-understand documentation. Prioritize clarity over extensive configurability for LM agents to reduce their cognitive load.","Reduces the cognitive load on the LM, decreases the likelihood of command misusage, and improves the agent's ability to effectively interact with the environment without extensive prior examples or fine-tuning.","['Agentic AI Patterns', 'Prompt Design Patterns', 'LLM-specific Patterns']","['Defining commands for file system navigation (e.g., `findfile`, `searchfile`, `searchdir`)', 'File editing (e.g., `edit`)', 'General environment interaction in agent-computer interfaces']",,7,
11,Compact and Efficient Actions,"Language Model (LM) agents can get stuck in inefficient loops or exhaust their context window and computational budget when higher-order operations require composing many simple, granular actions across multiple turns, hindering meaningful progress towards a goal.","Designing the action space for LM agents in an interactive environment where tasks involve multi-step operations (e.g., multi-line editing, complex search workflows).","Consolidate important, frequently composed operations into as few actions as possible. Design actions that enable the agent to make meaningful progress towards a goal in a single step, rather than requiring multiple turns for a basic effect.","Improves agent efficiency, reduces the number of interaction turns, conserves context window space, and accelerates task completion.","['Agentic AI Patterns', 'LLM-specific Patterns', 'Context Window Optimization']","['File editing (e.g., `edit` command replacing a range of lines in one step)', 'Search and navigation (e.g., summarized search results instead of iterative, one-by-one results)']",,7,
12,Informative but Concise Environment Feedback,"Language Model (LM) agents can be overwhelmed by verbose or irrelevant environment feedback, which consumes valuable context window space and computational resources, and can distract from critical information, leading to reduced performance. Conversely, too little feedback makes it difficult to understand the current state or the effect of actions.",Providing feedback to LM agents after their actions in an interactive digital environment.,"Design environment feedback to be substantive, providing crucial information about the current state and the immediate effects of the agent's actions, while strictly avoiding unnecessary details or verbose output. This includes displaying updated content after an edit, contextualizing file views with line numbers, and providing specific messages for silent command successes.","Enhances the agent's understanding of the environment, helps it focus on relevant information, reduces context window bloat, and improves decision-making without incurring excessive computational costs.","['LLM-specific Patterns', 'Prompt Design Patterns', 'Context Window Optimization']","['File viewer output (windowed view with line numbers and context indicators)', 'Feedback after file edits (showing updated content)', 'Search results (summarized results)', ""Success messages for silent commands (e.g., 'Your command ran successfully and did not produce any output')""]",,7,
13,Guardrails for Error Mitigation and Recovery,"Language Model (LM) agents frequently make mistakes (e.g., syntax errors in code edits, invalid commands, overly broad search queries) and can struggle to recognize these errors, recover from them, or prevent cascading failures, significantly hindering task performance.",Designing interactive systems for LM agents where agent actions can introduce errors or lead to unproductive states.,"Implement automated checks and interventions (guardrails) that detect common agent mistakes, prevent the propagation of errors, and provide clear, actionable feedback to guide recovery. Examples include code syntax checkers that block invalid edits, mechanisms to discard malformed commands, and suggestions to refine overly broad search queries.","Reduces the frequency and impact of agent errors, improves the agent's ability to self-correct, shortens recovery times, and enhances overall task reliability and success rates.","['Agentic AI Patterns', 'LLM-specific Patterns', 'Prompt Design Patterns']","['Integrated code linter for file editing (discards edits with syntax errors, shows error message and diff)', 'Search command limits (suggests refining queries if too many results)', 'Error responses for malformed agent generations']",,7,
14,Context Window Optimization (for History and Viewing),"Language Models (LMs) have finite context windows, and providing a full, unmanaged history of interactions or extensive file content can quickly exhaust this limit, leading to 'lost in the middle' phenomena, increased inference costs, and reduced performance.","Managing the information presented to an LM agent during a multi-turn interaction with a digital environment, particularly concerning long interaction histories and large files.","Implement strategies to keep the agent's context concise and relevant. This includes collapsing older observations into single-line summaries, limiting the window of visible lines when viewing large files, and selectively omitting redundant or less critical historical information while preserving essential plan and action history.","Maximizes the number of interaction turns possible within the LM's context window, reduces inference costs, improves the LM's ability to focus on recent and critical information, and mitigates performance degradation due to excessive context.","['LLM-specific Patterns', 'Prompt Design Patterns', 'Agentic AI Patterns']","['Collapsing observations preceding the last N turns in the history', 'Windowed file viewer (e.g., showing at most 100 lines of a file at a time)', 'History processors to manage message history length']",,7,
15,Specialized Search Interface,"Language Model (LM) agents struggle with traditional, verbose search tools (e.g., `grep`, `find` with many options) that can return overwhelming amounts of irrelevant information, exhaust context windows, or require inefficient iterative exploration to find relevant content.",Enabling LM agents to efficiently locate specific files or content within a large codebase or digital repository.,"Provide specialized, LM-friendly search commands that offer targeted search capabilities (e.g., `findfile` for names, `searchfile` for strings in files, `searchdir` for strings in directories). These commands should prioritize summarized, filtered results, and provide guidance for refining queries if initial searches yield too many results.","Improves the agent's efficiency in code localization, reduces the amount of irrelevant information presented, conserves context window space, and helps agents quickly zoom in on problematic areas.","['Agentic AI Patterns', 'Tools Integration Patterns', 'Informative but Concise Environment Feedback', 'Guardrails for Error Mitigation and Recovery']","['`findfile`, `searchfile`, `searchdir` commands in SWEagent, including features like limiting results (e.g., to 50 matches) and suggesting more specific queries.']",,7,
16,Efficient File Editing,"Language Model (LM) agents find file editing cumbersome and error-prone when relying on low-level, multi-step commands (e.g., `sed` for single-line changes, redirection for entire file replacement) that lack immediate feedback and require complex arithmetic or state tracking. This leads to inefficiency, cascading errors, and poor performance.","Enabling LM agents to modify code or text files within a digital environment, especially in software engineering tasks.","Provide a consolidated, high-level `edit` command that works in conjunction with a file viewer. This command should allow for replacing a specific range of lines with new text in a single action, automatically display the updated content, and ideally integrate guardrails like syntax checking.","Streamlines the editing process, reduces the number of actions required for multi-line changes, provides immediate and clear feedback on modifications, and mitigates syntax-related errors, significantly improving agent productivity and accuracy.","['Agentic AI Patterns', 'Tools Integration Patterns', 'Compact and Efficient Actions', 'Informative but Concise Environment Feedback', 'Guardrails for Error Mitigation and Recovery']","[""SWEagent's `edit` command, which takes start/end lines and replacement text, displays updated content, and integrates a code linter.""]",,7,
17,LLM-Assisted Symbolic World Model Construction,"Manually creating accurate and comprehensive symbolic world models (e.g., PDDL) is time-consuming and requires specialized expertise. Direct LLM planning is unreliable for correctness.",A reliable symbolic world model is needed for classical planning or plan validation. Users may lack expertise in formal symbolic languages like PDDL.,"Leverage Large Language Models (LLMs), such as GPT-4, to generate PDDL domain models from natural language descriptions of actions and domain context. This involves careful prompt engineering, iterating over actions, and maintaining a shared predicate list for consistency.","High-quality PDDL models are produced for complex domains, significantly reducing the burden on human experts and enabling the use of sound classical planners or robust plan validators.","['Natural Language to Formal Representation', 'Structured Prompt for Model Generation', 'Iterative Model Construction with Shared Predicate List', 'LLM for Knowledge Acquisition']","['Generating PDDL models for classical planners', 'Validating LLM-generated plans', 'Guiding skill learning for embodied agents']",,10,
18,LLM as a Feedback Interface for Symbolic Models,Correcting errors in symbolic models (like PDDL) requires expertise in the formal language. Technical feedback from validators or humans may be difficult for non-experts to interpret and apply.,"An LLM-generated symbolic model may contain errors (syntax, factual) that need correction. End-users providing feedback may not understand the symbolic language.","Use an LLM as an intermediary layer. It translates the symbolic model (or technical validator feedback) into natural language for human understanding and then incorporates natural language corrective feedback back into the symbolic model, typically through a dialogue.","Enables non-experts to provide effective feedback, streamlines the correction process, and significantly improves the accuracy and functionality of the symbolic model while concealing its complexity.","['PDDL to Natural Language Translation', 'Dialogue-Based Correction', 'Early Feedback Loop for Model Correction', 'Self-Documenting Model']","['Correcting LLM-generated PDDL models', 'Making symbolic model inspection accessible to non-experts', 'Integrating PDDL validator feedback']",,47,
19,Classical Planning with LLM-Generated World Model,LLMs alone struggle with combinatorial search and ensuring correctness for complex planning tasks. Manually crafting accurate PDDL domain models for classical planners is laborious.,A reliable and correct plan is needed for an agent to achieve a goal in a complex domain. An LLM has been successfully used to construct a symbolic world model.,"The LLM first translates user instructions into PDDL goal specifications. This, along with the LLM-generated (and corrected) PDDL domain model, is then fed into a sound, domain-independent classical planner (e.g., Fast Downward) to find a plan.","Generates highly reliable, correct, and executable plans by leveraging the LLM's natural language understanding and model acquisition capabilities, combined with the classical planner's robust search and correctness guarantees.","['LLM-Assisted Symbolic World Model Construction', 'LLM for Goal Specification Translation', 'Tools Integration Patterns']","['Reliable task planning for AI agents', 'Achieving goals in complex environments', 'Integrating natural language commands with symbolic planning systems']",,10,
20,LLM Plan Refinement with Symbolic Validation,"LLM-generated plans often lack correctness, overlook preconditions, or exhibit physically implausible actions, making them unreliable for execution.","An LLM is used as a primary planner, but its output needs rigorous validation and iterative correction. A symbolic world model (PDDL) is available.","The LLM-generated symbolic world model (PDDL) is used as a 'symbolic simulator' or 'human proxy' to validate plans proposed by an LLM planner. A validation tool (e.g., VAL) checks for unmet preconditions or goal conditions. This validation feedback is translated into natural language by an LLM and provided back to the LLM planner for iterative refinement (reprompting).","Significantly improves the correctness and executability of LLM-generated plans compared to vanilla LLM planning, reducing reliance on costly physical simulators or extensive manual inspection.","['LLM as a Feedback Interface for Symbolic Models', 'Dialogue-Based Correction', 'Feedback-Driven Refinement (general concept)', 'Tools Integration Patterns']","['Improving the reliability of LLM planners', 'Reducing iteration cost in plan generation', 'Providing grounded feedback for LLMs to self-correct']",,10,
21,Iterative Model Construction with Shared Predicate List,"When constructing a comprehensive symbolic domain model (e.g., PDDL) across multiple actions, LLMs might generate inconsistent or redundant predicates, or miss implicit preconditions in initial passes.","Building a complex symbolic domain model from natural language, where consistency and completeness of predicates are crucial for the model's functionality.",Generate PDDL models for each action sequentially. Maintain an actively updated list of all newly defined predicates and their natural language descriptions. Provide this list to the LLM in subsequent prompts to encourage reuse of existing predicates and ensure consistency. A second pass with the full predicate list can further refine the model.,"Ensures predicate consistency, reduces redundancy, helps LLMs infer implicit preconditions, and improves the overall quality and completeness of the symbolic model.","['LLM-Assisted Symbolic World Model Construction', 'Structured Prompt for Model Generation', 'Self-Documenting Model']","['Building complex symbolic domain models systematically', 'Maintaining consistency in knowledge representation across actions', 'Improving model completeness by surfacing implicit dependencies']",,10,
22,Structured Prompt for Model Generation,"LLMs require specific and structured guidance to reliably produce accurate, consistent, and syntactically correct formal representations (like PDDL) from natural language inputs.","Using an LLM to generate formal, structured outputs such as PDDL action models, where the output format and content rules are strict.","Design a comprehensive prompt template that includes: a) detailed instructions for the task, b) one or two few-shot examples illustrating input and desired output formats, c) contextual information about the domain, d) a natural language description of the specific action, and e) a dynamically updated list of existing predicates.","Significantly improves the LLM's ability to generate high-quality, syntactically correct, and semantically consistent symbolic models, reducing errors and noise in the output.","['Few-Shot Prompting (implicit)', 'LLM-Assisted Symbolic World Model Construction', 'Iterative Model Construction with Shared Predicate List']","['Generating PDDL models or other formal specifications', 'Creating structured code or data from natural language instructions', 'Ensuring consistent output format from LLMs']",,10,
23,Natural Language to Formal Representation,"Bridging the semantic gap between human-understandable natural language descriptions and precise, machine-executable formal representations required by AI systems.","Humans provide high-level instructions, domain knowledge, or descriptions in natural language, but the AI system (e.g., a planner) requires a formal, symbolic representation for reasoning or execution.","Leverage LLMs' advanced natural language understanding capabilities to parse natural language descriptions (of actions, goals, environment states) and translate them into a specified formal language (e.g., PDDL preconditions, effects, goal specifications, predicate definitions).","Automates the creation of formal models and specifications, reduces the need for human experts in formal languages, and makes AI systems more accessible to end-users.","['LLM-Assisted Symbolic World Model Construction', 'LLM for Goal Specification Translation', 'LLM for State Grounding']","['Generating PDDL models from action descriptions', 'Defining planning goals from user instructions', 'Grounding initial states from environment descriptions', 'Acquiring knowledge in formal formats']",,10,
24,LLM for Knowledge Acquisition,"Expanding the set of actions or understanding implicit, commonsense constraints for an AI agent, especially when only minimal natural language descriptions of actions are available.",An AI agent needs to learn new skills or expand its operational capabilities. Domain engineers may provide only high-level or minimal descriptions of actions.,"Use an LLM, leveraging its extensive common-world knowledge encoded from its training data, to infer and propose detailed preconditions and effects for actions based on minimal natural language descriptions. This goes beyond mere translation to 'suggesting meaningful action models.'","Accelerates the process of knowledge acquisition for new skills, helping engineers set up training environments, define new agent capabilities, or generate initial hypotheses for action models.","['LLM-Assisted Symbolic World Model Construction', 'Natural Language to Formal Representation']","['Defining new skills for AI agents', 'Expanding agent capabilities with inferred knowledge', 'Generating initial action model hypotheses for domain engineers', 'Integrating commonsense knowledge into symbolic models']",,5,
25,Early Feedback Loop for Model Correction,"Correcting errors in plans generated by LLMs is inefficient and repetitive, as similar mistakes might recur across different planning instances. Traditional feedback is often collected online during plan execution.","LLMs are used in planning, but their direct plan generation is error-prone. The goal is to improve overall system reliability and reduce human effort in the long run.","Shift the feedback and correction process from individual plan execution to the underlying symbolic world model itself, at an earlier stage in the development lifecycle. Users correct the domain model (e.g., PDDL) once, at the beginning of the process, rather than repeatedly inspecting and correcting every generated plan.","Reduces human involvement and frustration, as corrections made to the foundational model propagate to all future plans, offering correctness guarantees from external planners and improving overall system efficiency.","['LLM as a Feedback Interface for Symbolic Models', 'Human-in-the-Loop Correction (implicit)']","['Improving efficiency and reliability of AI planning systems', 'Reducing operational costs by front-loading corrections', 'Ensuring foundational model correctness upfront']",,47,
26,Self-Documenting Model,"Symbolic models, such as PDDL, can be opaque and difficult for non-experts to understand, inspect, or debug, hindering collaboration and validation.","Generating complex symbolic representations that need to be understood and validated by human users, some of whom may not be experts in the formal language.","When defining new elements within the symbolic model (e.g., predicates, parameters, actions in PDDL), require the LLM to also provide a clear and concise natural language description of that element.","Makes the symbolic model transparent and understandable for non-PDDL experts, facilitating inspection, debugging, and collaboration, and aids in downstream tasks like automatic state grounding.","['PDDL to Natural Language Translation (implicit)', 'LLM-Assisted Symbolic World Model Construction']","['Improving interpretability of symbolic models', 'Facilitating human oversight and validation', 'Enhancing model maintainability and collaboration', 'Aiding automatic state grounding']",,10,
27,LLM for Goal Specification Translation,"End-users typically express their desired goals in natural language, but symbolic planners require precise, formal goal specifications (e.g., PDDL goals).",Integrating natural language user commands or high-level instructions with symbolic planning systems.,"Use an LLM to parse natural language user instructions and convert them into a structured, symbolic goal specification in the PDDL format, utilizing the predicates and objects defined in the domain model.","Enables natural language interaction with classical planning systems, making them more accessible and user-friendly, and automates a crucial step in setting up planning problems.","['Natural Language to Formal Representation', 'Classical Planning with LLM-Generated World Model']","['User interface for planning systems', 'Automating goal definition for symbolic planners', 'Bridging human intent to formal planning requirements']",,10,
28,LLM for State Grounding,"Initial states of an environment are often described in natural language or observed visually, but symbolic planners require these descriptions to be translated into grounded PDDL predicate values.","Preparing an initial state for a classical planner from human descriptions, environmental observations, or sensor data.",Leverage LLMs to translate natural language descriptions of the environment into grounded PDDL predicate values. This can also be combined with vision-language models to derive predicate values directly from visual observations through a question-answering approach.,"Automates the creation of accurate initial state descriptions for symbolic planners, bridging the gap between perception/description and formal symbolic representation.","['Natural Language to Formal Representation', 'Self-Documenting Model (predicate descriptions aid grounding)']","['Initializing planning problems for symbolic planners', 'Connecting perception to symbolic reasoning systems', 'Automating environment setup for simulations']",,10,
29,Dialogue-Based Correction,Correcting errors in LLM-generated outputs (such as PDDL models or plans) often requires an iterative process involving multiple rounds of feedback and refinement.,"An LLM has generated an initial (potentially erroneous) output, and feedback (from validators or humans) needs to be incorporated to improve its quality.","Integrate corrective feedback by 'replaying and continuing the original generation dialogue.' The LLM receives the feedback (often translated to natural language) and attempts to rectify its previous output within the context of the ongoing conversation, allowing for incremental improvements.","Facilitates iterative refinement of LLM outputs, enabling incremental improvements, addressing errors effectively, and making the correction process more interactive and user-friendly.","['LLM as a Feedback Interface for Symbolic Models', 'Early Feedback Loop for Model Correction', 'Prompt Design Patterns']","['Refining LLM-generated code, models, or text based on iterative feedback', 'Interactive debugging of LLM outputs', 'Collaborative improvement of AI-generated content']",,47,
30,LLM as Heuristic/Seed for Classical Planning,"While classical planners ensure correctness, they can be computationally expensive for large search spaces. LLMs can offer commonsense insights that might guide the search more efficiently.","Using a classical planner, but aiming to accelerate its search or guide it towards plans that align more with human intuition or preferences.","Utilize an LLM to generate a preliminary, high-level plan or to score potential actions. This LLM-generated information (e.g., a 'seed plan') is then provided to a local-search classical planner (e.g., LPG) to accelerate its search process.","Potentially speeds up classical planning by providing a good starting point or heuristic guidance, and can incorporate implicit human preferences or commonsense into the planning process.",['Classical Planning with LLM-Generated World Model'],"['Hybrid planning approaches combining LLM intuition with classical rigor', 'Guiding classical search algorithms', 'Incorporating commonsense into planning for more human-like solutions']",,17,
31,Zeroshot Prompting,How to enable foundation models to understand tool functionalities and usage with minimal or no examples.,Foundation models with strong few-shot and zero-shot learning capabilities. Tools are accompanied by manuals or descriptions.,"Construct prompts that describe API functionalities, their input/output formats, and possible parameters.",The model can understand the tasks each API can tackle without explicit demonstrations.,"['Fewshot Prompting', 'Tool Understanding']","['Teaching foundation models about tools', 'Rapid adaptation to new tools (e.g., weather API understanding)']",,18,
32,Fewshot Prompting,How to enable foundation models to learn to utilize tools effectively by observing examples.,Foundation models with strong few-shot learning capabilities. Availability of concrete tool-use demonstrations.,Provide concrete tool-use demonstrations within the prompt to the model.,The model learns how to utilize tools by mimicking human behaviors from these demonstrations.,"['Zeroshot Prompting', 'Tool Understanding', 'Learning from Demonstrations']","['Teaching foundation models about tools (e.g., weather API usage examples)']",,27,
33,Chain of Thought (CoT) Prompting,Foundation models struggle with problems requiring complex reasoning when using vanilla fewshot prompting.,Large foundation models (hundreds of billions of parameters) that can generate intermediate reasoning traces during complex problem-solving.,"Insert the reasoning trace required to derive the final answer for each example in the prompt, prompting the model to generate its thoughts on intermediate steps.","Significantly boosts performance on a wide range of tasks requiring complex reasoning (arithmetic, commonsense, symbolic reasoning), and enables the controller to effectively decompose complex problems and determine which tool to call.","['Introspective Reasoning', 'Extrospective Reasoning', 'Planning with Reasoning']","['Arithmetic reasoning', 'Commonsense reasoning', 'Symbolic reasoning', 'Task decomposition', 'Tool selection']",,26,
34,Instruction Tuning for Intent Understanding,How to enable foundation models to accurately understand diverse and potentially vague user instructions for tool-oriented tasks.,"Foundation models can acquire extraordinary proficiency in comprehending user instructions, especially after fine-tuning on diverse instructions.",Fine-tune large language models on a collection of datasets templated with human instructions.,Models generalize effectively to instructions for unseen tasks and provide more personalized responses with a better user experience.,['Personalized Tool Learning'],"['Improving intent understanding for tool selection and planning', 'Generalizing to unseen tasks']",,35,
35,Introspective Reasoning (Static Planning),"Generating a multi-step plan for tool use without immediate feedback from the environment, which can lead to unrealistic plans.","Tasks where a full plan can be generated upfront. Foundation models with planning capabilities, but without direct environment interaction during planning.","The controller directly generates a complete, multi-step plan for tool use without knowing intermediate execution results. This can involve generating code or sequential decisions.","Enables models to decompose high-level tasks into semantically plausible sub-plans and generate executable programs (e.g., Python code, robot policies).","['Chain of Thought Prompting', 'Programming Interface', 'Extrospective Reasoning']","['Generating Python code for reasoning steps (PAL)', 'Generating executable programs for embodied agents (ProgPrompt, Code-as-Policies)', 'Interleaving vision foundation models with ChatGPT (Visual ChatGPT)']",,8,
36,Extrospective Reasoning (Iterative Planning with Feedback),"Adapting a plan in response to intermediate execution results and unexpected situations in dynamic environments, which introspective reasoning cannot handle.","Complex tasks (e.g., multi-step QA, embodied learning) where decision-making at each step depends on the preceding context and environmental feedback.","The controller generates plans incrementally, often one step at a time, with subsequent plans dependent on previous execution results and feedback from the environment and user. This creates a closed-loop interaction.","More rational and feasible plans, improved accuracy on multi-step tasks, and enhanced planning capabilities for embodied agents by handling exceptions and adapting to the current situation.","['Chain of Thought Prompting', 'Environment Feedback', 'Reinforcement Learning for Tool Learning', 'Introspective Reasoning']","['Multi-step QA (SelfAsk, ReAct, ToolFormer)', 'Embodied learning (Inner Monologue, LLMPlanner)', 'Determining when to cease generating action tokens during planning (ReAct)']",,8,
37,Multi-agent Collaboration,"Complex tasks often demand collaboration among multiple agents, each possessing unique abilities and expertise, which a single agent may struggle to complete.",Scenarios where task decomposition allows for parallel or specialized sub-tasks that can be handled by different agents. Foundation models capable of simulating human behaviors and communication.,"Design methods for communication, coordination, and negotiation among multiple AI agents, each potentially modeled with a foundation model, to work together to achieve complex tasks.","More effective and efficient problem-solving for complex tasks, unlocking capabilities beyond single-agent systems.","['Extrospective Reasoning', 'Parallel Tool Execution']","['Simulating human behaviors (e.g., interpersonal communication in interactive scenarios)', 'Orchestrating complex workflows']",,11,
38,Proactive Systems,"Most foundation models are designed as reactive systems, only responding to user queries without initiating any actions on their own, leading to a less seamless and personalized user experience.",Systems that can leverage user interaction history and anticipate user needs to improve user experience.,"Design AI systems that can take action on behalf of the user, continually improving performance and tailoring responses based on learned preferences and historical interactions.","A more personalized and seamless user experience, with the system anticipating needs and initiating helpful actions. Requires careful design for safety and ethical implications.","['Personalized Tool Learning', 'Human Feedback']","['Personalized assistance', 'Context-aware recommendations']",,35,
39,Semantic Interface,"Facilitating a consistent and natural way for models to trigger tool actions using natural language, but mapping needs to be predefined and models may fail to produce precise forms.",Foundation models excel at generating and understanding natural language. The desire for intuitive and natural interaction with tools.,"Utilize a specific text span (e.g., action name) as the action trigger, mapping generated natural language directly to specific tool actions.","An intuitive and natural way for models to interact with tools, leveraging their language generation capabilities.","['Programming Interface', 'GUI Interface']","[""Robotic manipulation (e.g., 'pick up the sponge')"", ""Search engine interaction (e.g., 'Action: Search' for ReAct)""]",,35,
40,GUI Interface,"Enabling AI models to interact with digital environments in a human-like manner through graphical user interfaces, which restrict models to predefined options.","Humans primarily interact with the digital world via GUIs (mouse, keyboard). Models need to map predicted tokens to these physical interactions.",Establish a virtual environment that facilitates mapping predicted tokens to human-like mouse movements and keyboard inputs. Leverage foundation models to introduce prior knowledge about common GUI actions.,"Models can perform web-based tasks (e.g., using keyboard and mouse actions), browse and purchase products, expanding potential actions beyond predefined options.","['Semantic Interface', 'Programming Interface']","['Web-based agents to complete tasks (e.g., WebShop)']",,35,
41,Programming Interface,"Enabling models to specify actions with a high degree of flexibility and control, going beyond pure natural language, and modeling complex logic.","Complex tool learning logic, need for precise control, explicit calls of external APIs. Code-generating language models (CLMs) are available.","Allow the model to specify its actions using a program, requiring it to be acquainted with the syntax of function calls. CLMs serve as the backbone for generating this code.","Models can leverage code grammar to execute complex actions, generalize to novel instructions, and provide precise control with accurate parameter values to functions. Enables modeling complex control flows and explicit API calls.","['Introspective Reasoning (PAL, Code-as-Policies)', 'Semantic Interface', 'GUI Interface']","['Robotic control (Code as Policies)', 'Mathematical problem solving', 'Interacting with databases (SQL queries)', 'Software engineering tasks']",,49,
42,Formalism Integration for Reasoning,"Enhancing agents' performance in complex reasoning tasks beyond plain natural text, which may be insufficient for certain domains.","LLM-based agents inherently comprehend and generate language. External formalisms (mathematical tools, non-natural language forms) can provide structured reasoning capabilities.",Incorporate external formalisms such as probabilistic graph models (PGM) or integrate intelligent agents into conventional Robotic Process Automation (RPA) like Agentic Process Automation (APA).,Significantly enhances agents' decision-making capabilities and intelligence in complex reasoning tasks while remaining controllable.,"['Knowledge Augmentation', 'Extrospective Reasoning']","['Multi-agent reasoning (PGM integration)', 'Agentic Process Automation (APA)']",,11,
43,Retrieval Augmented Generation (RAG),"Foundation models suffer from limitations in memorization, real-time knowledge coverage, and can hallucinate knowledge, leading to inaccurate or outdated generations.","Foundation models need to incorporate domain-specific or up-to-date knowledge. External knowledge sources (local repositories, web) are available.","Augment language generation by retrieving knowledge from external sources (e.g., text retrievers, search engines) and integrating it into the model's generation process.","Mitigates memorization limitations, provides real-time and up-to-date knowledge, enhances factual accuracy, and reduces hallucination.","['Knowledge Augmentation', 'Conflict Detection & Resolution']","['Augmenting language generation', 'Open-domain question answering', 'Information retrieval (WebGPT)']",,3,
44,Supervised Learning (Behavior Cloning for Tool Use),Training foundation models to mimic expert behavior in tool-oriented tasks when expert demonstrations are available.,Availability of datasets consisting of user queries paired with human demonstration annotations for tool use. Task-general inductive bias of foundation models.,Fine-tune foundation models (as policy networks) in a supervised fashion to imitate human experts' actions given certain inputs or conditions.,"Significantly improves both in-domain performance and out-of-distribution generalization for tool use, enabling models to manipulate tools like search engines or interactive web environments.","['Learning from Demonstrations', 'Semisupervised Learning', 'Self-supervised Learning']","['Autonomous vehicles and robotic applications', 'Finetuning GPT-3 to clone human web search behaviors (WebGPT)', 'Training agents for web-based interactive environments (WebShop)']",,27,
45,Semisupervised Learning (Pseudo-labeling for Tool Use),Reducing the heavy requirement for human behavior annotation in tool learning when large-scale unlabeled data is available but seed labeled data is limited.,"Human behavior for tool use is costly to record, but unlabeled data is abundant. A small amount of seed labeled data is available.","Train a less capable model on a small amount of seed labeled data to predict pseudo-labels of actions on unlabeled data, then use these pseudo-labels to train a more powerful model.",Enables training powerful tool-using models without requiring extensive human rollout or large-scale gold-standard human behavior annotation.,"['Supervised Learning (Behavior Cloning)', 'Self-supervised Learning']",['Training models for Minecraft video game actions'],,27,
46,Self-supervised Learning (Bootstrapping Tool Use Examples),"Reducing the reliance on human or pseudo-labeled annotations for tool learning, especially for generating diverse tool-use examples.","Foundation models with in-context learning abilities, and a few human-written examples for tool use.",Leverage the in-context learning ability of foundation models to iteratively bootstrap tool-use examples based on a handful of human-written examples. These auto-generated examples are then filtered to reduce noise.,"Creates a sufficient dataset of tool-use supervisions, significantly improving tool-use performance with minimal human effort.","['Supervised Learning (Behavior Cloning)', 'Semisupervised Learning', 'Fewshot Prompting']",['Enhancing tool-use capabilities (Toolformer)'],,27,
47,Reinforcement Learning for Tool Learning,"Enabling artificial agents to learn from trial and error and adapt their tool-use behaviors in complex, dynamic environments to maximize rewards.","Tool learning scenarios where the action space is defined by tools, and agents need to optimize their policy based on consequences. Foundation models can serve as policy initialization.","Frame tool learning as an RL scenario, where the agent learns to select appropriate tools and perform correct actions that maximize a reward signal, often initialized by a foundation model.","Agents learn to reflect on the current state, select tools, and perform actions leading to the highest expected reward, adapting to changes and optimizing decisions based on dynamic feedback.","['Environment Feedback', 'Human Feedback', 'Extrospective Reasoning', 'Reinforcement Learning from Human Feedback (RLHF)']","['Robotic grasping', 'Multi-agent auto-curricula', 'Enhancing LLM tool-using capabilities (ETO)']",,48,
48,Environment Feedback,Providing agents with information about the consequences of their tool actions to allow for policy updates and behavior adaptation.,Interactive environments where tool actions trigger observable changes. Feedback can be ultimate (result) or intermediate (state changes).,"The controller interacts with the environment and receives feedback, which can be 1) result feedback (indicating task completion success/failure) or 2) intermediate feedback (state changes triggered by an action).","Models can iteratively update their planning strategy, adjust decision-making, and learn whether each action is effective and appropriate, improving tool-use behavior.","['Extrospective Reasoning', 'Reinforcement Learning for Tool Learning']","['WebShop (reward based on product similarity)', 'Search engine interaction (observing rendered information)', 'Embodied learning (current scene information)']",,48,
49,Human Feedback,Aligning model behavior with human preferences and values in tool-use scenarios and regulating its behavior.,Human judgment is crucial for assessing the quality and appropriateness of model-generated plans and actions. Feedback can be explicit or implicit.,"Humans provide the model with rewards and penalties based on its generated plans, which can be explicit (direct ratings on a scale) or implicit (derived from user behavior like comparisons, response time, or actions taken).","Regulates model behavior, aligns tool-use with human preferences, and helps in manipulating tools more effectively to answer questions or complete tasks.","['Reinforcement Learning from Human Feedback (RLHF)', 'Proactive Systems']","['Guiding policy models to align with human preferences in search engines (WebGPT)', 'Text summarization']",,48,
50,Reinforcement Learning from Human Feedback (RLHF),"The accuracy and stability of human feedback are valuable, but it is label-intensive and has high latency for direct use in training.",Availability of human feedback (explicit or implicit) on model-generated outputs or actions.,"Fine-tune a separate model to imitate humans in giving rewards, and then use these learned rewards to optimize the main policy model with RL algorithms (e.g., PPO).","Yields exceptional performance in aligning models with human preferences, improving tool-use capabilities even after supervised training, by leveraging human judgment more efficiently.","['Human Feedback', 'Reinforcement Learning for Tool Learning']","['Text summarization', 'Improving search engine manipulation (WebGPT)', 'Enhancing LLM tool-using proficiency (e.g., DPO algorithm)']",,48,
51,Meta Tool Learning,"Enabling models to generalize tool-use knowledge and strategies to new, unfamiliar tasks or domains efficiently.",Models need to identify common underlying principles or patterns in tool-use strategies across different tools or domains.,"Train the model to not only use a tool but also to learn the optimal strategy for its use, allowing it to identify common patterns in tool-use strategies.","Models can transfer their knowledge of tool-use strategies to new tools or domains (e.g., from one search engine to another, or a calculator for different mathematical problems), leading to more adaptable and intelligent ML models.","['Curriculum Tool Learning', 'Generalizable Tool Learning']","['Transferring knowledge between similar search engines (e.g., Bing to Google Search)', 'Generalizing calculator use to solve different types of mathematical problems']",,35,
52,Curriculum Tool Learning,Effectively teaching models to use complex tools by gradually building knowledge and skills in a manageable way.,"Tools with varying levels of complexity, where starting with simple functionalities can ease the learning curve.","A pedagogical strategy that starts with simple tools or basic functionalities of a tool and gradually introduces the model to more complex tools or advanced concepts, allowing it to build upon prior knowledge.","Ensures the model masters essential features before moving to complex concepts, making learning manageable and effective. Improves generalization by enabling the model to identify similarities/differences and adapt its approach.","['Meta Tool Learning', 'Generalizable Tool Learning']","['Teaching models to use mathematical software (e.g., Mathematica, starting with addition/subtraction then calculus)', 'Learning complex algorithms or operations']",,35,
53,Personalized Tool Learning,"Foundation models, typically trained on generic domains, struggle to provide tailored assistance to users with varying needs and preferences for tool learning.","Users have diverse preferences for tool planning, selection, and interaction. Personal information can be heterogeneous.","Integrate user-specific information (e.g., language style, social networks, preferences for platforms) into general-purpose tool learning models. This involves: 1) modeling heterogeneous user information, 2) personalized tool planning, and 3) personalized tool calls.","Models provide tailored assistance, adapt to diverse expressions of intent, and generate personalized tool execution plans and inputs, leading to a more personalized user experience.","['Proactive Systems', 'Human Feedback', 'Instruction Tuning for Intent Understanding']","['Personalized email tool usage', 'Customized online shopping experiences']",,35,
54,Conflict Detection & Resolution,"Discrepancies and inconsistencies (knowledge conflicts) arise between model knowledge (memorized from training data) and augmented knowledge (from tool execution), or among knowledge from different tools.","Foundation models augmented by various knowledge sources, where some knowledge might be outdated, false, or biased. Need for accuracy and reliability in model generation.","Implement mechanisms for: 1) Conflict Detection: Identify potential conflicts among different knowledge sources and flag them. 2) Conflict Resolution: Verify reliability, choose trustworthy sources, and provide explanations for the model's final generation by interpreting which knowledge source was considered.","Improves the accuracy and reliability of model generation and planning, especially in high-stakes domains, by ensuring models can distinguish and verify knowledge sources.","['Retrieval Augmented Generation (RAG)', 'Knowledge Augmentation']","['Medical assistance', 'Legal advice', 'Question answering', 'Correcting model beliefs with retrieved information', 'Discerning knowledge conflicts from different sources']",,40,
55,Tools by AI (AI-Generated Tools),"The traditional limitation of humans being the sole creators of tools, hindering the potential for AI to develop solutions specifically optimized for its own use or to encapsulate existing functionalities.",Large code models capable of generating executable programs based on language descriptions. Foundation models can understand and extend existing API functionalities.,"Leverage foundation models, particularly large code models, to autonomously generate executable programs or encapsulate existing APIs into more advanced and tailored functions.","AI systems can transition from merely tool users to tool makers, creating tools optimized for their own processes, extending existing functionalities, and developing sophisticated solutions autonomously.","['Programming Interface', 'Tools for AI']","['Generating executable programs from language descriptions (large code models)', 'Encapsulating existing APIs into more advanced functions (e.g., extending weather forecast API, integrating stock market data, automated medical diagnosis systems)']",,21,
56,Tools for AI (Model-Optimized Tools),"Existing tools are primarily designed for human preference and convenience, not necessarily optimal for how AI models process information, leading to suboptimal interaction and utilization.","AI models process information differently from humans, and current tools may not be modular or have suitable input/output formats for AI.","Create tools specifically suited for models by: 1) Modularity: Decomposing tools into smaller, more modular units. 2) New Input/Output Formats: Developing formats tailored to the needs of AI models.","Improves interaction and utilization of tools by AI models, enabling more seamless integration and communication between models and tools, and allowing models to use components in a fine-grained and compositional manner.",['Tools by AI'],"['Designing tools with AI consumption in mind', 'Developing new input/output formats for AI-tool interaction']",,21,
57,Interpretability through Tool Execution,"Foundation models are often criticized for lacking transparency (being 'black boxes') in their decision-making processes, which is a significant concern in critical applications.",Applications like healthcare or finance where understanding the rationale behind AI decisions is critical for making informed decisions and building trust.,Design the AI system such that the process of tool execution reflects the steps taken by the model to solve complex requests.,"Allows for better interpretability and transparency. Users can easily understand why certain tools were called and how they contribute to the final output, improving trust and facilitating human-machine collaboration.","['Extrospective Reasoning', 'Chain of Thought Prompting']","['Healthcare', 'Finance', 'Improving human-machine collaboration']",,21,
58,Robustness through Tools,Foundation models are susceptible to adversarial attacks (slight input modifications flipping predictions) due to their reliance on statistical patterns.,Applications requiring high reliability and resistance to malicious inputs.,"Augment foundation models with specialized tools designed for specific use cases, which may be agnostic to input perturbation.","Tools make the overall AI system more resistant to adversarial attacks, enhancing its reliability and stability in real-world deployments.",[],"['Enhancing security in AI applications', 'Defending against adversarial attacks']",,0,
59,Democratizing Tool Access,"Complex tools often have steep learning curves and require specialized technical expertise, limiting their accessibility to non-technical users.",The powerful intent understanding capabilities of foundation models.,"Leverage foundation models to simplify intricate tasks into a natural language format, allowing users to provide high-level guidance and direction. The model then comprehends the intent and manipulates complex tools.","Lowers the barrier to entry for new users, enabling even novice users to easily and quickly get started with new tools, regardless of their prior experience or technical expertise, unlocking innovation and creativity.","['Instruction Tuning for Intent Understanding', 'Personalized Tool Learning']",['Making complex tools accessible to individuals without specialized technical knowledge'],,35,
60,Task Decomposition,"Real-life tasks are usually complicated and multistep, bringing severe hardness for LLM planning. A one-step planning process is formidable, and LLMs may suffer from task forgetting and hallucinations for complex tasks.","LLM-based autonomous agents need to accomplish complex, multistep tasks in environments characterized by complexity and variability.","Adopt a 'divide and conquer' strategy by decomposing a complicated task into several simpler subtasks and then sequentially planning for each subtask. This can be implemented in two manners:
1.  **Decomposition-First:** Decompose the entire task into subgoals first, then plan for each subgoal successively.
2.  **Interleaved Decomposition:** Dynamically adjust task decomposition and subtask planning based on environmental feedback, revealing only a few subtasks at the current state.","Simplifies complex tasks, making them manageable for LLMs. Decomposition-first methods create a stronger correlation between subtasks and original tasks, reducing the risk of task forgetting and hallucinations. Interleaved decomposition improves fault tolerance by dynamically adjusting based on feedback.","['Agentic AI Patterns', 'Planning Patterns', 'Prompt Design Patterns']","['Chain-of-Thought (CoT)', 'ReAct', 'HuggingGPT', 'PlanandSolve', 'ProgPrompt', 'PAL (Program-Aided Language models)', 'Program-of-Thought (PoT)', 'Visual ChatGPT']",,6,
61,Multiplan Selection,"Due to task complexity and the inherent uncertainty of LLMs, a single plan generated by an LLM is likely to be suboptimal or even infeasible.",LLM agents need to find robust and optimal plans for complex tasks where initial LLM outputs might be unreliable or require validation.,"Generate various alternative plans for a given task (Multiplan Generation) and then employ a task-related search algorithm to select the optimal plan to execute (Optimal Plan Selection).
1.  **Multiplan Generation:** Achieved by employing uncertainty in the decoding process of generative models (e.g., temperature sampling, top-k sampling) or by explicitly instructing the LLM to generate multiple plans via few-shot examples in prompts.
2.  **Optimal Plan Selection:** Utilizes diverse strategies adopted as heuristic search algorithms, such as majority vote, conventional BFS/DFS, Monte Carlo Tree Search (MCTS), or the A* algorithm.","Provides a broader exploration of potential solutions in the expansive search space, leading to more robust and potentially optimal plans by mitigating the unreliability of a single LLM output.","['Agentic AI Patterns', 'Planning Patterns', 'Generative AI Patterns', 'LLM-specific Patterns']","['Self-consistency', 'Tree-of-Thought (ToT)', 'Graph-of-Thought (GoT)', 'LLMMCTS', 'RAP', 'LLM-A*']",,17,
62,External Planner-Aided Planning,"LLMs face challenges when confronted with environments featuring intricate constraints (e.g., mathematical problem-solving, generating admissible actions) and may struggle with the efficiency and feasibility of generated plans.","LLM agents need to perform planning in environments with strict rules, complex logical dependencies, or where computational efficiency and guaranteed plan feasibility are critical.","Integrate LLMs with specialized external planners. The LLM primarily plays a supportive role, parsing textual feedback, formalizing tasks (e.g., into PDDL or ASP), and providing additional reasoning information, while the external planner handles the precise, constrained planning.
1.  **Symbolic Planner:** LLM converts natural language problems into well-established symbolic formalized models (e.g., PDDL, atomic facts for ASP), which are then solved by classical symbolic planners (e.g., FastDownward, BFS solver, CLINGO, LPG).
2.  **Neural Planner:** LLM combines with deep models trained on planning data (e.g., RL-based policy networks, Decision Transformers). LLM can generate candidate actions or handle complex 'slow thinking,' while the neural planner performs rapid, domain-specific 'fast thinking.'","Elevates planning proficiency, addresses issues of efficiency and infeasibility of generated plans, and enables LLMs to deal with more general tasks for symbolic AI. It combines LLM's semantic understanding and code generation capabilities with the theoretical completeness, stability, and interpretability of symbolic systems or the efficiency of neural planners.","['Tools Integration Patterns', 'Planning Patterns', 'Knowledge & Reasoning Patterns', 'Agentic AI Patterns', 'Classical AI']","['LLMP', 'LLMDP', 'LLMPDDL', 'LLMASP']",,10,
63,Reflection and Refinement,"LLM agents may suffer from hallucinations, make errors, or get stuck in thought loops during planning due to insufficient reasoning abilities for complex problems and limited feedback.","LLM agents need to improve their performance over time, learn from mistakes, and self-correct to enhance fault tolerance and achieve specified goals.","Implement an iterative process where the LLM reflects on its past actions and failures, summarizes feedback, and then refines its plan. This involves generating an initial plan, receiving feedback (internal or external, potentially from external tools like Knowledge Bases or Search Engines), evaluating trajectories, generating self-reflections upon error detection, and using these textual feedbacks to adjust subsequent planning outputs.","Enhances the fault tolerance and error correction capabilities of LLM agent planning. It helps agents correct errors, break out of thought loops, and improves performance on complex tasks by leveraging self-reflection.","['Agentic AI Patterns', 'Planning Patterns', 'Prompt Design Patterns', 'Knowledge & Reasoning Patterns']","['SelfRefine', 'Reflexion', 'CRITIC', 'InteRecAgent', 'LEMA']",,23,
64,Memory-Augmented Planning,"LLMs have limited context length, leading to 'forgetting' valuable information (commonsense knowledge, past experiences, domain-specific knowledge) crucial for long-term or complex planning, potentially causing hallucinations or suboptimal decisions.","LLM agents need to retain and leverage a broad range of information over time to enhance planning capabilities, enable growth, and improve fault tolerance.","Integrate an additional memory module to store valuable information, which is then retrieved and used as auxiliary signals during planning. This can be achieved through two major approaches:
1.  **RAG-based Memory (Retrieval Augmented Generation):** Stores memories (e.g., texts, tabular forms, knowledge graphs) in external storage. Task-relevant experiences are retrieved based on similarity (e.g., using vector embeddings and indexing structures like FAISS) and provided to the LLM as additional context.
2.  **Embodied Memory (Finetuning-based):** Embeds memories directly into the LLM's parameters by finetuning the model with the agent's historical experiential samples (e.g., ground truth action trajectories, Markov decision process data). Parameter-Efficient Finetuning (PEFT) techniques can be leveraged to reduce costs.","Provides LLM agents with long-term memory, allowing them to leverage past experiences, commonsense knowledge, and domain-specific priors. Enhances planning capabilities, growth, and fault tolerance by reducing task forgetting and improving decision-making.","['Knowledge & Reasoning Patterns', 'LLM-specific Patterns', 'Agentic AI Patterns', 'Generative AI Patterns', 'Personalization Pattern']","['Generative Agents', 'MemoryBank', 'TiM', 'RecMind', 'MemGPT', 'REMEMBER']",,29,
65,LLM as Planner,"Traditional embodied agents require extensive labeled data (language instructions and gold trajectories) for each task, hindering versatility and quick learning of new tasks.","Designing embodied agents that can follow natural language instructions to complete complex tasks in visually-perceived, often partially-observable, environments. The goal is to reduce data cost and improve sample efficiency.","Leverage Large Language Models (LLMs) to directly generate plans (e.g., high-level plans or sequences of subgoals) for the agent's actions, rather than requiring the agent to learn plans from scratch through many demonstrations. This utilizes the LLM's pre-trained knowledge and reasoning capabilities.","Enables few-shot planning, significantly reducing the amount of paired training data needed. Agents can quickly learn new tasks and achieve competitive performance with less data compared to methods requiring full training datasets. Reduces the need for extensive environment-specific knowledge a priori.","['Few-shot Learning (with LLMs)', 'In-Context Learning', 'Hierarchical Planning', 'Grounded Planning / Environmental Grounding', 'Dynamic Replanning', 'Prompt Engineering', 'LLM as Skill Ranker / Admissible Action Filter (contrasted)']","['Generating high-level plans for embodied agents', 'Decision-making in complex environments', 'Instruction following for robots']","['LLM-specific Patterns', 'Planning Patterns', 'Agentic AI Patterns']",8,
66,Few-shot Learning (with LLMs),"Training AI models for new tasks typically requires a large amount of labeled data, which is costly and time-consuming to acquire, especially for diverse or complex tasks.","Developing AI systems, particularly those using Large Language Models (LLMs), to perform new tasks efficiently with minimal task-specific labeled examples. This is crucial for achieving versatility and rapid adaptation in embodied agents.","Adapt LLMs to new tasks by providing only a small number of demonstration examples (paired inputs and desired outputs) within the prompt, without requiring any parameter updates to the underlying LLM. This leverages the LLM's vast pre-trained knowledge to generalize from limited examples.",Dramatically reduces data annotation costs and development time. Enables models to learn and perform new tasks quickly and achieve competitive performance even with a fraction of the data typically required.,"['In-Context Learning', 'Prompt Engineering', 'Dynamic In-Context Example Retrieval', 'LLM as Planner']","['Rapid adaptation of LLMs to new tasks', 'Reducing data requirements for embodied agents', 'Quick prototyping of AI functionalities', 'Building versatile agents']","['LLM-specific Patterns', 'Classical AI']",18,
67,In-Context Learning,"Adapting a pre-trained Large Language Model (LLM) to a specific downstream task without costly fine-tuning or parameter updates, especially when only a few examples are available.","Utilizing the capabilities of large, pre-trained language models for various tasks by providing task-specific instructions and demonstrations directly within the input prompt. This method is used when parameter updates are undesirable or infeasible.","Formulate the task as a sequence of input-output examples or instructions, which are then concatenated with the actual test input and fed to the LLM. The LLM then generates the output based on the provided context, inferring the task from the examples.","Enables zero-shot or few-shot adaptation of LLMs to new tasks without model retraining, saving significant computational resources and development time. Allows for flexible task definition and rapid experimentation.","['Few-shot Learning (with LLMs)', 'Prompt Engineering', 'Dynamic In-Context Example Retrieval', 'LLM as Planner']","['Adapting LLMs for classification, generation, planning, summarization, and other NLP tasks', 'Enabling few-shot learning for embodied agents']","['LLM-specific Patterns', 'Prompt Design Patterns']",25,
68,Hierarchical Planning,"Complex, long-horizon tasks for embodied agents are difficult to plan directly with a single-level planner, leading to inefficiency, unmanageable state spaces, and difficulty in reasoning about high-level goals.","Designing planning systems for embodied agents (e.g., robots) that need to accomplish complex tasks involving multiple steps and interactions in dynamic environments.","Decompose the overall task into a hierarchy of plans. A high-level planner generates a sequence of abstract subgoals (e.g., 'Navigate to fridge', 'Pickup potato'). A separate low-level planner then translates each subgoal into a sequence of primitive actions executable by the agent (e.g., 'move forward 0.1m', 'turn left 10 degrees', 'grasp').","Simplifies complex planning problems by breaking them into manageable sub-problems. Improves efficiency and robustness, as the low-level planner can focus on execution details while the high-level planner focuses on strategic goal achievement. Allows for modularity and easier integration of different planning components.","['LLM as Planner', 'Grounded Planning / Environmental Grounding', 'Dynamic Replanning', 'Modular AI System Design']","['Embodied AI', 'Robotics', 'Complex task automation', 'Long-horizon decision-making']","['Planning Patterns', 'Classical AI']",8,
69,Grounded Planning / Environmental Grounding,"Plans generated by language models, while plausible, often lack 'physical grounding' to the specific, dynamic, and partially-observable environment the agent is operating in, leading to unattainable or incorrect actions.",When using Large Language Models (LLMs) for planning in embodied AI systems where the generated plans must be executable and relevant to the agent's current physical surroundings and perceived objects.,"Enhance the LLM's planning process by explicitly incorporating real-time environmental observations (e.g., a list of visible objects, their properties, or a scene description) into the prompt or as a constraint. This feedback loop allows the LLM to generate plans that are physically grounded and adaptable to the current state of the environment.","Produces more robust and executable plans that are relevant to the agent's actual environment. Helps overcome issues like referring to non-existent objects, suggesting impossible actions, or disambiguating objects based on context. Enables the agent to dynamically adapt its plan based on what it perceives.","['Dynamic Replanning', 'Prompt Engineering', 'LLM as Planner', 'Tools Integration Patterns (Object Detector)']","['Embodied agents', 'Robotics', 'Vision-and-language navigation', 'Interactive AI systems that need to operate in physical environments']","['Agentic AI Patterns', 'Knowledge & Reasoning Patterns', 'LLM-specific Patterns']",8,
70,Dynamic Replanning,"Static plans generated upfront by AI models may become invalid or suboptimal due to unforeseen environmental changes, execution failures, or new observations during task execution, leading to agents getting stuck or failing.","Embodied agents operating in dynamic, partially-observable, or unpredictable environments where an initial plan might not remain optimal or feasible throughout the entire task execution.","Implement a mechanism to monitor the agent's progress and environmental state during plan execution. If the agent encounters a failure (e.g., fails to execute an action, gets stuck, or takes too long) or new critical information is perceived, trigger a replanning phase. The AI planner (e.g., an LLM) then generates a new, updated plan or a continuation of the existing plan, incorporating the latest environmental observations and execution status.","Increases the robustness and adaptability of embodied agents to real-world complexities. Allows agents to recover from failures, adapt to changing environments, and achieve goals that would be impossible with static planning. Creates a closed-loop system between the agent, environment, and planner.","['Grounded Planning / Environmental Grounding', 'LLM as Planner', 'Hierarchical Planning', 'Prompt Engineering']","['Embodied AI', 'Robotics', 'Autonomous navigation', 'Long-horizon task execution in uncertain environments']","['Agentic AI Patterns', 'Planning Patterns']",1,
71,Prompt Engineering,"Large Language Models (LLMs) are highly sensitive to the exact wording, structure, and content of input prompts, making it challenging to reliably elicit desired behaviors or optimal performance for specific tasks.","Utilizing LLMs for various downstream tasks, especially in few-shot or zero-shot settings, where the quality of the LLM's output heavily depends on how the task is presented and demonstrated in the prompt.","Systematically design, test, and refine the input prompt given to an LLM. This involves crafting clear instructions, selecting effective in-context examples, specifying output formats, and potentially including environmental context or constraints to guide the LLM towards generating accurate, relevant, and well-structured responses.","Significantly improves the accuracy, relevance, and consistency of LLM outputs for specific tasks. Unlocks the full potential of pre-trained LLMs by aligning their internal knowledge with the task requirements, often without needing model fine-tuning.","['In-Context Learning', 'Few-shot Learning (with LLMs)', 'Dynamic In-Context Example Retrieval', 'Logit Bias / Constrained Generation', 'Grounded Planning / Environmental Grounding']","['Optimizing LLM performance for any task (e.g., planning, summarization, Q&A, code generation)', 'Adapting LLMs to new domains', 'Controlling output format and style']",['Prompt Design Patterns'],25,
72,Dynamic In-Context Example Retrieval,"Providing a fixed set of in-context examples to an LLM for few-shot learning might not be optimal for all test cases, as different examples might be more relevant or informative for different inputs.",Applying Few-shot Learning or In-Context Learning with LLMs where the effectiveness of the demonstrations can vary significantly depending on the similarity between the example and the current test task.,"Instead of using a static set of examples, dynamically select the most relevant in-context examples for each specific test input. This typically involves using a retriever (e.g., k-nearest neighbors based on embedding similarity) to find examples from a larger pool of training examples that are semantically similar to the current test instruction. These retrieved examples are then included in the prompt.",Improves the performance and robustness of in-context learning by providing the LLM with demonstrations that are highly relevant to the current task. Reduces the 'sensitivity' to example choice and leads to more consistent high-quality outputs.,"['In-Context Learning', 'Few-shot Learning (with LLMs)', 'Prompt Engineering']","['Enhancing few-shot learning for LLMs', 'Improving prompt effectiveness', 'Adapting LLMs to diverse task variations within a domain']","['Prompt Design Patterns', 'LLM-specific Patterns']",18,
73,Logit Bias / Constrained Generation,"LLMs can generate arbitrary text, which may include irrelevant or invalid tokens (e.g., actions not allowed, objects not present) when trying to generate structured outputs like plans or specific entity mentions. This can lead to unexecutable plans or incorrect information.","Using LLMs to generate outputs where there are specific constraints on the vocabulary, structure, or entities that should be used, such as generating a plan with a predefined set of actions or mentioning only observed objects.","Apply 'logit biases' during the LLM's generation process. This involves programmatically increasing or decreasing the probability (logits) of specific tokens before sampling. For example, boosting the logits of allowed action tokens or observed object names, and potentially suppressing irrelevant tokens, to guide the LLM towards generating valid and grounded outputs.","Constrains the LLM's output to a desired vocabulary or structure, increasing the likelihood of generating valid and executable plans. Improves grounding by prioritizing observed objects and allowed actions. Reduces errors and makes the LLM's output more reliable for downstream systems.","['Prompt Engineering', 'Grounded Planning / Environmental Grounding', 'LLM as Planner']","['Guiding LLM output for structured data generation', 'Ensuring valid actions in planning', 'Disambiguating object references', 'Enforcing grammar or format constraints']","['Prompt Design Patterns', 'LLM-specific Patterns']",25,
74,LLM as Auxiliary Helper,"Main AI models (e.g., vision-language models, navigation agents) may lack sufficient world knowledge or reasoning capabilities to infer crucial information needed for their primary task, such as likely object locations or relevant landmarks.","Enhancing the performance of an existing AI system by leveraging the commonsense knowledge and reasoning abilities of an LLM, without making the LLM the primary decision-maker or planner.","Use an LLM to generate supplementary information or provide relevant context that assists a separate, specialized model in its core task. For example, an LLM might be prompted to suggest likely locations for a goal object, generate a list of landmarks from a navigation instruction, or provide commonsense explanations. This information is then fed into the main model.","Improves the robustness and intelligence of the main AI system by augmenting it with external knowledge and reasoning. Allows the main model to focus on its specialized task (e.g., vision processing, low-level control) while benefiting from the LLM's broader understanding.","['Tools Integration Patterns', 'Knowledge & Reasoning Patterns']","['Providing commonsense knowledge for navigation', 'Inferring object properties', 'Generating explanations', 'Augmenting perception modules']","['LLM-specific Patterns', 'Tools Integration Patterns']",5,
75,LLM as Skill Ranker / Admissible Action Filter,"In environments with a predefined set of admissible actions or skills, it's challenging to select the most appropriate action at each step, especially when the choice depends on natural language instructions and environmental context.",Embodied agents operating in environments where a comprehensive list of all possible (admissible) actions or skills is known a priori. The task is to choose the best action from this list given the current state and instruction.,"Instead of generating actions directly, use an LLM to rank or filter a pre-enumerated list of admissible actions or skills based on the natural language instruction and potentially the current environmental state. The LLM evaluates each candidate action for its relevance and likelihood of contributing to the task goal.",Simplifies action selection by leveraging the LLM's understanding of language and context to choose from a constrained set of options. Can be effective in environments where admissible actions are easily enumerable.,"['LLM as Planner (contrasted)', 'Tools Integration Patterns']","['Action selection in embodied agents', 'Ranking potential responses', 'Filtering options in interactive systems']","['LLM-specific Patterns', 'Planning Patterns']",5,
76,Modular AI System Design,"Building complex AI systems for tasks like embodied instruction following often involves diverse functionalities (e.g., perception, planning, control, language understanding), making a monolithic design difficult to manage, debug, and improve.","Developing AI systems that integrate multiple specialized components, where each component handles a distinct part of the overall task, allowing for independent development, optimization, and replacement.","Design the AI system as a collection of loosely coupled modules, each responsible for a specific functionality. For example, separating high-level planning, low-level planning, object detection, and perception into distinct modules with well-defined interfaces. This allows for components to be developed or replaced independently.","Enhances system flexibility, maintainability, and scalability. Facilitates independent development and optimization of components. Allows for easier integration of new technologies (e.g., a new LLM planner or a different object detector) without overhauling the entire system.","['Tools Integration Patterns', 'Hierarchical Planning']","['Embodied AI', 'Robotics', 'MLOps (for managing components)', 'Complex multi-modal AI systems']","['MLOps Patterns', 'Tools Integration Patterns']",11,
77,Retrieval-Augmented Generation (RAG),"Large pretrained language models (LMs) have limited ability to access and precisely manipulate knowledge, leading to lower performance on knowledge-intensive tasks, difficulty providing provenance, challenges in updating world knowledge, and propensity for hallucinations.","Knowledge-intensive NLP tasks (e.g., Open-domain QA, abstractive QA, question generation, fact verification) where systems require access to external, up-to-date, and verifiable factual knowledge beyond what is implicitly stored in model parameters.","Combine a pretrained parametric memory (a seq2seq model like BART) with a differentiable access mechanism to an explicit, non-parametric memory (a dense vector index of a knowledge source like Wikipedia accessed by a neural retriever like DPR). The entire system (retriever and generator) is fine-tuned end-to-end, treating retrieved documents as latent variables and marginalizing over them during generation.","Achieves state-of-the-art results on open-domain QA tasks, generates more specific, diverse, and factual language than parametric-only baselines, reduces hallucinations, provides a mechanism for dynamic knowledge updates (hot-swapping the index), and offers a degree of interpretability through inspectable retrieved text.","['Hybrid Parametric and Non-parametric Memory', 'Learned Retrieval', 'Marginalization over Latent Documents', 'RAGSequence Model', 'RAGToken Model', 'REALM (Retrieval-Augmented Language Model pretraining)', 'ORQA (Open-Retrieval Question Answering)', 'Memory Networks', 'Retrieve-and-Edit approaches']","['Open-domain Question Answering (NQ, TriviaQA, WebQuestions, CuratedTrec)', 'Abstractive Question Answering (MSMARCO NLG)', 'Jeopardy Question Generation', 'Fact Verification (FEVER)', 'Any knowledge-intensive NLP task requiring factual grounding and generation']",,3,
78,RAGSequence Model,"How to integrate retrieved documents when generating an entire sequence, assuming a single latent document is most relevant for the whole output, while still leveraging multiple retrieved documents probabilistically.","Retrieval-Augmented Generation (RAG) where the target sequence is relatively coherent and likely supported by a single primary document, or when the entire output sequence benefits from a consistent context. Decoding efficiency is a secondary concern.","The model uses the same retrieved document to predict all tokens in the complete target sequence. It treats the retrieved document as a single latent variable that is marginalized to calculate the seq2seq probability. Top-K documents are retrieved for the input, the generator produces the output sequence probability for each document, and these probabilities are then marginalized. Decoding involves running beam search for each document and then marginalizing probabilities across hypotheses.",Effective for tasks where a single document provides sufficient context for the entire output. Can generate more diverse outputs than RAGToken in some generation tasks. Offers a principled way to incorporate document-level context.,"['Retrieval-Augmented Generation (RAG)', 'Marginalization over Latent Documents', 'RAGToken Model (alternative formulation)']","['Open-domain Question Answering', 'Abstractive Question Answering (MSMARCO NLG)', ""Fact Verification (FEVER, where it's equivalent to RAGToken for length-one sequences)""]",,22,
79,RAGToken Model,"How to integrate retrieved documents when generating a sequence where different parts of the output might benefit from different retrieved documents, allowing for more flexible, token-level information aggregation.","Retrieval-Augmented Generation (RAG) where the target sequence might draw information from multiple sources, or when more flexible and potentially more specific token-level generation is desired. Tasks where the output requires combining facts from various documents.","The model can draw a different latent document for each target token and marginalize accordingly. The top-K documents are retrieved once for the input. For each output token, the generator produces a probability distribution for the next token given each document, before marginalizing these distributions and repeating the process for the next token. Decoding can utilize a standard autoregressive beam decoder.","Performs well on tasks requiring aggregation of content from several documents (e.g., Jeopardy question generation). Offers potentially more specific token-level generation and more efficient decoding compared to RAGSequence. Can adapt context dynamically during generation.","['Retrieval-Augmented Generation (RAG)', 'Marginalization over Latent Documents', 'RAGSequence Model (alternative formulation)']","['Open-domain Question Answering', 'Jeopardy Question Generation (shows stronger performance than RAGSequence)', ""Fact Verification (FEVER, where it's equivalent to RAGSequence for length-one sequences)""]",,22,
80,Learned Retrieval,"Traditional retrieval methods (e.g., BM25) may not be optimally aligned with the specific requirements of a downstream NLP task, leading to suboptimal performance, as they are not trained to maximize the task's objective.","AI systems that rely on external knowledge retrieval where the relevance of retrieved documents is crucial for the performance of a subsequent task (e.g., generative question answering, summarization).","Optimize the retrieval module (e.g., a neural retriever like DPR) jointly with the downstream task model (e.g., a generator like BART) through end-to-end training. This allows the retriever to learn to find documents that are most useful for the specific task objective, rather than just general relevance based on keyword overlap.","Significantly improves results for many knowledge-intensive tasks, especially Open-Domain QA, compared to fixed or non-differentiable retrievers. The retriever becomes specialized to the needs of the generator and the task, leading to better overall system performance.","['Retrieval-Augmented Generation (RAG)', 'Dense Passage Retrieval (DPR-like Retriever)', 'Marginalization over Latent Documents', 'Latent Variable Approaches for Retrieval (REALM, ORQA)']","['Open-domain Question Answering', 'Abstractive Question Answering', 'Question Generation', 'Fact Verification']",,3,
81,Non-parametric Memory Hot-swapping (Dynamic Knowledge Update),"Parametric-only language models struggle to update their world knowledge as facts change, requiring expensive and time-consuming re-training or fine-tuning to incorporate new information, leading to outdated or incorrect responses.","AI systems that need to stay current with evolving factual knowledge (e.g., world leaders, current events, scientific discoveries) without incurring high computational costs for retraining the entire model.","Store factual knowledge in an explicit, external, non-parametric memory (e.g., a dense vector index of Wikipedia passages) that can be easily replaced or updated independently of the core parametric model. When new information becomes available, a new index can be built from an updated corpus and 'hot-swapped' into the system at test time.","Enables rapid and cost-effective updating of the model's world knowledge without requiring any retraining of the parametric components. Improves factual accuracy for recent information and allows the model to adapt to a changing world, contributing to the 'human-writable' aspect of the memory.","['Retrieval-Augmented Generation (RAG)', 'Hybrid Parametric and Non-parametric Memory']","['Question Answering (e.g., queries about current events or individuals whose roles change over time)', 'Fact Verification', 'Any application where the underlying knowledge base is dynamic and needs frequent updates']",,3,
82,Marginalization over Latent Documents,"When multiple documents are retrieved, it's uncertain which single document is 'correct' or most relevant, or if the desired output requires information from several sources. Relying on a single 'best' document can lead to errors if the retriever makes a mistake.","Retrieval-augmented models that retrieve multiple candidate documents for a given input and need to combine their information to produce a single, coherent output, especially in generative tasks.","Treat the retrieved documents as latent variables in a probabilistic model. During training and inference, sum (marginalize) over the probabilities of generating the target output given each of the top-K retrieved documents, weighted by the retriever's prior probability of selecting that document. This allows the model to consider a 'blend' of evidence.","Allows the model to consider evidence from multiple sources, improving robustness by not relying solely on a single 'best' retrieved document. Enables the model to generate correct answers even when the correct answer is not explicitly present in any single retrieved document (by combining clues), leading to more effective information aggregation.","['Retrieval-Augmented Generation (RAG)', 'RAGSequence Model', 'RAGToken Model', 'REALM (Retrieval-Augmented Language Model pretraining)', 'Latent Variable Approaches for Retrieval']","['Open-domain Question Answering', 'Abstractive Question Answering', 'Jeopardy Question Generation', 'Fact Verification']",,3,
83,Hybrid Parametric and Non-parametric Memory,"Purely parametric language models struggle with factual accuracy, interpretability, and dynamic knowledge updates due to their 'black box' nature and fixed knowledge base. Purely non-parametric (retrieval-only) models lack the strong generative capabilities and contextual understanding of large LMs.","Designing AI systems that require both strong general language understanding/generation capabilities (fluency, coherence) and access to precise, verifiable, and updatable factual knowledge.","Combine a large, pretrained parametric model (e.g., a seq2seq transformer like BART) that stores implicit general knowledge and language patterns in its parameters with an explicit, external non-parametric memory (e.g., a dense vector index of text) that stores factual knowledge. The parametric model uses the non-parametric memory to ground and augment its generation or reasoning.","Leverages the strengths of both approaches: the fluency, coherence, and broad understanding of parametric models, and the factual accuracy, verifiability, and dynamic updatability of non-parametric memory. Reduces hallucinations, improves factuality, and enhances interpretability by allowing inspection of accessed knowledge.","['Retrieval-Augmented Generation (RAG)', 'Non-parametric Memory Hot-swapping (Dynamic Knowledge Update)', 'Memory Networks', 'REALM (Retrieval-Augmented Language Model pretraining)']","['Knowledge-intensive NLP tasks (Question Answering, Generation, Fact Verification)', 'Building AI systems that require grounded, up-to-date, and explainable factual responses']",,3,
84,Denoising Sequence-to-Sequence Pretraining for Generation (BART-like Generator),"Training a robust and versatile sequence-to-sequence model for various natural language generation (NLG) tasks that can produce fluent, coherent, and grammatically correct text, and serve as a strong base for fine-tuning.","Developing a general-purpose generator component for NLP systems, particularly when it needs to be integrated into a larger architecture (like RAG) and perform well on diverse generation tasks, handling both discriminative and generative objectives.","Pretrain an encoder-decoder transformer model (like BART) using a denoising objective. This involves corrupting text with various noising functions (e.g., token masking, deletion, text infilling, sentence permutation) and training the model to reconstruct the original, uncorrupted text. This objective encourages learning robust representations and generation capabilities.","Obtains state-of-the-art results on a diverse set of generation tasks (e.g., summarization, translation). Provides a strong foundation for fine-tuning on specific seq2seq tasks, producing fluent and coherent text. Serves as an effective parametric generator component in hybrid models like RAG.","['Retrieval-Augmented Generation (RAG)', 'Hybrid Parametric and Non-parametric Memory', 'T5 (Text-to-Text Transformer)']","['General-purpose Natural Language Generation', 'Machine Translation', 'Summarization', 'Generative Question Answering', 'As the generator component in RAG models']",,22,
85,Dense Passage Retrieval (DPR-like Retriever),"Inefficient and ineffective retrieval of relevant text passages from very large, unstructured text corpora given a natural language query, which is crucial for open-domain knowledge-intensive tasks.","Building retrieval components for knowledge-intensive AI systems that need to access large, unstructured text corpora (e.g., Wikipedia) to find relevant evidence or context for a subsequent task.","Employ a bi-encoder architecture: a query encoder (e.g., BERT-based) creates a dense vector representation of the query, and a document encoder (e.g., BERT-based) creates dense vector representations for all documents in the corpus. Retrieval is then performed by finding documents whose embeddings have the highest inner product similarity with the query embedding using Maximum Inner Product Search (MIPS). The retriever is often pretrained with retrieval supervision (e.g., to retrieve passages containing answers to questions).","Achieves high retrieval recall and precision for open-domain tasks. Enables fast approximate nearest neighbor search (MIPS using FAISS) even with millions of documents. Provides a strong foundation for 'learned retrieval' by being fine-tunable, making it an effective component in RAG models.","['Learned Retrieval', 'Retrieval-Augmented Generation (RAG)', 'BM25 (word overlap-based retriever, often a baseline or comparison point)']","['Open-domain Question Answering', 'Fact Verification', 'As the retrieval component in RAG models', 'Efficient document retrieval from large corpora']",,3,
86,Production System,"How to define systems capable of complex, hierarchically structured behaviors through iterative rule application, often for symbolic manipulation.","Originating in efforts to characterize the limits of computation (e.g., string rewriting), later adopted by the AI community to capture human problem solving.","A formalism consisting of a set of rules, each specifying a precondition and an action. When the precondition is met, the action can be taken, modifying a state (e.g., a string or logical state).","Generates a set of outcomes by iteratively applying rules, capable of complex behaviors, and can be shown to be Turing complete when control flow is imposed.","['Cognitive Architecture', 'Language Models as Probabilistic Production Systems']","String manipulation, formal languages (Chomsky's phrase structure grammar), logical systems, early AI problem-solving (e.g., thermostat agent).","['Classical AI', 'Knowledge & Reasoning']",11,
87,Cognitive Architecture,"How to build flexible, rational, real-time agents that mimic human cognition, explicitly instantiating processes such as perception, memory, and planning.","Building on production systems, AI researchers sought to integrate these rule-based systems with sensory input, actuators, and knowledge bases to achieve human-like problem-solving.","Augments a production system with sensory groundings, various types of long-term memory (procedural, semantic, episodic), and a generalized decision procedure for selecting, applying, and even generating new productions.","Agents capable of flexible, rational, and real-time behaviors, with structured internal state management and adaptive learning mechanisms.","['Production System', 'Modular Memory', 'Generalized Decision-Making Loop', 'Grounding Actions']","Psychological modeling, robotics, military simulations, intelligent tutoring (e.g., Soar architecture).","['Agentic AI', 'Classical AI', 'Knowledge & Reasoning']",11,
88,Modular Memory,"Large language models (LLMs) are stateless; how to enable language agents to persist, organize, and maintain information internally for multistep interactions with the world.","Designing language agents that require internal state to remember past interactions, store knowledge, track current goals, and support reasoning and planning across multiple LLM calls.","Organize information (primarily textual, but other modalities allowed) into multiple distinct memory modules, each containing a different form of information. These typically include short-term working memory and several long-term memories (episodic, semantic, and procedural).","Agents can maintain internal state, leverage past experiences, access world knowledge, and store operational procedures, overcoming the inherent statelessness of LLMs and enabling complex, long-duration interactions.","['Working Memory', 'Episodic Memory', 'Semantic Memory', 'Procedural Memory', 'Cognitive Architecture']","Language agents, cognitive language agents (e.g., CoALA framework), any AI system requiring persistent internal state.","['Agentic AI', 'Knowledge & Reasoning', 'LLM-specific']",29,
89,Working Memory,"How to manage active and readily available information for an agent's current decision cycle, including perceptual inputs, active knowledge, and intermediate reasoning results.","Within a language agent, needing a dynamic data structure that persists across LLM calls to track the agent's current circumstances and facilitate decision-making.","A memory module that maintains active and readily available information as symbolic variables for the current decision cycle. It stores perceptual inputs, active knowledge generated by reasoning or retrieved from long-term memory, and core information carried over from previous cycles (e.g., active goals). LLM input is synthesized from it, and LLM output is parsed back into it.","Serves as the central hub connecting different components of a language agent (LLM, long-term memories, grounding interfaces), enabling context-aware and responsive decision-making.","['Modular Memory', 'Reasoning Actions', 'Structured Output Parsing']","Cognitive architectures (Soar), language agents (CoALA framework), prompt templates and variable population for LLM calls.","['Agentic AI', 'Knowledge & Reasoning']",29,
90,Episodic Memory,"How to store and leverage experience from earlier decision cycles, past behaviors, or event flows for future reasoning, learning, or planning.","Agents needing to learn from their history, retrieve relevant past events, or reflect on sequences of actions and observations to adapt their behavior.","A memory module that stores representations of the agent's experiences, such as training input-output pairs, history event flows, or game trajectories. These episodes can be retrieved into working memory during the planning stage to support reasoning.","Allows agents to access and learn from their past, enabling recall of relevant examples, bases for reasoning, and support for learning by storing new experiences.","['Modular Memory', 'Learning Actions', 'Reflection / Self-Improvement']","RL agents storing episodic trajectories, Generative Agents (storing events), support for reasoning and decision-making in language agents.","['Agentic AI', 'Knowledge & Reasoning']",29,
91,Semantic Memory,"How to store and access an agent's explicit knowledge about the world and itself, including facts, concepts, and general information.","Agents requiring external knowledge support for reasoning or decision-making, or needing to incrementally build up their understanding of the world from experience.","A memory module that stores an agent's knowledge about the world and itself, often initialized from external databases (e.g., Wikipedia) or game manuals. Language agents may also write new knowledge obtained from LLM reasoning into semantic memory as a form of learning.","Provides a knowledge base that can be retrieved to inform reasoning and decision-making, allowing agents to ground their understanding and incrementally build world knowledge.","['Modular Memory', 'Retrieval Augmented Generation (RAG)', 'Learning Actions', 'Reflection / Self-Improvement']","Retrieval-augmented NLP methods, reading-to-learn RL approaches, Generative Agents (storing reflections), robotics (semantic maps).","['Agentic AI', 'Knowledge & Reasoning']",29,
92,Procedural Memory,"How to store the agent's operational knowledge, including its rules of behavior, skills, and how to implement actions and decision-making processes.",Agents requiring both implicit (learned via weights) and explicit (coded rules and procedures) knowledge to define their behavior and control flow.,"Comprises two forms: implicit knowledge stored in the LLM weights, and explicit knowledge written in the agent's code. Explicit knowledge includes procedures for implementing actions (reasoning, retrieval, grounding, learning) and procedures for decision-making itself. Learning actions can update this memory by writing new agent code.","Defines the agent's capabilities and control flow, allowing for both the flexible, implicit knowledge of LLMs and the interpretable, extensible (but potentially brittle) explicit code.","['Modular Memory', 'Code as Policies / Skill Library', 'Learning Actions', 'LLM-Code Hybrid Control']","Cognitive architectures (Soar), language agents (CoALA framework), Voyager (skill library), prompt templates.","['Agentic AI', 'Knowledge & Reasoning']",11,
93,Structured Action Space,"How to systematically define the range of actions an agent can take, encompassing both interactions with the external world and internal cognitive processes, for clear and task-suitable agent design.","Designing language agents that need to perform complex tasks requiring both external manipulation (e.g., controlling a robot, using tools) and internal deliberation (e.g., reasoning, memory access, learning).","Divide the agent's action space into two main categories: external actions (interacting with external environments through 'grounding') and internal actions (interacting with internal memories and processes, further decomposed into retrieval, reasoning, and learning).","Provides a clear and task-suitable set of capabilities for the agent, enabling the design of more complex and autonomous systems by systematically considering all possible operations.","['Grounding Actions', 'Retrieval Actions', 'Reasoning Actions', 'Learning Actions', 'Modular Agent Design']","Language agent design (CoALA framework), reinforcement learning, cognitive architectures.","['Agentic AI', 'Tools Integration']",11,
94,Grounding Actions,"How to connect an agent's internal (often language-based) representations and decisions to its external environment, enabling it to perceive and act in the real or digital world.","Language agents needing to interact with physical robots, engage in dialogue with humans, or manipulate digital interfaces (games, APIs, websites) where direct LLM output is insufficient.","Procedures that execute external actions (e.g., motor commands, API calls, natural language responses) and process environmental feedback (e.g., sensor data, website observations, human dialogue) into textual representations for the agent's working memory.","Allows language models to operate in interactive environments, translating high-level instructions into concrete actions and observations into text, effectively simplifying external interaction to a 'text game'.","['Structured Action Space', 'Tool Use / Digital Grounding']","Robotics (Ahn et al. 2022), web manipulation (Yao et al. 2022a), human-agent dialogue, game interaction.","['Tools Integration', 'AI-Human Interaction', 'Agentic AI']",8,
95,Retrieval Actions,"How to efficiently access and bring relevant information from an agent's long-term memories (episodic, semantic, procedural) into its working memory for current decision-making.","Agents needing to leverage stored experiences, world knowledge, or operational skills to inform their reasoning, planning, or to provide context-specific information.","A procedure that reads information from long-term memory modules into working memory. This can be implemented using various methods such as rule-based, sparse, or dense retrieval, depending on the information and memory type.","Enhances decision-making by providing context-specific information from the agent's knowledge base and history, improving the agent's ability to plan and reason.","['Structured Action Space', 'Modular Memory', 'Retrieval Augmented Generation (RAG)']","Voyager (loading code-based skills), Generative Agents (retrieving relevant events), DocPrompting (leveraging library documents), supporting planning stages in decision cycles.","['Knowledge & Reasoning', 'Agentic AI']",29,
96,Reasoning Actions,"How to process the contents of working memory to generate new information, distill insights, or elaborate on observations, supporting learning and decision-making.","Language agents needing to analyze recent observations, summarize trajectories, or process retrieved information to form new conclusions or intermediate steps before acting or learning.","Procedures that read from and write to working memory, often using the LLM itself to summarize, distill insights, or infer new information about observations, trajectories, or retrieved facts.","Supports learning by generating results that can be written into long-term memory, and enhances decision-making by providing additional context or intermediate steps for subsequent LLM calls.","['Structured Action Space', 'Working Memory', 'Chain-of-Thought (CoT) Prompting', 'Reflection / Self-Improvement']","ReAct (analyze situation, remake action plans), Generative Agents (generate reflections), Tree of Thoughts (propose thoughts), creating additional context for LLM prompts.","['Knowledge & Reasoning', 'Agentic AI', 'LLM-specific']",29,
97,Learning Actions,"How to enable an agent to acquire new knowledge, skills, or modify its behavior over its lifetime by committing information to long-term memory.","Agents needing to adapt to new environments, improve performance, or build up their internal representations of the world and their capabilities.","Procedures that involve writing information to long-term memory modules (episodic, semantic, procedural) or updating LLM parameters. This spectrum includes storing experiences, inferred knowledge, finetuning LLMs (implicit procedural knowledge), or modifying agent code (explicit procedural knowledge).","Allows agents to continuously improve, adapt, and build a richer internal model of the world and their own capabilities, leading to efficient lifelong learning.","['Structured Action Space', 'Modular Memory', 'Episodic Memory', 'Semantic Memory', 'Procedural Memory', 'Reflection / Self-Improvement', 'Metalearning for Agents']","Voyager (adding new grounding procedures), Generative Agents (storing reflections), Reflexion (storing inferences from failed episodes), finetuning LLMs via supervised or reinforcement learning.","['Agentic AI', 'MLOps', 'Knowledge & Reasoning']",29,
98,Generalized Decision-Making Loop,"How to structure the top-level control flow of a language agent to enable deliberate, multi-step planning and action selection, moving beyond direct, single-step action generation.","Agents facing complex tasks where a direct action might not be optimal, requiring foresight, evaluation of alternatives, and iterative refinement of plans.","A repeated decision cycle that yields an external (grounding) action or an internal (learning) action. In each cycle, program code defines a sequence of reasoning and retrieval actions to propose and evaluate alternatives (planning stage), then executes the selected action (execution stage), and the cycle loops again after an observation.","Enables agents to engage in more sophisticated, deliberative behaviors, including iterative improvement of plans and consideration of multiple alternatives, leading to more robust and intelligent actions.","['Cognitive Architecture', 'Propose-Evaluate-Select Planning', 'Reasoning Actions', 'Retrieval Actions']","CoALA framework for language agents, Tree of Thoughts, RAP (Reasoning and Acting with Planning).","['Planning', 'Agentic AI']",17,
99,Propose-Evaluate-Select Planning,"Within a generalized decision cycle, how to systematically generate, assess, and choose among multiple possible actions or 'thoughts' to make informed decisions.","Agents needing to consider potential outcomes and optimize for desired goals in complex situations where a direct action is insufficient, requiring deliberation.","A substage within the planning stage of a decision loop, comprising three steps: 1) Proposal: Generating one or more action candidates (e.g., via LLM sampling, code structures, simulators). 2) Evaluation: Assigning a value to each candidate (e.g., heuristic rules, LLM perplexity, learned values, internal simulation). 3) Selection: Choosing the best action based on evaluation (e.g., argmax, softmax, majority vote) or rejecting candidates and looping back to proposal.","Enables agents to make more informed and robust decisions by considering potential outcomes, optimizing for desired goals, and allowing for iterative improvement of candidate solutions.","['Generalized Decision-Making Loop', 'Reasoning Actions', 'Tree of Thoughts (ToT)']","Classical planning algorithms, Tree of Thoughts, RAP, advanced language agents.","['Planning', 'Agentic AI']",17,
100,Language Models as Probabilistic Production Systems,"How to conceptually link modern LLMs to classical AI formalisms, understanding their generative capabilities in a structured way that supports agent design.",Drawing parallels between LLMs and historical production systems to build a theoretical foundation for cognitive language agents and apply control mechanisms.,"View an LLM as defining a probability distribution over which 'productions' (string completions or modifications) to select when presented with an input (prompt), yielding a distribution P(Y|X). This treats the LLM as sampling a possible completion each time it is called.","Provides a theoretical bridge between symbolic AI and LLMs, suggesting that control mechanisms from cognitive architectures, originally used with production systems, can be equally applicable to transform LLMs into language agents.","['Production System', 'Cognitive Language Agent']","Conceptual framework for LLM behavior, foundation for LLM-based agent design, understanding LLM's role in a cognitive architecture.","['LLM-specific', 'Generative AI', 'Classical AI']",11,
101,Prompt Engineering,"How to guide or bias the output of an LLM towards high-quality, task-specific responses or to elicit targeted reasoning, without altering its internal weights.","Using LLMs for various tasks where their general knowledge needs to be focused, or their reasoning capabilities need to be enhanced for specific problems.","Preprocessing the input string (the prompt) by concatenating additional text, few-shot examples, instructions, or external observations to influence the LLM's conditional distribution over completions. These manipulations can be seen as 'productions' themselves.","Allows users to elicit targeted reasoning and desired outputs, making LLMs more versatile for specific tasks, and serving as a form of task-specific prioritization of productions.","['Zeroshot Prompting', 'Fewshot Prompting', 'Retrieval Augmented Generation (RAG)', 'Socratic Models', 'Self-Critique Prompting', 'Chain-of-Thought (CoT) Prompting', 'Prompt Chaining']","Question answering, code generation, various NLP tasks, biasing LLMs towards specific behaviors or reasoning processes.","['Prompt Design', 'LLM-specific']",18,
102,Zeroshot Prompting,"How to make an LLM perform a task it hasn't been explicitly trained for, with minimal or no examples, relying solely on its pre-trained knowledge.","Applying LLMs to novel tasks where labeled data for finetuning is unavailable, too costly, or when a quick, initial solution is needed.","Formulating the task directly as an input string (a question or instruction) to the LLM, expecting it to generate a relevant completion or answer based on its vast pre-training corpus.","Enables immediate application of LLMs to a wide range of tasks 'out of the box' without task-specific examples, leveraging their implicit world knowledge.",['Prompt Engineering'],"General question answering, simple instruction following, initial exploration of LLM capabilities for a new task.","['Prompt Design', 'LLM-specific']",18,
103,Fewshot Prompting,"How to improve an LLM's performance on a specific task by providing a small number of in-context examples, especially when finetuning is not feasible or desired.","Tasks where zeroshot performance is insufficient, but full finetuning is impractical due to data scarcity, cost, or a desire for rapid adaptation.","Including a few input-output examples directly in the prompt, demonstrating the desired task behavior before presenting the actual query. These examples guide the LLM's inference for the new input.","Biases the LLM towards high-quality productions relevant to the task, improving accuracy and adherence to desired formats by showing the model 'how' to respond.",['Prompt Engineering'],"Adapting LLMs to novel tasks with limited data, improving specific task performance, demonstration-based learning.","['Prompt Design', 'LLM-specific']",18,
104,Retrieval Augmented Generation (RAG),"How to ground LLM generations in external, up-to-date, or specific knowledge, mitigating hallucinations, outdated information, and limitations of the LLM's internal knowledge base.","LLMs needing to answer questions or generate text that requires specific factual information not present in their training data, or to be current and attributable.","Preprocessing the prompt by retrieving relevant external observations or documents (e.g., from Wikipedia, databases, or specialized knowledge bases) and concatenating them with the original query before passing it to the LLM.","LLMs can generate more accurate, grounded, and up-to-date responses by leveraging external knowledge sources, reducing hallucinations and improving factual consistency.","['Prompt Engineering', 'Semantic Memory', 'Retrieval Actions']","Knowledge-intensive NLP tasks, open-domain question answering, fact-checking, generating text with specific external context.","['Prompt Design', 'Knowledge & Reasoning', 'Generative AI', 'LLM-specific']",3,
105,Socratic Models,"How to combine different specialized models (e.g., vision, language) to perform complex multimodal reasoning, orchestrating their individual strengths.","Tasks requiring processing of multiple modalities (e.g., image and text) where a single LLM might not be sufficient or efficient, and modularity is desired.","Using an LLM as a central orchestrator that queries other specialized models (e.g., a Vision-Language Model for observations) to translate perceptual data into text, and then integrates their outputs into its reasoning process.","Enables multimodal reasoning by leveraging the strengths of different models, with the LLM providing high-level control and coherence for complex tasks.","['Prompt Engineering', 'Tools Integration', 'Grounding Actions']","Multimodal question answering, embodied reasoning, integrating specialized perception models with LLMs.","['Prompt Design', 'Generative AI', 'Tools Integration', 'LLM-specific']",26,
106,Self-Critique Prompting,"How to enable an LLM to evaluate and refine its own outputs, improving quality and correctness without relying solely on external human feedback.","Improving the reliability, accuracy, or creativity of LLM generations, especially for tasks where a single pass might produce errors, suboptimal results, or require iterative improvement.","After an initial generation, the LLM is prompted again, often with its previous output and a specific request to critique, identify defects, or propose modifications. This can occur in multiple iterations, allowing the LLM to act as its own evaluator.","Leads to iterative refinement and potentially higher-quality, more accurate outputs by allowing the LLM to identify and correct its own mistakes, mimicking human self-correction.","['Prompt Engineering', 'Reasoning Actions', 'Reflection / Self-Improvement', 'Prompt Chaining']","Improving code, creative writing, complex problem-solving, self-evaluation in agents.","['Prompt Design', 'Generative AI', 'LLM-specific']",23,
107,Chain-of-Thought (CoT) Prompting,"How to enable LLMs to perform complex reasoning tasks that require multiple intermediate steps, making their decision process more transparent and accurate.","Tasks that humans would solve by breaking them down into logical steps (e.g., arithmetic, multi-step logical problems), where direct generation often fails or produces incorrect results.",Prompting the LLM to generate a series of intermediate reasoning steps before providing the final answer. This can be achieved by adding explicit instructions like 'Let's think step by step' or by providing few-shot examples that include the desired reasoning process.,"Elicits more deliberate and accurate reasoning, improving performance on arithmetic, common sense, and symbolic reasoning tasks, and making the LLM's process more interpretable.","['Prompt Engineering', 'Reasoning Actions']","Complex problem-solving, arithmetic, logical reasoning, improving transparency of LLM decisions.","['Prompt Design', 'Generative AI', 'LLM-specific']",26,
108,Prompt Chaining,"How to construct increasingly complicated algorithms or multi-step processes by sequencing multiple calls to an LLM, where each step's output informs the next.","Tasks that require a sequence of operations, or iterative refinement, moving beyond single-shot prompting to build more sophisticated workflows or algorithms.","Structuring a series of LLM calls, where the output of one call is processed (parsed, modified, combined with other information) and then used to construct the prompt for the subsequent LLM call.","Enables the implementation of complex, multi-stage algorithms and workflows, turning LLMs into more versatile computational units capable of multi-step reasoning and iterative processes.","['Prompt Engineering', 'Generalized Decision-Making Loop', 'Self-Critique Prompting']","Multi-step reasoning, iterative refinement, agent control, generating complicated algorithms.","['Prompt Design', 'LLM-specific', 'Agentic AI']",23,
109,Cognitive Language Agent,"How to design and implement AI systems that leverage the powerful language understanding and generation capabilities of LLMs to interact with complex environments, perform reasoning, and learn over time.",Building autonomous or semi-autonomous agents that combine the strengths of LLMs with structured internal processes and a feedback loop with the external world.,"Place an LLM as the core computational unit within a larger cognitive architecture (like CoALA), integrating it into a feedback loop with an external environment. The agent uses the LLM to manage internal state (via modular memory), perform intermediate reasoning, and employ sophisticated learning strategies.","Creates agents capable of more human-like intelligence, adapting to novel tasks, performing complex reasoning, and operating effectively in interactive environments.","['Cognitive Architecture', 'Language Models as Probabilistic Production Systems', 'Modular Memory', 'Structured Action Space', 'Generalized Decision-Making Loop']","Robotics, web manipulation, puzzle solving, interactive code generation, social simulation, general-purpose language agents.","['Agentic AI', 'LLM-specific']",11,
110,Tool Use / Digital Grounding,"How to extend the capabilities of LLMs beyond text generation to perform actions in digital environments or access external functionalities, overcoming their inherent limitations in computation or real-time data access.","LLMs needing to interact with games, APIs, websites, or execute code to gather information, perform computations, or carry out operations beyond their inherent knowledge or capabilities.","Packaging external digital services (e.g., search engines, calculators, translators, web browsers, code interpreters) as 'tools' that the LLM can call. The LLM's output includes calls to these tools, and their results are fed back to the LLM as observations or additional context.","Greatly expands the LLM's agency and problem-solving capacity in digital domains, allowing it to perform computations, access real-time data, interact with software, and augment its knowledge.","['Grounding Actions', 'Structured Action Space', 'Socratic Models']","Web agents, API interaction, general code execution, augmented NLP tasks requiring external knowledge or computation.","['Tools Integration', 'Agentic AI', 'LLM-specific']",44,
111,Reflection / Self-Improvement,"How to enable an agent to learn from its past failures or experiences by critically analyzing its performance and generating new insights, knowledge, or behavioral modifications.","Agents needing to improve their strategies or knowledge base based on feedback from the environment or internal evaluation of trajectories, leading to more robust and efficient future behaviors.","Using an LLM to reason about past episodes, failed attempts, or observed trajectories, distilling insights, generating new semantic inferences, or proposing modifications to its behavior. These insights are then often stored in long-term memory (semantic or episodic) to inform future decisions.","Allows agents to adapt and improve their future behaviors, making them more robust, efficient, and capable of learning from their own experiences, leading to continuous self-improvement.","['Learning Actions', 'Reasoning Actions', 'Episodic Memory', 'Semantic Memory', 'Self-Critique Prompting']","Reflexion (storing inferences from failed episodes), Generative Agents (generating reflections), adapting future behaviors based on past experience.","['Agentic AI', 'Knowledge & Reasoning', 'LLM-specific']",23,
112,Code as Policies / Skill Library,"How to enable an agent to acquire, store, and reuse complex, code-based skills for interacting with its environment, particularly in domains requiring precise, programmatic actions.","Agents operating in environments (e.g., robotics, complex digital interfaces like Minecraft) that require a diverse set of specific, programmatic actions, where LLMs can generate high-level plans but need concrete execution mechanisms.","Maintain a long-term procedural memory that stores a library of code-based grounding procedures (skills). These skills can be retrieved (e.g., via dense retrieval) and executed by the agent. The agent can also learn new skills by generating code and adding it to this library.","Provides a hierarchical and extensible action space, allowing agents to master complex tasks and generalize to unseen scenarios by composing and learning new skills, complementing LLM planning with deterministic execution.","['Procedural Memory', 'Learning Actions', 'Retrieval Actions', 'Tool Use / Digital Grounding', 'LLM-Code Hybrid Control']","Voyager in Minecraft (e.g., combatZombie, craftStoneSword), robotic control, general code generation for agent actions.","['Tools Integration', 'Agentic AI', 'MLOps']",11,
113,Tree of Thoughts (ToT),"How to perform deliberate, multi-step problem-solving with LLMs that involves exploring multiple reasoning paths, backtracking, and foresight, similar to classical search algorithms, for tasks beyond simple sequential reasoning.","Complex reasoning problems (e.g., game of 24, creative writing, crosswords) where a single chain of thought is insufficient, and evaluation of intermediate steps ('thoughts') and global exploration is crucial.","The LLM iteratively proposes 'thoughts' (which are reasoning actions) and evaluates them, maintaining them via a tree search algorithm (e.g., BFS, DFS, Monte Carlo Tree Search). The LLM is used to generate proposals (simulate rollouts conditioned on an action) and evaluate outcomes (value the outcome of the proposed action).","Enables global exploration and local backtracking, significantly improving performance on tasks requiring planning, systematic exploration of possibilities, and deliberate problem-solving beyond linear thought processes.","['Propose-Evaluate-Select Planning', 'Reasoning Actions', 'Generalized Decision-Making Loop']","Game of 24, creative writing, crosswords, complex reasoning problems requiring search and evaluation.","['Planning', 'LLM-specific']",17,
114,Modular Agent Design,"The proliferation of custom terminology and implementations makes it difficult to compare, evolve, and build new language agents, leading to technical debt and compatibility issues.","The emerging field of language agents lacks a standardized framework for organization and development, hindering research progress and industrial deployment.","Structure agents into distinct, interchangeable modules (e.g., Memory, Action, Agent classes) with standardized interfaces and terminology. This involves defining useful abstractions and casting simpler agents into such a framework.","Consolidates technical investment, improves compatibility, facilitates modular 'plug-and-play' and reuse of components, and standardizes the customer experience in industry applications, reducing technical debt.","['Cognitive Architecture', 'Structured Action Space', 'Modular Memory']","Academic research frameworks (like CoALA), industry language agent libraries, general software engineering for AI systems.","['MLOps', 'Agentic AI']",11,
115,LLM-Code Hybrid Control,"How to combine the strengths of LLMs (flexibility, commonsense priors, zero-shot generalization) with the reliability, interpretability, and deterministic control of traditional agent code.","LLMs are powerful but often brittle, opaque, and stochastic, while handcrafted code is reliable but lacks generalization and adaptability to unforeseen situations.","Use agent code to implement generic, deterministic algorithms (e.g., tree search, conditional logic, loops) that complement LLM limitations, while leveraging LLMs for flexible, context-dependent reasoning, planning, and generation in new contexts.","Creates more robust and capable agents that benefit from both explicit, reliable control and learned flexibility, mitigating the weaknesses of each approach and improving overall agent performance.","['Procedural Memory', 'Tool Use / Digital Grounding', 'Tree of Thoughts (ToT)']","Implementing tree search with LLMs, managing complex control flows, building agents that require both precise execution and adaptable reasoning.","['Tools Integration', 'LLM-specific', 'Agentic AI']",11,
116,Structured Output Parsing,"How to reliably extract specific, structured information from the free-form text output of an LLM and map it to internal variables or executable actions in an agent system.","Integrating LLMs into agent systems where their natural language output needs to drive specific, programmatic actions, update structured state variables, or conform to predefined data schemas.","Employing techniques or tools (e.g., OpenAI function calling, Guidance, custom parsers) to define expected output formats (e.g., JSON schema, function calls with arguments) and parse the LLM's generated text into corresponding structured variables or commands.","Enables seamless and reliable integration of LLM reasoning with code infrastructure, allowing LLM outputs to directly trigger agent actions, update working memory, or control other system components in a predictable and consistent manner.","['Prompt Engineering', 'Working Memory', 'Tools Integration']","LangChain, LlamaIndex, OpenAI function calling, Guidance, any agent system needing to convert LLM text to structured data or actions.","['Tools Integration', 'Prompt Design', 'LLM-specific']",44,
117,Metalearning for Agents,"How to enable agents to learn *how* to learn more effectively, rather than just learning specific tasks or acquiring specific knowledge.","Improving the efficiency, adaptability, and long-term self-improvement capabilities of agents, especially in complex or open-ended environments where learning strategies themselves can be optimized.","Modifying the agent's own code or procedures related to its learning, retrieval, or decision-making processes. Examples include learning better retrieval procedures (e.g., query-document expansion, retrieval distillation) or adapting learning schedules.","Enhances the agent's ability to acquire and utilize information and skills, leading to more significant adaptability and self-improvement beyond what can be hard-coded, enabling agents to go beyond human-written code.","['Learning Actions', 'Procedural Memory']","Research direction for future agents, adaptive retrieval mechanisms, optimizing learning schedules, improving prompt templates.","['Agentic AI', 'MLOps', 'Knowledge & Reasoning']",11,
118,Synergistic Reasoning and Acting (ReAct),How to design language agents that effectively combine internal reasoning with external actions in a feedback loop to solve complex interactive tasks.,"Early language agents either acted directly or used predefined prompt chains. There was a need for agents that could dynamically reason about a situation and then act, using environmental feedback to refine subsequent reasoning.","A fixed decision cycle that uses a single internal reasoning action to analyze the situation and remake action plans, followed by generating an external grounding action. This process is repeated, creating a feedback loop where reasoning guides acting, and acting provides environmental feedback to support reasoning.","Demonstrates that combining internal reasoning with external actions in an interactive feedback loop leads to more effective and adaptive language agents, especially in interactive digital environments.","['Reasoning Actions', 'Grounding Actions', 'Generalized Decision-Making Loop', 'Cognitive Language Agent']","Text games, Wikipedia API interaction, general web-based tasks (e.g., ReAct agent).","['Agentic AI', 'LLM-specific', 'Planning']",8,
119,Retrieval Augmented Generation (RAG),"Traditional language models (LLMs) often suffer from hallucinations, lack access to up-to-date or domain-specific knowledge, and rely solely on parametric memory, which can be expensive to scale. Additionally, training conventional two-stage Open-Domain Question Answering (ODQA) systems often requires explicitly annotated context-question-answer triplets, which are difficult to acquire.","Designing AI systems for knowledge-intensive NLP tasks such as Open-Domain Question Answering, summarization, or conversational AI, where it is crucial to combine the generative capabilities of language models with a broad, external, and verifiable knowledge base.","Integrates a neural retriever (e.g., Dense Passage Retrieval - DPR) with a seq2seq generator (e.g., BART) into a unified architecture. The retriever component dynamically identifies and fetches relevant passages from a large external knowledge base (non-parametric memory) based on an input query. These retrieved passages then condition the generator, guiding it to produce more factual, grounded, and interpretable responses. The system can be finetuned by propagating gradients to the generator and the question encoder.","Reduced hallucinations, improved factual consistency, higher interpretability, and enhanced performance in knowledge-intensive tasks by leveraging external, up-to-date, and verifiable knowledge. Performs well on general-purpose, Wikipedia-based datasets.","['REALM', 'RETRO']","['Open-Domain Question Answering', 'Abstractive Summarization', 'Knowledge-grounded Conversational AI', 'Chatbot Frameworks', 'Fact-checking']",,3,
120,End-to-End Retrieval Augmented Generation (RAGend2end),"Standard RAG models, when finetuned for new, specialized domains (e.g., healthcare, news), often perform suboptimally because the passage encoder and the external knowledge base encoding are kept fixed during training, relying on prior training on general (e.g., Wikipedia) datasets. Adapting to new domains requires updating the domain-specific knowledge representations for both the retriever and the knowledge base itself, but re-encoding and re-indexing large knowledge bases is computationally intensive and can stall the training process.","Adapting Retrieval Augmented Generation (RAG) models to perform Open-Domain Question Answering or similar tasks in specialized domains where the external knowledge base is distinct from the general-purpose data on which the retriever was initially trained. There is a need for comprehensive, domain-specific retriever adaptation without incurring prohibitive computational costs during training.","Extends the RAG architecture to enable joint, end-to-end finetuning of *all* its components, including both the question encoder (EQ) and the passage encoder (EP) of the retriever (DPR), and dynamically updates the external knowledge base's embeddings and index during training. To manage the computational burden of re-encoding and re-indexing the knowledge base, these processes are run *asynchronously* in parallel to the main training loop, utilizing dedicated computational resources (e.g., multiple GPUs for re-encoding, multiple CPU cores for FAISS re-indexing).","Significantly improved domain adaptation performance for ODQA tasks, better adaptation of the retriever component to domain-specific data compared to standalone retriever finetuning, and enhanced overall accuracy across diverse specialized domains. This approach eliminates the need for separate retriever training with gold-standard passages for domain adaptation.","['Retrieval Augmented Generation (RAG)', 'Asynchronous Updates', 'REALM']","['Domain adaptation for Retrieval Augmented Generation models', 'Training neural retrievers for domain-specific retrieval tasks', 'Improving performance on in-domain datasets by dynamically updating knowledge representations']",,3,
121,Auxiliary Statement Reconstruction Signal,"Retrieval Augmented Generation (RAG) models, especially during domain adaptation, may not deeply learn domain-specific factual knowledge or effectively leverage retrieved passages to generate precise, grounded statements. There is a need for an additional training signal to inject more domain-specific understanding and improve the model's ability to synthesize information accurately from retrieved contexts.","Training Retrieval Augmented Generation (RAG) models, particularly in domain adaptation scenarios, where the goal is to enhance the model's factual grounding, reduce hallucinations, and improve the retriever's ability to find passages highly relevant for generating coherent and accurate statements about documents.","Incorporates a secondary, auxiliary training task alongside the primary task (e.g., question answering). This auxiliary task involves feeding the model an input statement (e.g., a sentence from a document abstract or a summary) and requiring it to reconstruct this statement using *only* a self-retrieved set of passages from the external knowledge base. A unique control token (e.g., '<p>') is prepended to the retrieved passages to explicitly differentiate this task from the main one. The input statement itself must be excluded from the retrieved passages to prevent simple memorization or overfitting on lexical content.","Forces the model to gain more domain-specific knowledge by learning to synthesize and reconstruct information from retrieved contexts. This improves both the retriever's accuracy in finding relevant passages and the overall answer generation accuracy, contributing to better factual consistency and reduced hallucinations in the generated outputs. It can moderately improve performance even without full end-to-end retriever training.","['Multi-task Learning', 'Retrieval Augmented Generation (RAG)']","['Enhancing domain adaptation of generative AI models', 'Improving factual consistency and reducing hallucinations in generative models', 'Strengthening the relevance capabilities of the retriever component', 'Pre-training or finetuning for summarization-like tasks']",,3,
122,Online Reinforcement Learning for LLM Grounding (GLAM),"Large Language Models (LLMs) often suffer from a lack of functional grounding in interactive environments. Their abstract knowledge, derived from text corpora, can be misaligned with the environment's physics and dynamics, limiting their functional competence and ability to solve tasks through direct interaction. This is due to training processes not incentivized for problem-solving in an environment, and a lack of ability to identify causal structures or learn from interaction data.","Interactive environments, especially textual worlds, where agents perceive observations and execute actions through natural language. The goal is to enable LLMs to act as agent policies, incrementally grounding and updating their knowledge with new observations collected through interaction.","Utilize a pre-trained LLM as the agent's policy. Progressively update the LLM's parameters using online Reinforcement Learning (RL), such as Proximal Policy Optimization (PPO), as the agent interacts with the environment. The LLM receives environmental observations and goal descriptions as input, selects actions, and uses the resulting rewards to finetune its policy, thereby functionally grounding its internal symbols to external dynamics.","Drastically improved performance in solving RL tasks, boosted sample efficiency compared to zero-shot use or supervised finetuning, and enhanced generalization abilities to new objects and certain new tasks. It enables LLMs to escape the 'Tabula Rasa' RL setting by leveraging their prior knowledge.","['Reinforcement Learning from Human Feedback (RLHF)', 'LLM as Probabilistic Policy', 'PPO Finetuning for LLM Policies', 'LLM as High-Level Planner']","['Solving decision-making problems in interactive textual environments', 'Learning spatial and navigation tasks', 'Object manipulation tasks', 'Sequential reasoning problems in language-conditioned settings']",,48,
123,LLM as High-Level Planner,"LLMs possess impressive abstract reasoning and planning capabilities, but they are not inherently grounded in the physical world or capable of executing low-level, embodied actions directly. This creates a gap between high-level plans generated by an LLM and the fine-grained actions required in interactive environments, particularly in robotics.","Embodied AI systems, such as robots, or complex textual environments where an agent needs to perform a sequence of low-level actions to achieve a high-level goal. The LLM's role is to provide strategic guidance rather than direct control.","Employ an LLM to generate high-level plans, action sequences, or sub-goals. An external, more grounded component (e.g., a low-level policy, an affordance function, or a dedicated 'actor' agent) then interprets, executes, or reranks these suggestions. Feedback from the environment or a 'reporter' observing the environment can be fed back to the LLM to refine its planning.","Leverages the LLM's prior knowledge and reasoning for complex tasks, allowing it to guide agent behavior effectively. However, it is often limited by the absence of direct grounding, requiring careful integration with environment-specific mechanisms.","['Online Reinforcement Learning for LLM Grounding (GLAM)', 'Prompt Template for Agent State/Goal Representation']","['Robotics for task planning (e.g., SayCan, Code as Policies, Inner Monologue)', 'Textual adventure games where LLMs plan actions for an agent', 'Complex decision-making scenarios requiring abstract strategic thinking']",,8,
124,Reinforcement Learning from Human Feedback (RLHF),"Aligning the outputs of large language models, particularly in natural language generation tasks, with subjective and nuanced human preferences, values, and instructions. Crafting a precise programmatic reward function for such alignment is often impractical or impossible.","Natural language generation tasks (e.g., summarization, dialogue, creative writing) where the quality, helpfulness, or safety of the generated text is best judged by humans. Text generation is viewed as a sequential decision-making process.","Collect a dataset of human preferences by having humans compare or rate different LLM-generated outputs. Train a separate 'reward model' (RM) on this human preference data to predict human-aligned rewards. Then, use a Reinforcement Learning algorithm (commonly PPO) to finetune the LLM's policy directly, using the learned reward model to provide a dense reward signal for each generated token or sequence, thereby optimizing the LLM to produce outputs that score highly with the RM.","LLMs generate more human-aligned, helpful, honest, and harmless outputs, often achieving better alignment with fewer parameters compared to models trained solely on next-token prediction.","['PPO Finetuning for LLM Policies', 'Online Reinforcement Learning for LLM Grounding (GLAM)']","['Improving chatbots (e.g., InstructGPT)', 'Summarization tasks', 'Dialogue systems', 'Any text generation task requiring fine-grained alignment with human values and preferences']",,12,
125,LLM as Probabilistic Policy,"How to effectively use a pre-trained Large Language Model, which is primarily designed for next-token prediction, as an agent's policy to select discrete actions from a predefined set in an interactive environment, especially when actions are described textually.",Interactive environments where actions are represented as sequences of tokens (textual commands). The LLM needs to output a probability distribution over these possible actions to enable decision-making and learning via RL.,"For each possible action (represented as a sequence of tokens, e.g., 'go forward'), compute its conditional probability given the current prompt (which includes the observation and goal) by multiplying the conditional probabilities of its constituent tokens, as calculated by the LLM's language modeling heads. These raw probabilities (or log probabilities) are then normalized (e.g., using a softmax function) across all possible actions to obtain a valid probability distribution from which an action can be sampled. This method can optionally use a variable temperature to address issues with very small probabilities.","Leverages the LLM's inherent language modeling capabilities and extensive pre-trained knowledge for action selection. It avoids the need for ad-hoc mappings from generated text to actions or the addition of separate, randomly initialized action heads. It is robust to any action space that can be represented textually.","['Online Reinforcement Learning for LLM Grounding (GLAM)', 'PPO Finetuning for LLM Policies', 'Prompt Template for Agent State/Goal Representation']","['Textual adventure games', 'Language-conditioned control tasks', 'Any interactive environment where actions are natural language commands and LLMs are used as policies']",,25,
126,PPO Finetuning for LLM Policies,"Efficiently adapting large pre-trained language models (LLMs) to serve as effective policies in reinforcement learning tasks, especially when the reward signal is sparse or originates from an external environment, and simultaneously learning a robust value function.","Reinforcement Learning settings where an LLM is used as an agent's policy. The goal is to optimize the LLM's behavior to maximize cumulative rewards through interaction. LLMs are computationally intensive, making training challenging.","Implement the Proximal Policy Optimization (PPO) algorithm to update the LLM's parameters. The LLM's language modeling heads (or a dedicated action head) are used to compute the probability distribution over actions. To learn a value function, an additional value head (e.g., a MultiLayer Perceptron) is added on top of the LLM's final layers. Gradients are backpropagated through both the LLM's policy and its value head, allowing for concurrent optimization of action selection and value estimation.",Improves the LLM's ability to learn optimal policies and value functions in interactive environments. It leverages the LLM's pre-trained knowledge for better sample efficiency and faster adaptation compared to training from scratch.,"['Online Reinforcement Learning for LLM Grounding (GLAM)', 'Reinforcement Learning from Human Feedback (RLHF)', 'LLM as Probabilistic Policy', 'Distributed Inference and Training for LLM Agents']","['Training LLM agents for interactive tasks (e.g., navigation, object manipulation)', 'Language-conditioned control', 'Policy learning in textual environments', 'Fine-tuning foundation models for specific RL objectives']",,48,
127,Distributed Inference and Training for LLM Agents,"The computational expense and memory footprint of using very large LLMs as agent policies make online reinforcement learning (RL) intractable. Specifically, frequent forward passes for action probability computation for potentially many actions, combined with the need for numerous environment interactions, create significant bottlenecks for both inference and training.","Online RL scenarios involving LLM agents, where the LLM is large (hundreds of millions to billions of parameters), the action space might be extensive, and a high volume of environmental interactions is required for learning.","Employ a distributed architecture where multiple LLM 'workers' are deployed in parallel. For inference, distribute the task of computing action probabilities across these workers, with each worker handling a subset of actions, achieving quasi-linear speedup. For training, utilize distributed data parallelism, where forward and backward passes for minibatches are dispatched across different LLM instances, and gradients are gathered and synchronized before updating the models. A library (e.g., Lamorel) can abstract these distributed operations.","Overcomes computational bottlenecks, enabling large-scale online RL finetuning of LLMs. Allows for the use of larger LLMs and more complex environments, making research and practical deployment of LLM-based agents feasible.","['PPO Finetuning for LLM Policies', 'MLOps Patterns (specifically distributed training/inference)', 'Tools Integration Patterns']","['Scaling online RL finetuning of LLMs (e.g., FlanT5 780M, FlanT5 3B)', 'Enabling research on the impact of LLM size and action space on learning', 'Deploying LLM-based agents in environments requiring fast, frequent decisions']",,34,
128,Prompt Template for Agent State/Goal Representation,"Effectively conveying the current state of an interactive environment, the agent's goal, and the available actions in a structured, coherent manner to a Large Language Model so it can accurately interpret the context and generate appropriate actions or responses.",Using LLMs as agents in textual or language-conditioned environments where all relevant information for decision-making must be encapsulated within a single textual prompt. The LLM's performance is highly sensitive to prompt structure.,"Design a standardized and structured prompt template that concatenates various pieces of information. This typically includes: 1) A header listing all possible actions, 2) The explicit goal description for the agent, 3) A short-term memory component (e.g., descriptions of the last few observations and actions), 4) A detailed description of the current observation, and 5) A clear indication for where the LLM should 'fill in' the next action. While finetuning can help the LLM adapt, a well-designed template can significantly improve initial performance.","Provides the LLM with a comprehensive and consistent input, allowing it to better understand the task context, current situation, and available choices. This structured input facilitates more accurate and relevant action selection by the LLM.","['Prompt Design Patterns', 'LLM as Probabilistic Policy', 'Online Reinforcement Learning for LLM Grounding (GLAM)']","['LLM-based agents in text-based games (e.g., BabyAIText)', 'Language-conditioned control tasks', 'Any interactive system where LLMs act as decision-makers based on textual context']",,25,
129,Pretraining with Behavioral Cloning (then Finetuning with RL),"Training an agent policy efficiently in complex environments. While expert demonstrations can provide a strong initial policy, they might not cover all scenarios, lead to optimal performance, or allow for adaptation to new dynamics. Pure online RL from scratch can be sample-inefficient.","Reinforcement Learning settings where a dataset of expert trajectories or demonstrations is available, but the ultimate goal is to achieve performance beyond the demonstrations, or to adapt to a dynamic environment through interaction. Applicable to various policy architectures, including Transformer models and LLMs.","First, pretrain the agent's policy using Behavioral Cloning (BC) or Offline Reinforcement Learning on a dataset of expert trajectories. This initial phase leverages existing knowledge to provide a strong, stable starting point. Subsequently, the pre-trained model is finetuned using online Reinforcement Learning (RL) by allowing the agent to interact with the environment, collect its own experiences, and optimize its policy based on environmental rewards.","Combines the benefits of learning from demonstrations (e.g., improved sample efficiency, a good initialization) with the benefits of online RL (e.g., adaptability, potential for super-human performance, exploration). It can lead to better performance than BC alone, as direct interaction allows for crucial grounding.","['Online Reinforcement Learning for LLM Grounding (GLAM)', 'PPO Finetuning for LLM Policies', 'MLOps Patterns (for data collection and management)']","['Robotics for learning skills from demonstrations and adapting to real-world variations', 'Embodied AI tasks', 'Training large language models for decision-making (e.g., Online Decision Transformer)', 'Any task where a combination of expert knowledge and environmental interaction is beneficial for policy learning']",,48,
130,InContext Retrieval-Augmented Language Model (InContext RALM),"Large Language Models (LLMs) often generate factually inaccurate text, lack source attribution, struggle to incorporate up-to-date information, and perform poorly in uncommon or private domains. Existing Retrieval-Augmented Language Modeling (RALM) approaches require modifying the LM architecture or dedicated retraining, complicating deployment and preventing use with frozen or API-accessed models.","When deploying or using pre-trained LLMs, especially via API access, where architectural modifications or further training are not feasible or desirable, but grounding the model with external, relevant knowledge is crucial for accuracy, recency, and trustworthiness.","Instead of modifying the LM architecture, pre-pend relevant grounding documents retrieved from an external knowledge source directly to the LLM's input text (context window). The LLM then processes this concatenated input (documents + original prefix) for generation without any internal architectural changes or further training.","Significantly improves LLM performance (e.g., perplexity), mitigates factual inaccuracies, provides a natural source attribution mechanism, and makes RALM widely applicable to off-the-shelf and API-accessed LMs, increasing the prevalence of LM grounding.","['Retrieval-Augmented Generation (RAG)', 'In-Context Learning']","['Generative AI Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns', 'Prompt Design Patterns']",,3,
131,Retrieval Stride Optimization,"In Retrieval-Augmented Language Models (RALMs), performing a retrieval operation at every generation step (i.e., for every token) can be computationally expensive due to the cost of calling the retriever and recomputing LM embeddings. This creates a trade-off between grounding frequency (performance) and runtime cost.","When designing a RALM system where the frequency of document retrieval impacts both the quality of grounding and the computational efficiency, and a balance between these factors is required.","Introduce a 'retrieval stride' (s), which defines the number of tokens generated between consecutive retrieval operations. Instead of retrieving for every token (s=1), retrieval is performed only once every `s` tokens, and the retrieved documents are then used for the subsequent `s` generation steps.","Allows for balancing runtime costs and performance. Smaller strides (more frequent retrieval) generally lead to better LM performance by providing higher-resolution grounding, while larger strides reduce computational overhead. An optimal stride (e.g., s=4 in the paper) can be found to balance these concerns.","['Latency Reduction', 'Cost Optimization']","['Generative AI Patterns', 'LLM-specific Patterns', 'MLOps Patterns']",,31,
132,Retrieval Query Length Optimization,"When constructing a query for a retriever in a Retrieval-Augmented Language Model (RALM), the length of the query (i.e., how many preceding tokens from the prefix are used) significantly impacts the relevance of the retrieved documents. Queries that are too short may lack sufficient context, while queries that are too long can dilute the importance of the most recent, and often most relevant, tokens for the current generation step.","When designing the query formulation for the retriever component in a RALM system, where the goal is to maximize the relevance of retrieved documents to the immediate generation task.","Restrict the retrieval query to a specific number of the most recent tokens from the LM's prefix (e.g., the last `l` tokens). This involves empirically determining an optimal query length (`l`) that captures enough context without introducing irrelevant or diluting information.","Leads to improved LM performance by ensuring that the retrieval query is optimally focused on the most relevant contextual information for the upcoming generation. An optimal length (e.g., 32 tokens for BM25, 64 tokens for dense retrievers) balances contextualization and recency.","['Context Window Management', 'Relevance Tuning']","['Generative AI Patterns', 'LLM-specific Patterns', 'Prompt Design Patterns']",,31,
133,Sparse Retriever Preference for InContext RALM,"Choosing an effective and efficient document retriever for InContext Retrieval-Augmented Language Models (RALMs), especially in zero-shot settings or when computational resources are limited, as dense neural retrievers can be computationally intensive and may not always yield superior performance.","When implementing InContext RALM, particularly in scenarios where the LM is off-the-shelf, no specific retriever training data is available for the task, or computational cost is a significant factor.","Utilize a sparse, lexical retriever (e.g., BM25) as the primary document selection mechanism. This involves leveraging keyword-based matching rather than dense vector representations.","The sparse retriever (BM25) can outperform more complex dense neural retrievers (e.g., BERT-based, Contriever, Spider) in zero-shot InContext RALM settings, while also being significantly cheaper to apply. This makes the overall InContext RALM system more accessible and cost-effective.","['Cost Optimization', 'Zero-Shot Learning']","['MLOps Patterns', 'LLM-specific Patterns', 'Tools Integration Patterns']",,3,
134,LM as Zero-Shot Reranker,"While an initial retriever (like BM25) can identify a set of potentially relevant documents, its lexical nature may limit its ability to semantically understand the query and prioritize the most useful document for the LLM's generation. Training a dedicated reranker is not always feasible (e.g., no training data, API-only LM, time constraints).","When an InContext RALM system needs to improve the selection of the most relevant document from a candidate set (e.g., top-k from an initial retriever) without requiring specific training for the reranking task, or when the main LLM is accessed via API and its internal log-probabilities are not directly accessible for fine-tuning.","Use an off-the-shelf Language Model (LM), potentially a smaller or a different model than the main generation LM, to perform zero-shot reranking. For each candidate document `d_i` and a short segment of the prefix `y'` (representing the upcoming text or a relevant part of the query), the LM computes `p(y' | d_i, prefix)`. The document that maximizes this probability is selected.","Consistently better document selection and improved LM performance compared to using the initial retriever's top result alone. This method is highly flexible as it can leverage existing LMs, including smaller ones for efficiency, and works even when the main LM is only accessible via API (as only forward passes for scoring are needed).","['Zero-Shot Learning', 'API Integration']","['Generative AI Patterns', 'LLM-specific Patterns', 'Prompt Design Patterns', 'Knowledge & Reasoning Patterns', 'Tools Integration Patterns']",,20,
135,Predictive Reranking (Trained LM-Dedicated Reranker),"To achieve the highest possible performance in document selection for InContext Retrieval-Augmented Language Models (RALMs), generic rerankers or zero-shot LM rerankers might not fully capture the nuances of what makes a document most 'predictive' for the LLM's upcoming generation in a specific domain or task.","When maximum document selection performance is required for InContext RALM, and domain-specific training data is available to specialize a reranking component. This is often applicable where the underlying LM is frozen, but the retrieval/reranking pipeline can be optimized.","Train a dedicated reranker (e.g., a fine-tuned RoBERTa-base classifier) to predict the relevance of a document `d_i` for predicting the upcoming text `y` given the current prefix `x_sj`. Training examples are generated by sampling prefixes and their upcoming text `y` from training data, retrieving candidate documents, and using the main LM's `p(y | d_i, x_sj)` as a target signal for relevance. The reranker then learns to output scores that resemble these probabilities.","Leads to significant gains in LM performance, outperforming both initial retrievers and zero-shot LM rerankers. This method allows for domain-specific optimization of document selection, fully leveraging available training data to tailor the reranker to the exact needs of the RALM task.","['Supervised Fine-Tuning', 'Domain Adaptation']","['Generative AI Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns', 'MLOps Patterns']",,20,
136,Tool-Augmented LLM (API-Agent LLM),"Large Language Models (LLMs) struggle to effectively use external tools via API calls due to unawareness of available APIs, how to use them, and frequently updated tool sets, leading to unfulfilled potential and hallucination.","An LLM needs to interact with a vast, dynamic, and potentially overlapping set of external tools or services, often exposed through APIs, to perform complex, real-world tasks that go beyond its internal knowledge or capabilities. The LLM must generate accurate, actionable API calls based on user instructions and available documentation.","Develop a specialized LLM (e.g., Gorilla, a finetuned LLaMA model) that is explicitly trained to understand user instructions and generate correct API calls. This involves finetuning the LLM on a dataset of instruction-API pairs and integrating it with a retrieval system that provides relevant, up-to-date API documentation at inference time. The LLM learns to parse user queries, reason about constraints, and select appropriate APIs.","The LLM can accurately select and invoke APIs from a large, changing set of tools, significantly reducing hallucination errors and adapting to test-time documentation changes. It can also reason about user-defined constraints (e.g., performance, parameters) when choosing APIs, increasing the reliability and applicability of its outputs.","['Retriever-Aware Training', 'Self-Instruct Finetuning for Tool Use', 'Instruction Finetuning', 'Dynamic API Documentation Adaptation', 'Constraint-Aware API Invocation']","['Program synthesis', 'Automating tasks requiring external service interaction', 'Enhancing conversational agents with computational capabilities', 'Dynamic knowledge base access', 'Integrating LLMs with massive API ecosystems']",,49,
137,Retriever-Aware Training (RAT),"LLMs struggle to adapt to frequently updated external knowledge bases (like API documentation) and can be negatively impacted by irrelevant or inaccurate information provided by a retriever at inference time, leading to reduced accuracy and increased hallucination.","An LLM needs to leverage external, dynamic documentation (retrieved information) during inference to perform tasks, but the retrieval mechanism is imperfect, providing potentially irrelevant or outdated information. The LLM must learn to effectively utilize relevant retrieved information while ignoring irrelevant context.","During instruction finetuning, augment the user prompt with retrieved documentation (even if potentially incorrect, reflecting real-world retriever imperfections) while providing the accurate ground-truth response. This teaches the LLM to 'judge' the retriever at inference time: to use the documentation when relevant and accurate, and to rely on its baked-in domain knowledge when the retrieved information is irrelevant or misleading.","The LLM gains the ability to dynamically adapt to test-time changes in documentation, improves performance over standard in-context learning, and substantially reduces hallucination errors by intelligently processing retrieved context.","['Tool-Augmented LLM', 'Instruction Finetuning', 'Self-Instruct Finetuning for Tool Use', 'Dynamic API Documentation Adaptation']","['LLMs requiring up-to-date external knowledge', 'Dynamic tool usage', 'Factual grounding of LLM responses', 'Mitigating context-induced hallucination', 'Adapting LLMs to evolving information sources']",,3,
138,Self-Instruct Finetuning for Tool Use,"Training LLMs to accurately generate API calls requires a large, high-quality dataset of natural language instructions paired with correct API invocations, which is labor-intensive and difficult to scale, especially for a vast and dynamic API ecosystem.","A base LLM needs to be adapted to a specific domain (e.g., API generation) or task (e.g., tool use) where extensive human-labeled instruction-response pairs are unavailable or costly to create. The target task involves complex structured outputs (like API calls) based on natural language input.","Leverage an existing powerful LLM (e.g., GPT-4) to automatically generate synthetic instruction-API pairs. This involves providing the LLM with a few in-context examples (seed data) and reference documentation, then instructing it to generate diverse real-world use cases and their corresponding API calls. The generated synthetic data is then used to instruction-finetune a smaller, target LLM.","Efficiently creates a large and diverse training dataset for specialized LLM tasks, reducing reliance on manual annotation. The finetuned LLM learns to follow instructions and generate domain-specific structured outputs, such as accurate API calls, without directly memorizing API names or hints from the instructions.","['Instruction Finetuning', 'Tool-Augmented LLM', 'Prompt Engineering (for generating instructions)']","['Adapting LLMs to new domains', 'Generating synthetic training data for specialized tasks', 'Building specialized LLMs for code generation', 'API invocation', 'Structured data output']",,49,
139,AST-based Hallucination and Accuracy Metric,"Evaluating the correctness and factual accuracy (lack of hallucination) of LLM-generated code or API calls is challenging. Simple string matching is insufficient due to syntactic variations, multiple correct solutions, and the need to distinguish between functional errors and outright fabrication (hallucination). Manual execution is time-consuming and resource-intensive.","An LLM is designed to generate structured code or API calls, and there's a need for an automated, robust, and semantically-aware evaluation method that can differentiate between correct, incorrect, and hallucinatory outputs. The target domain (e.g., API invocation) has well-defined syntax and a known set of valid constructs.","Employ Abstract Syntax Tree (AST) subtree matching to compare generated API calls against a database of known, valid API definitions. Define: 
- **Accuracy:** A generated API call whose AST is a subtree match of a reference API in the database (considering required arguments).
- **Error:** A generated API call that matches a known API but uses incorrect arguments or structure (not a valid subtree match beyond the API name).
- **Hallucination:** A generated API call whose AST is not a subtree of any API in the database, indicating an entirely imagined or fabricated tool.","Provides a precise, automated, and semantically grounded metric for evaluating LLM-generated code/API calls. It accurately distinguishes between functional errors and hallucination, correlates strongly with human judgment, and significantly reduces the need for manual code execution or validation.","['MLOps for Code Generation', 'LLM Evaluation']","['Automated testing of code-generating LLMs', 'Benchmarking API invocation models', 'Identifying and mitigating hallucination in structured output generation', 'Continuous integration/deployment for LLM-powered code assistants']",,49,
140,Constraint-Aware API Invocation,"LLMs, when generating API calls, typically focus on functional correctness but often fail to consider or reason about additional user-defined constraints (e.g., performance, resource limits, specific model properties) that are critical for real-world application, leading to suboptimal or unusable API selections.","Users provide natural language instructions for API usage that include both functional requirements and non-functional constraints (e.g., 'model with less than 10M parameters and 70% accuracy'). The LLM needs to select an API that satisfies all specified criteria, potentially involving trade-offs or complex reasoning over API metadata.","Incorporate instructions with explicit constraints into the LLM's training dataset (e.g., via self-instruct finetuning). This trains the LLM to parse and reason about these constraints alongside functional descriptions. During inference, the LLM processes the user's prompt, potentially augmented with retrieved API documentation containing constraint-related metadata (like model parameters, accuracy, latency), and selects an API that best fits the combined functional and non-functional requirements.","The LLM can accurately interpret and respond to complex user queries that involve multiple constraints, selecting APIs that are not just functionally correct but also meet specific performance or resource criteria. This significantly enhances the practical utility and applicability of LLM-generated API calls in real-world scenarios.","['Tool-Augmented LLM', 'Retriever-Aware Training', 'Self-Instruct Finetuning for Tool Use']","['Intelligent assistants for developers', 'Automated resource provisioning', 'Optimizing API selection based on application requirements', 'Personalized tool recommendations', 'Decision-making systems for complex API ecosystems']",,49,
141,Dynamic API Documentation Adaptation,"The utility and reliability of LLMs that use external APIs degrade rapidly as underlying API documentation changes (e.g., version upgrades, argument modifications, new registries). Retraining LLMs to keep pace with these frequent updates is impractical and costly.","An LLM relies on external API documentation to generate correct API calls. This documentation is subject to frequent, unpredictable changes that outpace the LLM's training cycle, making the LLM brittle to information shifts.","Implement a mechanism (e.g., Retriever-Aware Training) that trains the LLM to leverage retrieved, up-to-date API documentation at inference time. The LLM learns to interpret and incorporate information from the latest documentation, even if it differs from the documentation seen during its initial training, and to reason about the relevance of retrieved documents.","The LLM maintains its accuracy and relevance over extended periods without constant retraining, dynamically adapting to changes in API versions, arguments, and even preferred API sources. This ensures the LLM remains effective despite the rapid evolution of the external tool ecosystem.","['Retriever-Aware Training', 'Tool-Augmented LLM']","['LLMs interacting with rapidly evolving software libraries', 'Cloud services', 'Any external system with frequently updated interfaces', 'Maintaining long-term relevance of LLM-powered tools']",,49,
142,Instruction Finetuning,"General-purpose pre-trained LLMs often struggle to follow specific instructions or generate desired structured outputs for specialized tasks, leading to suboptimal performance, poor adherence to formats, or unhelpful responses.","A large pre-trained language model possesses vast knowledge but lacks the ability to consistently perform a specific downstream task (e.g., API call generation, summarization, question answering) according to explicit user instructions or desired output formats.","Further train the pre-trained LLM on a dataset of instruction-response pairs, where each input is a natural language instruction (and potentially context) and the output is the desired response. This process adapts the model to better understand and follow instructions, aligning its behavior with the target task's requirements. The data can be human-labeled or synthetically generated (e.g., using self-instruct).","The LLM becomes significantly more adept at following instructions, generating task-specific outputs, and adhering to desired formats, leading to improved performance on the target task compared to zero-shot or few-shot prompting of the base model.","['Self-Instruct Finetuning for Tool Use', 'Prompt Engineering', 'Retriever-Aware Training']","['Adapting LLMs to specific applications', 'Improving task-specific accuracy', 'Aligning LLMs with user intent', 'Reducing the need for extensive prompt engineering at inference time', 'Building specialized models from general foundation models']",,25,
143,LLM-KG Interactive Reasoning Paradigm (LLM <-> KG),"Large Language Models (LLMs) suffer from hallucination problems, struggle with deep and responsible reasoning (especially with specialized, out-of-date, or multi-hop knowledge), and lack explainability/transparency. Training LLMs for knowledge updates is expensive and time-consuming. Existing LLM-KG integrations (LLM -> KG) are loosely coupled, with LLMs acting only as translators, and their success depends heavily on the completeness and high quality of the Knowledge Graph (KG).","AI systems requiring LLMs to answer complex questions that demand accurate, traceable, and up-to-date knowledge, leveraging structured external knowledge from KGs. The LLM needs to be an active participant in the reasoning process on the KG, rather than just translating queries.","Establish a tight-coupling between the LLM and the KG, where the LLM acts as an agent that interactively explores related entities and relations on the KG. The LLM dynamically makes decisions at each step of the graph reasoning process, retrieving relevant knowledge, and performing reasoning based on this retrieved information. This paradigm moves beyond simple prompt augmentation by enabling the LLM to actively navigate and reason over the KG in an iterative fashion (e.g., ThinkonGraph framework).","Enhanced deep reasoning capabilities, mitigation of hallucination, improved knowledge traceability and correctability, and the ability for LLMs to generate correct answers with reliable, retrieved knowledge. It allows LLMs and KGs to complement each other's strengths, leading to better overall performance in knowledge-intensive tasks.","['Retrieval-Augmented Generation (RAG)', 'Agentic AI Patterns', 'Knowledge & Reasoning Patterns', 'LLM-specific Patterns', 'LLM -> KG (Loose Coupling) - contrasting pattern']","['Complex Knowledge Base Question Answering (KBQA)', 'Open-domain Question Answering (QA)', 'Fact-checking', 'Slot filling', 'Any application where LLMs need to reason over and interact with structured external knowledge with high accuracy and explainability.']",,15,
144,ThinkonGraph (ToG) Framework,"Implementing the LLM-KG Interactive Reasoning Paradigm effectively requires a structured algorithmic approach that allows LLMs to perform iterative, dynamic exploration and reasoning on KGs to address issues like hallucination and limited deep reasoning, without requiring additional training.","An LLM-KG Interactive Reasoning Paradigm is adopted, and a concrete, training-free algorithmic framework is needed to enable the LLM to act as an agent, iteratively exploring KGs and making reasoning decisions to solve knowledge-intensive tasks.","ToG implements the LLM <-> KG paradigm by prompting an LLM to perform iterative beam search on a Knowledge Graph. It constantly updates and maintains top-N reasoning paths for a given question. The entire inference process contains three main phases:
1.  **Initialization:** The LLM extracts initial topic entities from the question to start the reasoning paths.
2.  **Exploration:** The LLM iteratively identifies the most relevant relations and entities by performing 'Search' and 'Prune' steps on the KG. This involves a two-step strategy for both relation exploration and entity exploration, where the LLM acts as an agent to select promising candidates.
3.  **Reasoning:** The LLM evaluates whether the current reasoning paths are adequate to answer the question. If positive, it generates the answer; otherwise, it repeats the Exploration step or falls back to its inherent knowledge if the maximum search depth is reached.","Provides a robust, training-free method for deep, responsible, and efficient LLM reasoning. Achieves state-of-the-art (SOTA) performance in many knowledge-intensive tasks, enhances LLM's deep reasoning capabilities, and offers a flexible plug-and-play framework. It can enable smaller LLMs to be competitive with larger ones, reducing deployment costs.","['LLM-KG Interactive Reasoning Paradigm', 'LLM-Guided Beam Search for KG Exploration and Pruning', 'Knowledge Traceability and Correctability with KG Feedback', 'Plug-and-Play LLM-KG Integration', 'Agentic AI Patterns', 'Planning Patterns', 'Prompt Design Patterns']","['Multi-hop Knowledge Base Question Answering (KBQA)', 'Open-domain Question Answering (QA)', 'Slot filling', 'Fact-checking', 'Enhancing the reasoning ability of small LLMs', 'Any task requiring deep, responsible, and verifiable reasoning over KGs with LLMs.']",,15,
145,LLM-Guided Beam Search for KG Exploration and Pruning,"Efficiently navigating large and complex Knowledge Graphs (KGs) for multi-hop reasoning requires an intelligent search strategy that can dynamically identify and filter relevant paths. Traditional beam search often relies on simpler heuristics or lightweight similarity metrics, which may not capture the complex semantic relevance or contextual nuances needed for accurate LLM-based reasoning, leading to suboptimal or irrelevant paths.","An LLM is acting as an agent to explore a Knowledge Graph for relevant information to answer a complex natural language question. The exploration involves iteratively extending reasoning paths, and at each step, there's a need to intelligently select the most promising paths from a multitude of candidates.","Integrate the LLM directly into the beam search algorithm to guide both the exploration ('Search') and selection ('Prune') of reasoning paths. At each iteration, the LLM performs the following roles:
1.  **Relation Search & Prune:** Searches for all candidate relations linked to current tail entities. The LLM then, through prompting, evaluates and scores these candidates based on their relevance to the original question and current paths, selecting the top-N most promising relations.
2.  **Entity Search & Prune:** Using the pruned relations, searches for candidate tail entities. The LLM again, through prompting, evaluates and scores these entities based on their contribution to the question, selecting the top-N most promising entities.
This iterative, LLM-driven process ensures that the beam search is semantically aware and contextually relevant, dynamically focusing on pertinent information.","More accurate and focused discovery of multi-hop reasoning paths compared to naive beam search or methods using lightweight pruning tools (e.g., BM25, SentenceBERT). Improves the deep reasoning capability of LLMs by providing highly relevant structured knowledge. Partially mitigates calibration error accumulation by considering top-N paths rather than just a single best path.","['Agentic AI Patterns', 'Planning Patterns', 'Prompt Design Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns', 'ThinkonGraph (ToG) Framework']","['Enhancing the precision and effectiveness of knowledge graph traversal for complex questions', 'Constructing high-quality, semantically relevant reasoning paths', 'Optimizing search in large KGs by intelligently reducing the search space', 'Improving the accuracy of multi-hop knowledge base question answering.']",,24,
146,Knowledge Traceability and Correctability with KG Feedback,"LLM-generated answers, even when augmented with KGs, can still contain errors (e.g., hallucinations, outdated or incorrect information from the KG). The lack of transparency in reasoning makes it difficult to diagnose and correct these errors, undermining trust and system reliability. There's a need for a mechanism to verify the provenance of answers and provide a feedback loop for knowledge base improvement.","An LLM-KG system generates answers by constructing explicit, step-by-step reasoning paths (e.g., sequences of KG triples). Users, experts, or even other AI systems need to understand how an answer was derived and have the ability to rectify errors or improve the underlying knowledge.","The system is designed to explicitly display the full, explicit reasoning paths (provenance) used by the LLM to derive an answer. If an error or uncertainty is detected in the LLM's output (by a human user/expert or another LLM), the system enables:
1.  **Traceability:** Users can trace back along the displayed reasoning path to pinpoint the specific triples or knowledge elements from the KG that contributed to the erroneous conclusion.
2.  **Correctability:** Armed with this insight, users can provide feedback to correct the identified suspicious or incorrect triples directly within the Knowledge Graph. This correction then improves the KG's quality, which in turn benefits future LLM reasoning, forming a self-improving feedback loop (knowledge infusion).","Significantly enhances the explainability and transparency of LLM reasoning. Builds user trust by providing verifiable provenance for answers. Enables continuous improvement of the underlying Knowledge Graph, reducing the recurrence of errors and potentially lowering the cost of KG construction and correction. Contributes to more responsible AI systems.","['AI-Human Interaction Patterns', 'MLOps Patterns (for data quality, monitoring, and feedback loops)', 'Knowledge & Reasoning Patterns', 'Responsible AI Patterns', 'ThinkonGraph (ToG) Framework']","['Debugging LLM outputs and identifying sources of error', 'Ensuring factual accuracy and reliability in AI-generated content', 'Facilitating expert-in-the-loop systems for knowledge curation', 'Improving data quality in KGs', 'Building more trustworthy and accountable AI applications.']",,15,
147,Plug-and-Play LLM-KG Integration,"Developing LLM-KG systems often results in rigid, tightly coupled components that are difficult to adapt to new LLM models or different Knowledge Graph schemas/implementations. This limits flexibility, increases development and maintenance costs, and hinders the rapid adoption of new technologies or knowledge sources.","Building knowledge-enhanced LLM applications that require high adaptability and reusability across various LLM backbones (e.g., proprietary APIs like GPT-4, open-source models like Llama2) and diverse Knowledge Graphs (e.g., Freebase, Wikidata), without incurring significant retraining or re-engineering costs.","Design the LLM-KG system as a modular, training-free framework where the LLM's interaction with the KG is abstracted and standardized. The LLM communicates with the KG through predefined formal queries (e.g., SPARQL, service APIs) for knowledge retrieval and updates. The core logic of exploration, pruning, and reasoning is implemented via generalized prompting strategies that are independent of the specific LLM model or KG backend. This architectural decoupling allows for seamless swapping of LLMs or KGs without requiring additional training or complex architectural changes.","High flexibility, generality, and reusability of the LLM-KG system. Reduced development and deployment costs by eliminating the need for extensive retraining. Enables rapid experimentation with different LLMs and KGs to find optimal configurations. Allows for frequent knowledge updates directly in the KG, bypassing expensive LLM retraining. Empowers smaller, more efficient LLMs to achieve competitive performance by leveraging external knowledge.","['Tools Integration Patterns', 'MLOps Patterns (for deployment flexibility, cost reduction, and scalability)', 'LLM-specific Patterns', 'ThinkonGraph (ToG) Framework']","['Rapid development of knowledge-intensive LLM applications', 'Building adaptable Knowledge Base Question Answering (KBQA) systems', 'Creating flexible knowledge-enhanced chatbots and virtual assistants', 'Developing scalable AI applications that can leverage evolving LLM technologies and diverse knowledge bases efficiently.']",,15,
148,Retrieval Augmented Fine-Tuning (RAFT),"Pretrained Large Language Models (LLMs) struggle to effectively adapt to specialized domains for Retrieval Augmented Generation (RAG) tasks, especially when dealing with imperfect retrieval (i.e., the presence of distractor documents) and leveraging the learning opportunity afforded by a fixed domain. Existing finetuning methods either fail to incorporate RAG at test time or don't account for retrieval imperfections during training.","Deploying LLMs in specialized domains (e.g., legal, medical, enterprise documents, code repositories) where the primary goal is to maximize accuracy based on a given set of external documents. The LLM needs to perform robustly in an 'open-book' setting, even when retrieved documents contain irrelevant information.","A training recipe that finetunes LLMs using question-answer pairs while explicitly referencing a set of documents, which includes both 'golden' (relevant) and 'distractor' (irrelevant) documents. The model is trained to generate Chain-of-Thought style answers that verbatim cite the correct sequences from the relevant documents. Crucially, a portion (1-P) of the training data also includes questions with only distractor documents (no golden document) to compel the model to either memorize or explicitly state it cannot answer from the context.","Significantly improves the LLM's ability to read, extract, and reason with information from in-domain documents. Enhances robustness against distracting retrieved information, leading to consistent performance improvements across various domain-specific RAG benchmarks compared to standard supervised finetuning or general-purpose LLMs with RAG.","['Retrieval Augmented Generation (RAG)', 'Supervised Finetuning (SFT)', 'Chain-of-Thought Prompting', 'Training with Distractor Documents for RAG Robustness']","['Generative AI Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns']",,3,
149,Chain-of-Thought Prompting,"Large Language Models (LLMs) may provide superficial or incorrect answers, especially for complex reasoning tasks, and can overfit to concise answers during training, leading to reduced robustness and accuracy.","Training or prompting LLMs for tasks that benefit from step-by-step reasoning, explanation, or justification, such as question answering, where the model's understanding and ability to derive answers from context need to be enhanced.","Instruct the LLM to generate an explicit reasoning process or 'chain of thought' alongside its final answer. This reasoning often involves breaking down the problem, explaining intermediate steps, and, in the context of RAG, citing verbatim sources from the provided documents.","Improves the model's ability to reason, enhances overall accuracy, and increases training robustness by guiding the model's understanding. It helps prevent overfitting to direct, short answers and encourages deeper processing of information, especially when used with external contexts.","['Retrieval Augmented Fine-Tuning (RAFT)', 'Instruction Finetuning', 'Self-Instruct']","['Prompt Design Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns']",,16,
150,Training with Distractor Documents for RAG Robustness,"Large Language Models (LLMs) are vulnerable to irrelevant text when performing Retrieval Augmented Generation (RAG). If trained solely on highly relevant (golden) documents, they may lack the ability to effectively discern and disregard irrelevant information, leading to degraded performance when real-world retrievers provide noisy, top-k documents.",Finetuning LLMs for domain-specific RAG applications where the retriever is not perfect and may present a mix of relevant and irrelevant documents (distractors) to the LLM at inference time. The model needs to be robust to varying numbers of distractors.,"During finetuning, incorporate a training strategy where each data point includes a mix of golden (relevant) documents and purposefully selected distractor (irrelevant) documents. Experiment with varying proportions of golden vs. distractor documents. Additionally, include a fraction of training samples where the golden document is absent, compelling the model to either rely on memorized knowledge or explicitly state its inability to answer from the provided context.","Significantly enhances the LLM's ability to filter out irrelevant information and focus on pertinent content. Makes the model more resilient to fluctuations in the number of documents encountered during testing, leading to improved and consistent performance in real-world RAG scenarios with imperfect retrievers.","['Retrieval Augmented Fine-Tuning (RAFT)', 'Retrieval Augmented Generation (RAG)']","['LLM-specific Patterns', 'Generative AI Patterns']",,3,
151,Domain-Specific Supervised Finetuning,"General-purpose Large Language Models (LLMs), despite vast pretraining, often lack the specific knowledge, terminology, and desired output style required for optimal performance in specialized domains. They may also struggle to align with user preferences for domain-specific tasks.","Adapting a pretrained LLM for tasks within a specific, narrower domain (e.g., medical, legal, specific software APIs, enterprise documents) where domain expertise and a particular answering style are critical.","Apply standard supervised finetuning (SFT) to the pretrained LLM using a dataset specifically curated for the target domain. This dataset typically consists of Question-Answer (Q, A) pairs relevant to the domain. The training can be done without providing additional context documents during the finetuning phase (0-shot prompting).","Enables the LLM to learn and adopt the appropriate answering style, become familiar with domain context, and incorporate domain-specific knowledge into its parameters. This significantly enhances its performance on tasks within that specialized domain, often serving as a strong baseline or a prerequisite for further RAG-based improvements.","['Retrieval Augmented Generation (RAG)', 'Retrieval Augmented Fine-Tuning (RAFT)']","['LLM-specific Patterns', 'Generative AI Patterns', 'Knowledge & Reasoning Patterns']",,25,
152,Retrieval Augmented Generation (RAG),"Large Language Models (LLMs) are limited by their pre-training data cutoff, making them unable to access real-time, proprietary, or highly specialized information. This can lead to factual inaccuracies (hallucinations), outdated responses, or an inability to answer questions requiring external, dynamic knowledge.","Deploying LLMs in applications where responses must be grounded in up-to-date, authoritative, or private external knowledge sources, rather than solely relying on the model's internal parameters. Tasks include question answering, content generation, and fact-checking.","Integrate a retrieval module with the LLM. The retriever queries an external knowledge base (e.g., document store, database, web search engine) to fetch relevant documents or information snippets based on the user's query. These retrieved documents are then provided as context to the LLM, which uses this information to formulate its response.","Significantly enhances the factual accuracy and relevance of LLM-generated responses by grounding them in external, verifiable information. Reduces hallucinations, allows access to dynamic and proprietary data, and enables LLMs to answer questions beyond their pre-training knowledge.","['Retrieval Augmented Fine-Tuning (RAFT)', 'Domain-Specific Supervised Finetuning']","['Generative AI Patterns', 'LLM-specific Patterns', 'Tools Integration Patterns', 'Knowledge & Reasoning Patterns']",,3,
153,LLM Augmenter for Grounded Generation,"Large Language Models (LLMs) suffer from hallucinations, lack up-to-date external knowledge, become stale for time-sensitive tasks, and are prohibitively expensive to fine-tune for specific applications while needing to interact with them as black-box models.","Designing reliable AI systems for mission-critical applications that rely on LLMs, where factual accuracy, up-to-dateness, and domain-specific knowledge are crucial, and direct fine-tuning of large black-box LLMs is not feasible or desirable. The system needs to improve LLM responses without modifying the LLM's internal parameters.","Augment a fixed, black-box LLM with a set of plug-and-play modules that interact in an iterative loop. These modules include:
1.  **Knowledge Consolidator:** Retrieves and processes external knowledge relevant to the user query from diverse sources.
2.  **Prompt Engine:** Constructs prompts for the LLM, incorporating user query, dialog history, consolidated external knowledge, and automated feedback.
3.  **Utility Module:** Evaluates the LLM's candidate response against task-specific criteria (e.g., factuality, groundedness) and generates a utility score and verbalized feedback.
4.  **Policy:** Determines the next action (e.g., retrieve more knowledge, query LLM, revise prompt with feedback, send final response), potentially iteratively, to maximize an expected reward.
5.  **Working Memory:** Maintains the current state of the conversation, including query, evidence, candidate responses, utility scores, and feedback.
The system iteratively revises LLM prompts using automated feedback until a satisfactory response is generated.","Significantly reduces LLM hallucinations, improves factual grounding and informativeness, and enables LLMs to leverage external, up-to-date, and domain-specific knowledge without requiring expensive fine-tuning. Empirically validated to improve usefulness and humanness of responses in conversational AI and question answering.","['Retrieval-Augmented Generation (RAG)', 'Self-Correction', 'Agentic Loop', 'External Knowledge Integration', 'Iterative Prompting', 'LLM-specific Patterns']","['Building robust conversational AI agents (e.g., customer service chatbots)', 'Open-domain question answering systems', 'Information-seeking conversational AI', 'Any LLM application requiring high factual accuracy and dynamic knowledge retrieval without LLM fine-tuning.']",,49,
154,External Knowledge Consolidation for LLMs,"Raw external knowledge (e.g., retrieved documents, database entries) can be incomplete, noisy, irrelevant, or too disparate to be directly useful for grounding LLM responses, leading to poor performance or continued hallucinations. LLMs alone often cannot effectively process and synthesize diverse external information for complex reasoning.","Augmenting LLMs with external knowledge for tasks requiring factual grounding, up-to-dateness, or domain-specific information, where the raw retrieved data needs refinement and structuring before being fed to the LLM. This is especially critical for multi-hop reasoning tasks across heterogeneous knowledge sources.","Implement a modular 'Knowledge Consolidator' that acts as an intermediary between raw external knowledge sources and the LLM. This module typically involves:
1.  **Knowledge Retrieval:** Generates search queries and calls APIs (e.g., Bing Search, REST APIs) to fetch raw evidence from various external knowledge sources (web documents, Wikipedia articles, proprietary databases, FAQs, customer reviews).
2.  **Entity Linking:** Enriches raw evidence by identifying and linking entities mentioned in it to related contextual information (e.g., Wikipedia descriptions) to form evidence graphs.
3.  **Evidence Chaining and Pruning:** Prunes irrelevant evidence from the graphs and forms a shortlist of highly relevant and coherent 'evidence chains' that are most pertinent to the user query, potentially involving multi-hop reasoning. The output is consolidated, high-quality evidence used in LLM prompts.","Provides the LLM with structured, relevant, and enriched external knowledge, significantly improving its ability to generate grounded and factual responses and mitigate hallucinations. This is particularly effective for complex, multi-hop reasoning tasks where information is scattered across different modalities and documents.","['Retrieval-Augmented Generation (RAG)', 'Information Retrieval', 'Knowledge Graph Construction', 'Tools Integration Patterns', 'Knowledge & Reasoning Patterns']","['Enhancing LLM factuality in question answering', 'Supporting evidence-based response generation in dialog systems', 'Integrating diverse data sources into LLM workflows', 'Multi-hop reasoning tasks', 'Reducing hallucinations by providing structured context.']",,40,
155,Iterative Self-Correction via Automated Feedback Loop,"Initial LLM responses may suffer from hallucinations, lack of grounding, or failure to meet specific task requirements. Correcting these issues often requires multiple attempts or human intervention, which is inefficient, especially for black-box LLMs.","Applications where the quality and alignment of LLM outputs are critical, and there's a need to automatically refine responses iteratively based on objective criteria, without requiring human-in-the-loop for each revision. This is crucial when using fixed, black-box LLMs where direct fine-tuning is not an option.","Establish an iterative loop where:
1.  An LLM generates a candidate response based on a given prompt.
2.  A 'Utility Module' evaluates this candidate response against task-specific criteria (e.g., factuality score, adherence to rules, groundedness). 
3.  If the response is deemed unsatisfactory, the Utility Module generates verbalized, actionable feedback (e.g., 'The response is inconsistent with the knowledge. Please generate again.').
4.  This feedback is then incorporated into the original prompt, and the LLM is queried again for a revised response.
This process repeats until the response meets the desired utility threshold or a maximum number of iterations is reached.","Significantly improves the factual accuracy, groundedness, and overall quality of LLM responses by systematically guiding the LLM towards better outputs. Reduces hallucinations and enhances the alignment of the model's output with specific requirements through automated refinement. Achieves substantial improvements in metrics like KF1 scores.","['Self-Correction', 'Prompt Engineering', 'Agentic Loop', 'Feedback Loops', 'LLM-specific Patterns', 'Prompt Design Patterns']","['Refining LLM-generated text for factual accuracy, style, or adherence to constraints', 'Improving conversational coherence and groundedness', 'Reducing post-generation editing effort', 'Enhancing the reliability of generative AI systems, especially in mission-critical applications.']",,38,
156,Trainable Policy for Adaptive LLM Orchestration,"In complex agentic systems augmenting LLMs, deciding the optimal sequence of actions (e.g., when to retrieve knowledge, when to query the LLM, when to use feedback, when to terminate and respond) based on the current state can be challenging and difficult to hardcode with static rules, leading to suboptimal performance or inefficient resource use.","Designing AI agents that interact dynamically with LLMs, external tools, and the environment to achieve specific goals, where the optimal strategy depends on the evolving conversation state and aims to maximize a cumulative reward. This is particularly relevant when the costs of different actions (e.g., API calls, LLM queries) vary.","Implement a 'Policy module' that learns to select the most appropriate next system action given the current dialog state (tracked in 'Working Memory'). This policy can be:
1.  **Bootstrapped:** Starting with manually crafted rule-based policies encoding domain expertise and business logic.
2.  **Trained:** Using Reinforcement Learning (e.g., REINFORCE) on human-system interactions or simulated user interactions to maximize an expected reward (e.g., based on utility functions like KF1).
The policy's actions include acquiring evidence via the Knowledge Consolidator, querying the LLM via the Prompt Engine, or sending the final response to users after verification.","Enables the AI system to adaptively and optimally orchestrate its interactions with the LLM and its modules, leading to improved performance (e.g., better grounding, reduced hallucinations) and more efficient resource utilization (e.g., only retrieving knowledge when necessary). Demonstrates learning capability to effectively select actions that maximize reward over time.","['Agentic AI Patterns', 'Reinforcement Learning', 'Planning Patterns', 'Tools Integration Patterns', 'Classical AI (for RL)']","['Building goal-directed conversational agents', 'Multi-step reasoning systems', 'Optimizing multi-tool use in LLM agents', 'Dynamic resource allocation for AI tasks', 'Systems requiring adaptive decision-making in human-AI conversations.']",,11,
157,Utility-Function-Based Automated Feedback Generation,"Objectively evaluating the quality of LLM-generated responses and automatically generating actionable textual feedback for improvement is complex, especially for nuanced criteria like factual accuracy, style, or alignment with specific business rules, making manual feedback impractical for iterative processes.","Any AI system requiring automated assessment and guidance for LLM outputs, particularly within iterative refinement processes or for self-improvement mechanisms, where human evaluation is impractical, too slow, or too costly. This is critical for ensuring LLM outputs meet predefined quality standards.","Develop a 'Utility Module' that integrates various utility functions to assess LLM candidate responses. These functions can be:
1.  **Model-based Utility Functions:** Trained on human preference data or annotated logs to assign preference scores to different dimensions of a response (e.g., fluency, informativeness, factuality).
2.  **Rule-based Utility Functions:** Implemented using heuristics or programmed functions to measure whether a response complies with a specific rule or factual grounding (e.g., using Knowledge F1 score to measure overlap with external evidence).
Based on these assessments, a text generation model (e.g., a seq2seq LLM or a template-based natural language generator) is used to produce clear, verbalized feedback that can guide the LLM in subsequent generations (e.g., 'The response is inconsistent with the knowledge. Please generate again.').","Provides an automated, objective, and actionable mechanism for evaluating and guiding LLM generations, enabling iterative self-correction and alignment with complex requirements. Enhances the quality and groundedness of LLM outputs and fosters self-improvement capabilities within AI agents without constant human oversight.","['Self-Correction', 'Feedback Loops', 'Evaluation Metrics', 'Prompt Design Patterns', 'LLM-specific Patterns (for quality control)', 'Generative AI Patterns']","['Automated quality assurance for generative AI', 'Improving response grounding and factuality in LLM applications', 'Enhancing conversational AI systems through iterative refinement', 'Training AI agents with self-supervision', 'Prompt optimization and adaptation.']",,12,
158,Agent Customization,"How to design individual agents that are capable, reusable, and effective in multi-agent collaboration, and exhibit complex behavior tailored to specific application needs.","Building LLM applications where agents need specialized capabilities and roles, potentially leveraging LLMs, human inputs, or tools, and where modularity and reusability are important.","Configure agents with a mix of basic backend types (LLMs, humans, tools). Reuse or extend built-in agents (e.g., `ConversableAgent`, `AssistantAgent`, `UserProxyAgent`) to create agents with specialized capabilities and roles. This involves setting parameters like `human_input_mode`, `termination_condition`, `code_execution_config`, and `llm_config`.",Easy creation of agents with specialized capabilities and roles. Agents can exhibit complex behavior in multi-agent conversations. Maximizes reusability of implemented agents.,"['Agentic AI Patterns', 'Tools Integration Patterns', 'AI-Human Interaction Patterns', 'LLM-specific Patterns']","['Math Problem Solving (A1)', 'Retrieval-augmented QA (A2)', 'Decision Making in Embodied Agents (A3)', 'Supply Chain Optimization (A4)', 'Conversational Chess (A6)', 'Online Decision Making in Web Interaction Tasks (A7)']",,11,
159,Automated Agent Chat,"How to enable agents to automatically exchange messages and progress a conversation without explicit, centralized control, and to allow for custom agent behaviors.","Multi-agent conversation frameworks where agents need to interact autonomously and flexibly, leveraging the conversational capabilities of LLMs.","Provide unified conversation interfaces (`send/receive`, `generate_reply`, `register_reply`) and an agent autoreply mechanism. Agents automatically invoke `generate_reply` and send a reply unless a termination condition is met. Built-in reply functions (LLM inference, code/function execution, human input) are provided, and custom reply functions can be registered via `register_reply` to define specific behavior patterns.","Conversation flow is naturally induced and proceeds automatically. Decentralized, modular, and unified workflow definition. Enables diverse conversation patterns.","['Agentic AI Patterns', 'LLM-specific Patterns', 'Composable Conversation Patterns']","['General infrastructure for AutoGen applications', 'All general agent interactions within AutoGen (Figure 2)']",,11,
160,Hybrid Control,"How to manage and control the flow of multi-agent conversations effectively, allowing for both intuitive, high-level guidance and precise, programmatic specification.","Developing complex LLM applications where conversation flow needs to be managed with varying degrees of flexibility and precision, and where both human-like guidance and deterministic logic are required.","Allow control over conversation flow using a fusion of natural language (prompting LLM-backed agents) and programming language (Python code for specifying termination conditions, human input modes, tool execution logic, and custom autoreply functions). Support dynamic transitions between these control modes.","Flexible control flow management. Enables both high-level, human-readable guidance and detailed, programmatic control. Reduces development effort for complex workflows.","['Agentic AI Patterns', 'Prompt Design Patterns', 'Tools Integration Patterns', 'LLM-specific Patterns', 'AI-Human Interaction Patterns']","[""AssistantAgent's default system message (natural language control, Figure 5)"", 'Configuring `human_input_mode` or `max_auto_replies` (programming language control)', 'LLM-proposed function calls']",,11,
161,Composable Conversation Patterns,"How to design and implement diverse and flexible multi-agent conversation workflows, from simple fixed interactions to complex, dynamic collaborations.","Building multi-agent LLM applications that require various interaction structures depending on the task's complexity, the number of agents, and whether the flow is static or dynamic.","Provide high-level interfaces for common patterns (two-agent chat, sequential chats, nested chat, group chat). Allow these patterns to be composed recursively (e.g., nested chat with group chat) and extended via low-level interfaces (e.g., `register_reply`) or dynamic mechanisms (custom reply functions, LLM-driven function calls).","Supports diverse conversation patterns (static and dynamic). Enables creation of complex, creative workflows (e.g., inner monologue). Enhances usability for common patterns.","['Agentic AI Patterns', 'Planning Patterns', 'LLM-specific Patterns', 'Automated Agent Chat']","['Two-agent chat (A1, A2, A3, A7)', 'Sequential chats (Listing 5)', 'Nested chat (Listing 3 for self-reflection)', 'Group chat (A4, A5, A6)']",,11,
162,Grounding Agent,"LLM-based agents may lack basic commonsense knowledge, hallucinate, or get stuck in repetitive error loops, especially in interactive decision-making tasks in dynamic environments, leading to task failures.","Multi-agent systems where an agent's decisions require specific, often commonsense, knowledge about the environment or task rules, and where errors can lead to persistent failures or invalid actions.","Introduce a specialized 'grounding agent' that provides crucial commonsense knowledge, factual information, or corrective feedback to the decision-making agents. This agent is triggered when specific conditions are met (e.g., early signs of recurring errors, repeated actions, invalid moves).","Enhances the system's ability to avoid error loops, mitigates persistence with flawed plans, improves task success rate, and ensures adherence to rules/facts.","['Agentic AI Patterns', 'Knowledge & Reasoning Patterns', 'LLM-specific Patterns']","['Decision Making in Embodied Agents (A3, for commonsense knowledge)', 'Conversational Chess (A6, board agent for rule validation)']",,23,
163,Safeguard Agent,"Ensuring the safety, security, and reliability of code or actions generated by LLM agents, especially when interacting with external tools or sensitive systems (e.g., preventing malicious code, information leakage, or unsafe operations).","Multi-agent systems where LLM agents generate code or make decisions that could have security implications, require adherence to specific safety protocols, or involve critical operations.","Introduce a specialized 'safeguard agent' responsible for reviewing and verifying the safety, security, and correctness of outputs (e.g., code) from other agents before execution or deployment. This agent can act in an adversarial manner to scrutinize proposed solutions.","Boosts performance in identifying unsafe code, reduces risks, and ensures adherence to safety guidelines. Enhances reliability and trustworthiness of the system. Improves user experience by preventing errors.","['Agentic AI Patterns', 'MLOps Patterns', 'LLM-specific Patterns', 'Tools Integration Patterns']","['Supply Chain Optimization (A4, checking code safety)']",,23,
164,Role Play Prompting,"How to guide an LLM agent to adopt a specific persona, mindset, or behavior within a multi-agent conversation, influencing its responses, reasoning, and actions to align with a desired role.","Designing LLM-backed agents that need to fulfill distinct roles (e.g., assistant, critic, engineer) or exhibit specific conversational styles or reasoning processes in a collaborative environment.","Incorporate explicit role-playing instructions into the agent's system message or initial prompt, clearly defining its persona, responsibilities, and expected behavior.","Agent adopts the specified role, leading to more aligned, focused, and effective contributions to the multi-agent conversation. Can be used for dynamic speaker selection and fostering specific interaction dynamics.","['Prompt Design Patterns', 'Generative AI Patterns', 'Agentic AI Patterns']","[""AssistantAgent's default system message (Figure 5: 'You are a helpful AI assistant')"", 'Dynamic Task Solving with Group Chat (A5, for speaker selection)', ""CAMEL (related work, uses 'Inception prompting')""]",,5,
165,Control Flow Prompting,"How to use natural language instructions within prompts to influence the sequence of operations, decision-making logic, or overall flow of an LLM agent's behavior in a multi-agent system.","LLM agents need to follow specific steps, conditions, or iterative processes in their problem-solving, and developers prefer natural language for high-level guidance.","Embed instructions related to the conversation flow, step-by-step reasoning, planning, or conditional actions directly into the LLM's system message or prompt.","LLM agents follow the intended control flow, performing tasks in a structured manner, suggesting code, explaining plans, or debugging when needed.","['Prompt Design Patterns', 'Planning Patterns', 'Agentic AI Patterns']","[""AssistantAgent's default system message (Figure 5: 'Solve the task step by step if you need to. If a plan is not provided explain your plan first. Be clear which step uses code and which step uses your language skill. If the result indicates there is an error fix the error and output the code again')""]",,23,
166,Output Confinement Prompting,"How to constrain the output format or content of an LLM agent to make it easily consumable by other agents or external tools, and to improve system robustness and parsing reliability.","Multi-agent systems where agents exchange messages that need to adhere to specific structural or content requirements for successful automated processing by other agents, tools, or parsers.","Include explicit instructions in the LLM's prompt to confine its output to a particular format (e.g., Python coding block, shell script block, specific keywords, single code block per response, `TERMINATE` signal).","LLM outputs are structured and predictable, making it easier for other agents or tools to parse and act upon them. Reduces parsing errors and improves inter-agent communication and system stability.","['Prompt Design Patterns', 'LLM-specific Patterns', 'Tools Integration Patterns']","[""AssistantAgent's default system message (Figure 5: 'suggest python code in a python coding block or shell script in a sh coding block', 'Do not include multiple code blocks in one response', 'Reply TERMINATE in the end when everything is done')""]",,44,
167,Facilitate Automation Prompting,"How to design prompts that encourage LLM agents to produce outputs that directly enable or streamline automated processes, such as code execution or tool invocation, without manual intervention.","LLM agents are expected to interact with tools or execute code generated by themselves or other agents, and the system needs to automate these interactions efficiently.","Instruct the LLM agent to suggest executable code or function calls in a specific, parseable format, and to use `print` statements for output capture, rather than asking for manual copy-pasting or complex parsing.","Streamlines the automation of code execution and tool usage, reducing manual intervention and improving the efficiency of multi-agent workflows. Enhances the seamless integration of LLMs with external systems.","['Prompt Design Patterns', 'Tools Integration Patterns', 'Automated Agent Chat', 'LLM-specific Patterns']","[""AssistantAgent's default system message (Figure 5: 'When using code you must indicate the script type in the code block', 'The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user cant modify your code. So do not suggest incomplete code which requires users to modify', 'Instead use print function for the output when relevant')""]",,44,
168,Grounding Prompting,"How to ensure that an LLM agent's responses and actions are factually accurate, aligned with external data, or adhere to specific domain rules, thereby preventing hallucinations or invalid outputs.","LLM agents operate in environments where factual accuracy, compliance with rules, or leveraging external knowledge is critical for task success and trustworthiness.","Provide instructions in the prompt that guide the LLM to verify answers, include verifiable evidence, or adhere to domain-specific rules. This can complement or be supported by a 'Grounding Agent' that actively injects relevant knowledge.","Improves factuality, reduces hallucinations, and ensures compliance with rules or external information. Builds trust in the agent's outputs.","['Prompt Design Patterns', 'Knowledge & Reasoning Patterns', 'LLM-specific Patterns', 'Grounding Agent']","[""AssistantAgent's default system message (Figure 5: 'When you find an answer verify the answer carefully. Include verifiable evidence in your response if possible')""]",,23,
169,Non-Retrieval QA (LLM-only),"Large Language Models (LLMs) may generate factually incorrect answers or struggle with queries requiring precise, current, or external knowledge because they rely solely on their parametric memory.",Queries that are straightforward and can be accurately answered by the LLM's internal knowledge without the need for external information or retrieval.,"The LLM directly processes the input query (`q`) to generate an answer (`a = LLM(q)`), without accessing any external knowledge bases or retrieval modules.","High efficiency for simple, common knowledge queries. However, it is largely problematic for queries requiring specific, current, or external knowledge beyond the LLM's training data, leading to potential inaccuracies or hallucinations.","['Single-step Retrieval-Augmented QA', 'Multi-step Retrieval-Augmented QA', 'Adaptive Retrieval (General Baseline)', 'AdaptiveRAG']","['Simple factual questions', 'Common knowledge queries', 'Initial LLM deployments where external knowledge integration is not yet implemented or required.']",,31,"['Generative AI Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns']"
170,Single-step Retrieval-Augmented QA,"LLMs alone struggle with queries that require external, non-parametric knowledge not stored in their internal memory, leading to potential inaccuracies or an inability to answer certain questions.","Queries that require external knowledge, where the necessary information can typically be found within one or a few relevant documents through a single retrieval operation. This pattern is suitable for single-hop questions.","1. A `Retriever` module identifies and retrieves relevant documents (`d`) from an external knowledge source (`D`) based on the input query (`q`). 2. The retrieved documents (`d`) are then incorporated into the LLM's input, augmenting its context, and the LLM generates the answer (`a = LLM(q, d)`).","Significantly improves accuracy and currency for queries needing external knowledge compared to non-retrieval methods, mitigating hallucination. However, it may be insufficient for complex queries that require synthesizing information from multiple sources or multi-step reasoning.","['Non-Retrieval QA', 'Multi-step Retrieval-Augmented QA', 'Adaptive Retrieval (General Baseline)', 'AdaptiveRAG']","['Open-domain question answering for moderate complexity questions', 'Factual lookup and verification', 'Enhancing LLM accuracy with up-to-date external knowledge.']",,31,"['Generative AI Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns', 'Tools Integration Patterns']"
171,Multi-step Retrieval-Augmented QA (Iterative RAG),"Complex, multi-hop queries cannot be adequately answered by single-step retrieval, as they require synthesizing information from multiple documents and performing iterative reasoning steps to reach a conclusion.","Queries that demand sequential reasoning, information aggregation from disparate sources, and iterative refinement of understanding to formulate a complete answer. Typically applies to multi-hop questions.","The LLM and a `Retriever` interact iteratively over multiple rounds. 1. An initial query (`q`) is provided. 2. In each step `i`, new documents (`d_i`) are retrieved, often based on the query and a growing context (`c_i`) derived from previous documents and intermediate answers. 3. The LLM generates intermediate answers or refines its reasoning using the current query, retrieved documents, and accumulated context (`a_i = LLM(q, d_i, c_i)`). 4. This process continues until the final answer is derived or a maximum number of steps is reached.","Highly effective for complex multi-hop queries, enabling the LLM to build a comprehensive foundation for solving intricate problems. However, it is resource-intensive and computationally expensive due to repeated accesses to both the LLM and the retriever, making it inefficient for simpler queries.","['Single-step Retrieval-Augmented QA', 'Adaptive Retrieval (General Baseline)', 'AdaptiveRAG', 'Chain-of-Thought (as a reasoning mechanism)']","['Complex multi-hop question answering', 'Tasks requiring deep reasoning and information synthesis from multiple sources', 'Fact-checking and evidence aggregation.']",,31,"['Generative AI Patterns', 'Agentic AI Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns', 'Tools Integration Patterns', 'Planning Patterns']"
172,Adaptive Retrieval (General Baseline),"Traditional 'one-size-fits-all' retrieval strategies lead to inefficiencies (e.g., complex RAG for simple queries) or ineffectiveness (e.g., simple RAG for complex queries) when dealing with a wide range of query complexities.","Systems where user queries vary significantly in complexity, from straightforward to highly intricate, and a static retrieval approach is suboptimal for overall performance or resource utilization.","Dynamically adjusts the retrieval strategy based on an assessment of the query's complexity or characteristics. Examples include: 
*   **Mallen et al. 2023:** Decides whether to retrieve based on the frequency of entities in the query. 
*   **Qi et al. 2021:** Performs a fixed set of operations (retrieving, reading, reranking) repeatedly until an answer is derived. 
*   **Asai et al. 2024 (SelfRAG):** Trains the LLM to predict a special token, triggering retrieval if a certain threshold is met, then generates an answer.","Aims to optimize the trade-off between efficiency and effectiveness by applying retrieval only when necessary or by adjusting its intensity. However, these baseline approaches are often limited (e.g., a binary decision is too simple, fixed operations are not truly adaptive to varying complexities, or a single model attempts to handle all adaptivity internally) and may not be sufficiently fine-grained for all query complexities.","['Non-Retrieval QA', 'Single-step Retrieval-Augmented QA', 'Multi-step Retrieval-Augmented QA', 'AdaptiveRAG']","['Optimizing the performance and cost of RAG systems in environments with varied query types', 'Reducing latency for simple queries', 'Improving resource efficiency in LLM deployments.']",,31,"['Generative AI Patterns', 'Agentic AI Patterns', 'LLM-specific Patterns', 'MLOps Patterns', 'Tools Integration Patterns']"
173,AdaptiveRAG (Adaptive Retrieval-Augmented Generation),"Existing retrieval-augmented LLM (RAG) approaches are 'one-size-fits-all,' leading to either unnecessary computational overhead for simple queries (e.g., using multi-step RAG) or insufficient handling for complex, multi-step queries (e.g., using no-retrieval or single-step RAG). This results in suboptimal efficiency and accuracy across the spectrum of real-world query complexities.","Open-domain Question Answering (QA) systems where user queries exhibit a wide range of complexities, from straightforward (answerable by LLM's internal knowledge) to moderate (requiring single-step retrieval) to highly complex (demanding multi-step reasoning and iterative retrieval).","1.  **Query Complexity Assessment:** A dedicated, smaller Language Model (Classifier) is trained to predict the complexity level of an incoming query (`q`). The classifier outputs one of three labels: `A` (Straightforward: answerable by LLM itself, No Retrieval), `B` (Moderate: requires at least a single-step retrieval), or `C` (Complex: requires a multi-step retrieval and reasoning solution). 
2.  **Automated Classifier Training Data Generation:** Training data for the classifier is automatically constructed without human labeling by: 
    *   **Outcome-based Labeling:** Assigning complexity labels (A, B, C) to queries based on which of the three RAG strategies (No Retrieval, Single-step, Multi-step) successfully provides a correct answer, prioritizing simpler models in case of ties. 
    *   **Dataset Inductive Bias Labeling:** For queries not labeled by outcome, assign labels based on the known characteristics of the datasets they come from (e.g., 'B' for single-hop datasets, 'C' for multi-hop datasets). 
3.  **Dynamic Strategy Selection:** Based on the complexity level predicted by the classifier, AdaptiveRAG dynamically selects and executes the most suitable retrieval-augmented LLM strategy (No Retrieval, Single-step, or Multi-step) to answer the query.","Significantly enhances the overall accuracy and efficiency of QA systems by seamlessly adapting between different RAG strategies. It avoids unnecessary computational costs for simple queries while ensuring robust and comprehensive handling of complex ones, leading to a significant balance between performance and resource utilization across diverse query types. This approach offers a robust middle ground among existing methods.","['Non-Retrieval QA', 'Single-step Retrieval-Augmented QA', 'Multi-step Retrieval-Augmented QA', 'Adaptive Retrieval (General Baseline)', 'Agentic AI Patterns (classifier as a decision-making agent)']","['Production-grade open-domain QA systems', 'Intelligent conversational agents', 'Enterprise search and information retrieval systems', 'Any LLM application requiring dynamic resource allocation and tailored processing for varying input complexities to optimize accuracy, efficiency, and cost.']",,31,"['Generative AI Patterns', 'Agentic AI Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns', 'MLOps Patterns', 'Tools Integration Patterns', 'Planning Patterns']"
174,LACE (Local Agnostic attribute Contribution Explanation),"High-performing machine learning models, especially black-box classifiers, lack interpretability, making it difficult to understand the reasons behind individual instance predictions. Quantifying the influence of feature values and their subsets on predictions often faces exponential time complexity if all combinations are considered.","Post-hoc explanation for black-box classification models on structured data, where the goal is to provide a comprehensive understanding of individual predictions (both qualitative and quantitative). It is model-agnostic.","LACE trains a local rule-based surrogate model (an associative classifier) on the K nearest neighbors of the instance to be explained. This local model identifies relevant patterns (conjunctions of attribute-value pairs) that locally determine the prediction. A removal-based technique then computes the 'prediction difference' for these identified relevant patterns and individual attribute values, quantifying their influence on the prediction probability. An automatic heuristic is used to select the optimal 'K' for defining the locality.","Provides a dual explanation: a qualitative understanding through intrinsically interpretable local rules/patterns, and a quantitative understanding through prediction difference values. It accurately captures relevant feature associations and their joint effects, overcoming the exponential time complexity of evaluating all feature subsets. It helps in understanding and comparing the local behavior of different classifier models.","['LIME (contrasted for locality definition and interaction capture)', 'SHAP (contrasted for interaction summarization)', 'Anchor (contrasted for rule completeness)', 'IME (removal-based explanations)']","['Explaining individual black-box model predictions', 'Human-in-the-loop inspection of model predictions', 'Understanding local model behavior', 'Comparing local behavior across different classifiers']",AI–Human Interaction Patterns,37,
175,XPLAIN (Interactive Framework to Inspect Model Behavior),"The black-box nature of many high-performance machine learning models hinders interactive exploration and understanding of their behavior, making it challenging for users (experts and end-users) to trust, debug, and improve AI systems, especially for critical decisions.","Interactive analysis of individual predictions for any black-box classifier on structured data, where human-in-the-loop inspection, what-if analysis, and comparison across models/classes are desired to gain actionable insights.","XPLAIN is a web-based interactive tool that integrates LACE as its underlying explanation method. It allows users to: 1) Generate and compare class-dependent explanations for single instances (both correctly and misclassified). 2) Perform 'what-if' analyses by interactively changing attribute values and observing the impact on predictions and explanations. 3) Define user-specific rules to test domain knowledge assumptions. 4) Aggregate multiple local explanations into 'explanation metadata' (attribute, item, and local rule views) to derive global insights into the model's behavior.","Enables comprehensive human-in-the-loop inspection, debugging, and validation of AI models. It provides actionable insights for assessing trustworthiness, detecting wrong associations, and improving models. It supports GDPR-compliant ex-post explanations and facilitates knowledge discovery about model behavior.","['LACE (underlying explanation method)', 'SHAP (for combining local explanations to global insights)', 'LIME (for combining local explanations)']","['Model validation and testing', 'Debugging classifiers', 'Assessing model trustworthiness', 'Detecting wrong associations and biases', 'GDPR compliance for automated decisions', 'Comparing model behavior across different algorithms', 'Knowledge discovery in data']",AI–Human Interaction Patterns,37,
176,DivExplorer (Algorithm for Subgroup Divergence Analysis),"Identifying and characterizing data subgroups where a classification model behaves differently (anomalously or with bias) from its overall behavior is crucial for model validation and fairness. Existing methods often rely on predefined subgroups, non-exhaustive searches, or struggle to quantify individual item contributions to subgroup divergence.",Post-hoc analysis of black-box classification models (generalized to scoring and ranking systems) on structured data. The goal is to automatically discover and characterize all sufficiently represented (frequent) subgroups that exhibit anomalous or peculiar behavior.,"DivExplorer introduces the 'divergence' metric to quantify the difference in a chosen performance statistic (e.g., False Positive Rate, False Negative Rate, error rate) between a data subgroup (defined by an intrinsically interpretable pattern/itemset) and the overall dataset. It leverages efficient frequent pattern mining algorithms (like FPgrowth) to exhaustively identify all frequent itemsets and compute their divergence and statistical significance (informed by Bayesian statistics). It employs Shapley values to quantify the 'item contribution to itemset divergence' and introduces 'global item divergence' (a generalization of Shapley value) to measure an item's overall impact on divergence across all patterns. It also identifies 'corrective items' that reduce divergence.","Provides a complete and efficient exploration of all frequent divergent subgroups, revealing peculiar model behaviors, including those that might be masked by non-monotonic metrics or require specific item associations. It allows for detailed analysis of individual item contributions and global item influence on divergence, including the identification of corrective effects. This provides a deep understanding of model behavior at the subgroup level.","['Slice Finder (compared and contrasted for search exhaustiveness and metrics)', 'FairVIS (compared for subgroup generation and interpretability)', 'Errudite (related for error analysis)', 'Frequent Pattern Mining (underlying technique, e.g., Apriori, FPgrowth)', 'Shapley values (concept used for contribution quantification, similar to SHAP, IME)']","['Model validation and testing', 'Error analysis and model debugging', 'Evaluation of model fairness (especially intersectional bias)', 'Identifying bias in AI systems', 'Understanding underrepresented group behavior', 'Analysis of scoring functions and ranking systems']",Knowledge & Reasoning Patterns,37,
177,DivExplorer (Interactive System for Subgroup Divergence Analysis),"Users need an intuitive and interactive way to explore and understand the complex insights derived from the DivExplorer algorithm, such as lists of divergent subgroups, individual item contributions, and corrective phenomena, to effectively analyze and debug AI models or identify bias.","Interactive exploration of divergent subgroups identified by the DivExplorer algorithm, for black-box classifiers (or scoring/ranking systems) on structured data, where users need to drill down, summarize, and search for specific patterns.",A web-based interactive tool that integrates the DivExplorer algorithm. It provides a user interface to: 1) Display a sortable table of divergent itemsets. 2) Enable pruning of redundant itemsets for better summarization. 3) Allow users to drill down into specific itemsets to visualize individual item contributions to divergence using bar graphs (Shapley values) and an itemset lattice. 4) Visually highlight 'corrective items' and phenomena within the lattice. 5) Present the 'individual' and 'global divergence' of items across the entire dataset. 6) Offer interactive search functionalities for itemsets containing specific items or for supersets of a given itemset.,"Enables dynamic, intuitive, and comprehensive exploration of classifier behavior in data subgroups. It facilitates the analysis of problematic behaviors, identification of bias, and understanding of contributing factors, making the DivExplorer algorithm's insights accessible and actionable for debugging, validation, and fairness assessment.","['DivExplorer (underlying algorithm, Chapter 6)', 'Shapley values (concept used for contribution visualization)']","['Analyzing and debugging classifiers', 'Identifying bias in AI systems', 'Model validation and testing', 'Assessing model fairness', 'Interactive data exploration for peculiar behaviors']",AI–Human Interaction Patterns,37,
178,Retrieval-Augmented Generation (RAG),"Large Language Models (LLMs) often suffer from factual inaccuracies (hallucinations), lack access to up-to-date or domain-specific information, and their knowledge is limited to their training data, leading to suboptimal generation quality for knowledge-intensive tasks.","Scenarios requiring LLMs to generate accurate, relevant, and contextually rich responses based on external, dynamic, or specific knowledge, such as question answering, summarization, or content creation.","Integrate LLMs with external knowledge databases. A system first retrieves relevant documents or information from a knowledge base (e.g., vector database) based on the user's query. This retrieved information is then appended to the original prompt, creating an 'augmented request,' which is fed to the LLM for generation.","Significantly improved generation quality, reduced factual errors, access to up-to-date information, and enhanced contextual understanding, often outperforming finetuned LLMs for specific downstream tasks.","['KV Cache Management', 'Vector Search']","['Question Answering', 'Content Creation', 'Code Generation', 'Enhancing NLP tasks that require external knowledge']",Generative AI Patterns,3,
179,Knowledge Tree for KV Cache,"In RAG systems, the Key-Value (KV) tensors generated by LLMs are sensitive to the order of preceding tokens. When retrieving and injecting multiple documents, the KV tensors for a document sequence are different from another sequence. Efficiently storing, retrieving, and sharing these order-dependent KV tensors across multiple requests, especially when documents are frequently reused in varying orders, is challenging.","RAG systems that aim to cache and reuse the intermediate KV states of retrieved documents to reduce recomputation, particularly when LLMs use attention mechanisms where token order matters. The cache needs to support hierarchical memory (GPU/host).","Organize the intermediate KV tensors of retrieved documents in a tree structure, specifically a prefix tree (trie), based on document IDs. Each path from the root to a node represents a specific sequence of documents, with nodes holding the KV tensors for a referred document in that sequence. This allows different request paths to share common prefixes (nodes) and their associated KV tensors.","Facilitates fast and order-aware retrieval of KV tensors, enables sharing of intermediate states for common document prefixes across multiple requests, and aligns with hierarchical memory structures (GPU/host). Reduces redundant computation and improves efficiency.","['KV Cache Management', 'Retrieval-Augmented Generation (RAG)', 'Prefix-aware GreedyDualSizeFrequency (PGDSF) Replacement Policy']",['Managing and sharing KV cache for retrieved documents in RAG systems to optimize LLM inference'],LLM-specific Patterns,14,
180,Prefix-aware GreedyDualSizeFrequency (PGDSF) Replacement Policy,"In a hierarchical caching system for RAG's KV tensors (e.g., GPU and host memory), deciding which cached document KV tensors to evict is complex. Traditional policies (LRU, LFU, GDSF) do not adequately account for the variable sizes of document KV tensors, their access frequency, recency, and critically, the 'prefix-aware recomputation cost' which varies based on the document's position in a sequence and whether preceding documents are also cached. This leads to suboptimal cache hit rates and inefficient resource utilization.","Multilevel caching systems (e.g., GPU/host memory) for LLM KV caches in RAG, where documents have variable sizes, access patterns are skewed, and the cost of recomputing KV tensors is dependent on the cached prefix context.","Extend the classic GDSF policy by calculating a priority for each cached node (document KV tensor) based on its access frequency, size, last access time (recency), and a 'prefix-aware recomputation cost.' The cost estimation is sophisticated, amortizing the cost across non-cached tokens and using offline profiling with bilinear interpolation to estimate compute time for varying cached/non-cached token lengths. Nodes with lower priority are evicted first.","Minimizes cache miss rates, ensures that the most valuable and frequently used KV tensors (especially those forming critical prefixes) are retained in faster memory tiers. This leads to higher cache hit rates and significant reductions in Time-to-First-Token (TTFT) and improved throughput for RAG systems.","['Knowledge Tree (for KV Cache)', 'KV Cache Management']",['Cache eviction and placement decisions in hierarchical KV cache systems for RAG'],LLM-specific Patterns,14,
181,Cache-aware Reordering (Request Scheduling),"In LLM serving systems, particularly RAG, unpredictable arrival patterns of user requests can lead to cache thrashing. Requests that could benefit from shared KV cache might arrive in an order that forces frequent evictions and recomputations, resulting in a low cache hit rate and reduced overall efficiency, especially under high request loads.","LLM serving systems for RAG applications with a shared KV cache and a queue of incoming requests, where cache hit rate is critical for performance.","Implement a request scheduling algorithm that reorders incoming requests in a priority queue. The priority metric, `OrderPriority = Cached Length / Computation Length`, prioritizes requests that have a larger portion of their context already cached relative to the amount of new computation required. A window size is used to ensure fairness and prevent starvation.","Improves the cache hit rate, reduces total computation time, and optimizes resource utilization. This strategy effectively mitigates cache volatility and enhances overall system throughput and Time-to-First-Token (TTFT), particularly under high request rates.","['MLOps Serving Patterns', 'KV Cache Management']",['Optimizing request scheduling in RAG serving to maximize KV cache utilization'],MLOps Patterns,14,
182,Dynamic Speculative Pipelining (for RAG),"In the RAG workflow, vector retrieval (often CPU-bound) and LLM inference (GPU-bound) are typically executed sequentially. This leads to significant idle time for the GPU during retrieval and long end-to-end latency, especially when retrieval itself can be time-consuming or require high accuracy.","RAG systems where retrieval and generation are distinct steps, and retrieval can produce intermediate candidate results before completion. Minimizing end-to-end latency is a critical performance goal.","Overlap the knowledge retrieval and LLM inference steps. The vector search process is split into multiple stages, continuously providing candidate documents. The LLM engine can then initiate 'speculative generation' using these early candidate documents. If later stages of retrieval produce different (more accurate) documents, the current speculative generation is terminated, and a new one starts. If the documents remain the same, generation continues. This pipelining is dynamically enabled based on system load (e.g., only when the LLM request pool is empty or below a certain threshold) to avoid unnecessary computation under high load.","Significantly reduces end-to-end latency by minimizing idle GPU time, decreases non-overlapping vector search time, and improves Time-to-First-Token (TTFT). Balances latency reduction with computational overhead by dynamic activation.","['Tools Integration Patterns', 'MLOps Serving Patterns']","['Accelerating the RAG workflow by concurrently executing retrieval and generation, particularly beneficial when retrieval latency is substantial']",MLOps Patterns,14,
183,Swap-out-Only-Once for Hierarchical KV Cache,"In hierarchical caching systems (e.g., GPU HBM and host memory connected via PCIe), frequent data transfers of large Key-Value (KV) tensors between different memory tiers can become a significant performance bottleneck due to the lower bandwidth of interconnects like PCIe compared to GPU HBM.","Multilevel caching systems for LLM KV caches, where data needs to be moved between fast (GPU) and slower (host) memory, and minimizing transfer overhead is crucial.","When a KV tensor (node in the Knowledge Tree) is first evicted from GPU memory, it is swapped out and copied to the host memory. For all subsequent evictions of that same KV tensor from GPU memory, it is simply freed without copying it again, as a copy already exists in the host memory. The host memory retains its copy until the node is evicted from the entire cache system.","Minimizes redundant data transfers between GPU and host memory, significantly reducing the overhead associated with cache evictions and improving overall cache performance, especially when host memory capacity is much larger than GPU memory.","['KV Cache Management', 'Knowledge Tree (for KV Cache)']",['Optimizing memory transfer efficiency in hierarchical KV cache management for LLMs'],LLM-specific Patterns,14,
184,Fault-Tolerant Knowledge Cache,"In RAG systems employing a hierarchical KV cache (e.g., Knowledge Tree with GPU and host memory), hardware failures (like GPU failure) can lead to the invalidation of critical cached intermediate states (KV tensors). Given the prefix sensitivity of LLM inference, a GPU failure might invalidate an entire subtree of dependent KV caches, leading to significant data loss and recovery overhead. Request processing failures also need robust handling.","RAG systems with complex, stateful caching mechanisms (like Knowledge Tree) that are susceptible to hardware or processing failures, where quick recovery and continued operation are important for system reliability and user experience.","Implement mechanisms to ensure the resilience of the KV cache. This includes replicating a portion of the most frequently accessed and critical upper-level nodes (e.g., system prompts or common document prefixes) in the more resilient host memory. Additionally, a timeout mechanism is used to detect and retry failed requests. If a request fails before its first iteration, it's recomputed; otherwise, it can continue using its stored KV cache.",Enhances system reliability and availability by providing mechanisms for fast recovery from GPU failures and robust handling of request processing errors. Minimizes data loss and reduces the impact of transient failures on RAG service continuity.,"['Knowledge Tree (for KV Cache)', 'MLOps Reliability Patterns']",['Ensuring robustness and availability of RAG serving systems'],MLOps Patterns,14,
185,Tool Use,"Language agents powered by Large Language Models (LLMs) have limited inherent capabilities and access to real-world, dynamic information, restricting their ability to perform complex tasks.","When a language agent needs to interact with external environments, gather specific data (e.g., flight information, restaurant details), perform calculations, or execute actions beyond its inherent knowledge.","Equip language agents with a 'Toolbox' of external functions or APIs (e.g., CitySearch, FlightSearch, DistanceMatrix, RestaurantSearch, AttractionSearch, AccommodationSearch, NotebookWrite). The agent selects and employs these tools based on its reasoning to achieve complex goals.","Significantly expands the potential capabilities of language agents, allowing them to collect information, perform calculations, and interact with structured data. However, agents can struggle with argument errors in tool calls, getting trapped in dead loops, and dynamically adjusting plans based on tool feedback.","['ReAct', 'Reflexion', 'Memory Management', 'Task Decomposition', 'Retrieval']","Information collection (e.g., travel data), data retrieval, calculations, interacting with external databases, completing real-world tasks like travel planning, web navigation, and embodied agent control.","['Tools Integration Patterns', 'Agentic AI Patterns']",11,
186,Chain-of-Thought (CoT) Prompting,"Large Language Models (LLMs) struggle with complex reasoning tasks, often producing direct, incorrect answers without showing their intermediate thought processes.","When an LLM is tasked with a problem that requires logical deduction, multi-step problem-solving, or complex decision-making, and a direct answer is insufficient or often wrong.","Enhance the reasoning process by explicitly requiring intermediate steps. This is typically achieved by adding a prompt like 'Let's think step by step' to elicit a detailed, sequential thought process from the LLM.","Improves the LLM's capability to engage in step-by-step reasoning, leading to significant improvements in performance on complex tasks by making the LLM's internal thought process explicit and allowing for self-correction.","['Direct Prompting', 'ReAct', 'Reflexion', 'Task Decomposition', 'Tree of Thoughts', 'Graph of Thoughts']","Mathematical problem-solving, logical reasoning, planning, complex question answering, and general multi-step problem-solving.","['Prompt Design Patterns', 'Knowledge & Reasoning Patterns', 'LLM-specific Patterns']",26,
187,ReAct (Reasoning and Acting),"LLM-powered agents need to synergize reasoning with actions to effectively interact with dynamic environments, collect information, and make progress towards a goal, especially when explicit instructions are insufficient or environmental feedback is crucial.","An agent needs to perform a sequence of actions (e.g., using tools) in an environment, where each action's outcome might influence subsequent decisions, and continuous reasoning is required to stay on track and adapt.","The agent alternates between 'Thought' (reasoning about the current situation and planning the next step) and 'Action' (executing a tool or API call, or performing a specific operation). The 'Observation' (feedback from the environment after an action) then informs the next 'Thought' step, creating an iterative loop of reasoning, acting, and observing.","Enhances language agents' reasoning ability and effectiveness in tool interaction, enabling iterative information collection and decision-making. However, agents can still struggle to convert their reasoning into correct actions, keep track of global or multiple constraints, and avoid dead loops.","['Tool Use', 'Chain-of-Thought', 'Reflexion', 'Task Decomposition', 'Feedback from Environment']","Complex planning (e.g., travel planning), web navigation, embodied agents, tool-augmented LLMs, and problem-solving in dynamic, partially observable environments.","['Agentic AI Patterns', 'Prompt Design Patterns', 'Planning Patterns', 'Tools Integration Patterns', 'Knowledge & Reasoning Patterns']",8,
188,Reflexion,"Language agents often get trapped in repetitive errors, suboptimal paths, or fail to learn from past mistakes and improve their planning or execution over time.","When an agent attempts a task, receives feedback (e.g., an action is invalid, a plan is too costly, or a constraint is violated), and needs to adjust its strategy or re-plan to avoid similar failures in the future.","Extends the ReAct framework by incorporating a 'reflection' step. After a task attempt (especially a failed one), a separate reflection model (or the same LLM prompted for reflection) provides high-level insights and verbal feedback on why the attempt failed. This feedback is then used to refine the agent's internal 'thought' process or prompt for subsequent attempts, akin to 'verbal reinforcement learning'.","Aims to help agents identify and correct flawed reasoning, leading to improved performance over multiple attempts and increased robustness. However, agents might still struggle to align their actions with their reasoning or make substantial adjustments based on reflections.","['ReAct', 'Chain-of-Thought', 'Feedback from Environment', 'Memory Management']","Improving agent robustness, self-correction in planning, overcoming dead loops, refining strategies in complex, multi-step tasks, and learning from failures in interactive environments.","['Agentic AI Patterns', 'Planning Patterns', 'Knowledge & Reasoning Patterns', 'LLM-specific Patterns']",23,
189,Greedy Search,"Finding a solution or path in a search space where a quick, locally optimal decision is desired, without extensive exploration of future possibilities or global implications.","When a planning or decision-making task can be broken down into a series of steps, and at each step, the most immediate beneficial option (e.g., lowest cost, highest reward) is chosen, without necessarily considering its long-term impact.","At each decision point in the planning process, select the option that appears best at that moment according to a predefined heuristic or optimization objective (e.g., minimizing cost). For travel planning, this involves picking the cheapest flight, restaurant, accommodation, etc.","Provides a straightforward and often computationally efficient baseline solution. However, it frequently leads to suboptimal or infeasible global plans when long-term constraints or interdependencies are significant, as it struggles with multi-constraint tasks.",[],"Baselines for planning tasks, simple optimization problems, initial exploration of a search space, and situations where computational resources are limited.","['Classical AI', 'Planning Patterns']",17,
190,Direct Prompting,"Obtaining a specific output from an LLM without providing intermediate reasoning steps or complex interaction protocols, which can lead to errors for complex tasks.","When an LLM is capable of generating a response directly from a query, and the task does not explicitly require step-by-step reasoning or tool interaction.",The query is input directly into the model along with instructions detailing the task and any relevant information. The LLM is expected to generate the final output in one continuous response.,"Simple and fast for straightforward tasks. For complex tasks like multi-constraint planning, it often results in low accuracy, hallucination, and failure to meet constraints, as the LLM lacks explicit guidance for reasoning and validation.","['Chain-of-Thought', 'ReAct']","Simple question answering, text generation, summarization, and tasks where the LLM's inherent knowledge is sufficient for a direct response.","['Prompt Design Patterns', 'LLM-specific Patterns']",26,
191,Tree of Thoughts (ToT),"Traditional Chain-of-Thought prompting explores only one linear path of reasoning, which can be insufficient for problems requiring exploration of multiple reasoning paths or backtracking from dead ends.","Complex problems that involve exploration, lookahead, or require evaluating multiple possible reasoning steps before committing to a single path. These problems often have intermediate states that need to be evaluated for their prospects.","Decompose the problem into 'thought' steps, but instead of a linear chain, create a tree structure where each node represents an intermediate thought state. The agent can explore multiple branches (different reasoning paths), evaluate their prospects, and backtrack if a path leads to a dead end, allowing for more deliberate problem-solving.","Enables more exhaustive and deliberate problem-solving by exploring diverse reasoning paths. However, it requires extensive exploration of the search space, making it prohibitively costly for very complex problems like those in TravelPlanner.","['Chain-of-Thought', 'Graph of Thoughts', 'Planning', 'Task Decomposition']","Complex reasoning, strategic planning, puzzles, and tasks requiring combinatorial search or multi-path exploration.","['Planning Patterns', 'Knowledge & Reasoning Patterns', 'LLM-specific Patterns']",6,
192,Graph of Thoughts (GoT),"Tree of Thoughts can still be restrictive, as it assumes a tree structure, while complex problems might benefit from more flexible, graph-like connections between thoughts, allowing for merging and cycles in the reasoning process.","Problems where intermediate thoughts might not form a strict tree, but rather a more interconnected graph, allowing for richer relationships between reasoning steps, parallel exploration, and merging of ideas from different branches.","Represent the reasoning process as a graph where nodes are 'thoughts' and edges represent transitions or dependencies between thoughts. This allows for more dynamic and non-linear exploration of reasoning paths, including parallel processing and the merging of information from different branches.","Offers a more flexible and potentially powerful framework for deliberate problem-solving than ToT. Similar to ToT, it requires extensive exploration of the search space, making it prohibitively costly for very complex problems.","['Tree of Thoughts', 'Planning', 'Knowledge & Reasoning', 'Task Decomposition']","Elaborate problem-solving, advanced reasoning tasks, and complex planning where thought merging and non-linear reasoning are beneficial.","['Planning Patterns', 'Knowledge & Reasoning Patterns', 'LLM-specific Patterns']",6,
193,Memory Management (for Language Agents),"LLMs have limited context windows (short-term memory) and struggle to retain and process information over long interactions or complex, multi-step tasks, leading to 'Lost in the Middle' phenomena or exceeding token limits.","Language agents engaged in long-horizon planning, multi-turn dialogues, or complex problem-solving require continuous access to past information, intermediate plans, or external knowledge to maintain coherence and achieve goals.","Implement mechanisms to acquire and process information, often divided into long-term memory (the parametric memory inherent in LLMs) and short-term memory (in-context learning or working memory). Techniques like memory summarization, retrieval, and dedicated tools (e.g., `NotebookWrite`) are employed to store and manage necessary information.","Enables agents to handle longer contexts, access relevant past information, and avoid exceeding token limits. However, agents can still get 'lost in the middle' or struggle with effectively utilizing accumulated information.","['Memory Summarization', 'Retrieval', 'Tool Use']","Long-horizon planning, multi-turn conversations, maintaining state in agentic workflows, complex problem-solving, and enhancing LLM grounding.","['LLM-specific Patterns', 'Knowledge & Reasoning Patterns', 'Agentic AI Patterns']",29,
194,Memory Summarization,"The finite context window of LLMs means that long histories of interactions or extensive collected information cannot be fully retained, leading to the loss of crucial details or exceeding token limits during long-running tasks.","When a language agent needs to maintain a coherent understanding of a long interaction or a large amount of collected information, but the raw data is too extensive for the LLM's context window.","Periodically summarize past interactions, observations, or collected data into a concise representation. This summary then replaces or augments the raw history in the LLM's context, allowing key information to be retained.","Enhances the memory capabilities of language agents by compressing information, allowing them to retain key details over longer periods and manage context accumulation more effectively.","['Memory Management', 'Retrieval']","Long-running agentic tasks, multi-session dialogues, complex planning where intermediate states or past interactions need to be condensed for efficient recall.","['LLM-specific Patterns', 'Knowledge & Reasoning Patterns', 'Agentic AI Patterns']",29,
195,Retrieval (for Memory Augmentation),"LLMs have static knowledge bases, leading to potential hallucinations or an inability to access up-to-date, specific, or proprietary external information required for a task.","When an LLM needs to answer questions, generate plans, or make decisions based on a large, dynamic, or specialized external knowledge base that is not part of its training data.","Integrate a retrieval mechanism that fetches relevant information from an external database (e.g., structured data records) based on the current query or context. This retrieved information is then provided to the LLM as additional context for its reasoning and generation, effectively augmenting its memory.","Enhances the memory capabilities of language agents by providing access to external, specific information, reducing hallucinations and improving factual grounding. Allows LLMs to ground their responses in up-to-date and relevant data.","['Memory Management', 'Tool Use']","Knowledge-intensive tasks, grounding LLM generations in specific data, enhancing agent memory, and complex planning requiring external data (e.g., searching for flights, attractions, accommodations).","['LLM-specific Patterns', 'Knowledge & Reasoning Patterns', 'Tools Integration Patterns', 'Agentic AI Patterns']",29,
196,Task Decomposition,"Complex problems are often too large or intricate for an AI agent (especially an LLM) to solve in a single step, leading to errors, suboptimal solutions, or an inability to complete the task.","When a user query or a high-level goal requires a series of interdependent sub-problems to be solved, or a long sequence of actions to be planned and executed.","Break down the complex task into smaller, more manageable sub-tasks or sequential steps. The agent then addresses these sub-tasks iteratively or strategically, often by using reasoning or tool-use for each step, building up towards the overall solution.","Simplifies complex problems, making them tractable for AI agents. Improves the likelihood of successful completion by guiding the agent through a structured approach, leading to significant improvements in performance.","['Chain-of-Thought', 'ReAct', 'Planning', 'Tree of Thoughts', 'Graph of Thoughts']","Planning (e.g., multi-day travel itineraries), complex problem-solving, code generation, and multi-stage reasoning tasks.","['Planning Patterns', 'Knowledge & Reasoning Patterns', 'Agentic AI Patterns']",6,
197,Feedback from Environment,"AI agents, particularly LLMs, can make errors or suboptimal decisions without external validation or information about the real-world consequences of their actions, leading to persistent failures or illogical plans.","An agent operates in an interactive environment where its actions yield observable outcomes, and these outcomes can be used to guide subsequent decisions, correct errors, or inform further planning.","Design the agent to receive and process feedback from its environment after performing an action. This feedback (e.g., 'no flights found', 'cost is too high', 'actions are invalid', 'null results') is then incorporated into the agent's reasoning process to dynamically adjust its plan or strategy.","Enables agents to dynamically adjust their plans, learn from mistakes, and operate more robustly in dynamic settings. However, agents might struggle to effectively utilize this feedback for global planning or complex self-correction, sometimes repeating invalid actions.","['ReAct', 'Reflexion', 'Planning', 'Tool Use']","Interactive problem-solving, agentic systems, reinforcement learning, self-correction mechanisms, and dynamic planning in changing environments.","['Agentic AI Patterns', 'Planning Patterns']",1,
198,Retrieval Augmentation,"Large Language Models (LLMs) are susceptible to hallucinations, exhibit weaknesses in numerical reasoning, and may lack access to up-to-date or specific external information required for certain tasks.","LLMs performing NLP tasks, especially information-seeking question answering, where the required knowledge might be external, dynamic, specialized, or when numerical computation is involved.","Augment LLMs with external tools such as retrieval systems (using sparse or dense retrieval), specialized math tools (e.g., Wolfram math plugin), and code interpreters. These tools extract relevant knowledge from external corpora or perform computations.","Mitigates hallucinations by providing fact-checked, up-to-date knowledge; enhances numerical reasoning abilities; and improves performance in question answering by accessing external information.","['Program-of-Thought', 'PAL (Program-Aided Language Models)', 'MathPrompt', 'Code4Struct', 'Toolformer']","['Open-domain question answering', 'Fact-checking', 'Timely information benchmarks', 'Mathematical problem-solving', 'Code-based tasks (e.g., tabular data, math)']",,2,
199,Tool-Augmented LLMs,"LLMs, despite vast internal knowledge, suffer from challenges like hallucination, weak numerical reasoning, and inability to access real-time or domain-specific external information. Existing evaluation methods often fail to distinguish between recalling internal knowledge and genuine tool use.","Designing LLM-based systems that need to solve complex tasks requiring interaction with diverse external knowledge sources (text, tables, graphs) or specialized computational abilities (math, code).","Integrate LLMs with a suite of external, specialized tools (e.g., text retrieval, database operations, mathematical computations, graph operations, code interpreters). The LLM acts as a controller, selecting and orchestrating these tools to acquire information and solve problems.","Significantly enhances LLMs' ability to answer questions by leveraging external, current, and domain-specific knowledge, overcoming internal limitations. Leads to better performance on challenging tasks requiring complex reasoning and information synthesis from multiple sources.","['Retrieval Augmentation', 'Chameleon', 'ReAct', 'Program-of-Thought', 'PAL', 'MathPrompt', 'Code4Struct', 'Toolformer', 'HuggingGPT', 'ART', 'MMReAct', 'Visual ChatGPT']","['Question answering with external knowledge', 'Complex reasoning tasks', 'Data manipulation (tabular, graph)', 'Scientific information extraction', 'Personal agenda management', 'Mathematical problem-solving']",,2,
200,Autonomous Planning (for Tool Use),Complex tasks often require breaking down the problem into multiple intermediate steps and orchestrating a sequence of tool calls. LLMs might struggle with this multi-step reasoning and tool composition.,LLMs are tasked with solving problems that cannot be solved by a single tool call and require a logical sequence or composition of multiple tools.,Enable LLMs to autonomously break down complex tasks into intermediate reasoning steps and generate plans for tool use. This involves the LLM deciding which tools to call and in what order.,"Allows LLMs to tackle more challenging, multi-step problems by effectively composing different tools and managing the flow of information between them, leading to improved task completion rates.","['Chain-of-Thought', 'ReAct', 'Chameleon', 'Tree-of-Thoughts', 'Decomposed Prompting', 'LLMP']","['Multi-tool question answering', 'Complex reasoning tasks', 'Agentic behavior', 'Task decomposition for LLMs']",,6,
201,Self-Reflection / Feedback Loop,"LLMs, when using tools, may make incorrect decisions, call tools with wrong arguments, or generate infeasible plans. Without feedback, they cannot correct their mistakes.","LLM agents interacting with an environment where tool calls produce observable outcomes or errors, and there's a need for iterative refinement of actions or plans.","Prompt LLMs to self-reflect on their previous decisions and the environmental feedback (observations from tool execution). This allows them to identify errors, refine their tool-use chain, and generate subsequent, more accurate actions.","Improves the robustness and accuracy of LLM agents' tool use by enabling iterative error correction and adaptation, leading to better success rates on complex tasks.","['ReAct', 'Self-Refine', 'WebGPT', 'MMReAct', 'Visual ChatGPT']","['Iterative problem-solving', 'Error detection and correction in tool use', 'Enhancing the reliability of LLM agents', 'Adaptive planning']",,23,
202,Human-Guided Generation (for Dataset Curation),"Generating high-quality, diverse, and specific data (e.g., questions for a benchmark) solely with LLMs can lead to unanswerable questions, hallucinations, or content that doesn't meet specific evaluation criteria (e.g., answerable only with external tools). Manual generation is labor-intensive and hard to scale.","Creating specialized datasets or content where precise control over characteristics (e.g., knowledge source, difficulty, answerability) is required, balancing scalability with quality.","Implement a process that combines human guidance with LLM generation, such as a template-based question generation approach. Human experts define and validate question templates, which are then used by LLMs to generate concrete questions by sampling values from reference data. This is part of a larger three-phase process that includes Reference Data Collection and Programmatic Answer Generation.","Produces a scalable, high-quality dataset where questions are faithfully answerable only by external tools, minimizing LLM internal knowledge overlap, while requiring minimal human labeling efforts.","['Template-Based Question Generation', 'Prompt Engineering']","['Dataset curation', 'Benchmark creation', 'Generating domain-specific content with controlled properties', 'Reducing hallucination in generated content']",,40,
203,Programmatic Answer Generation (for Dataset Curation),"Ensuring the accuracy and validity of ground-truth answers for generated questions, especially for complex, multi-step reasoning tasks that rely on external data and tool interactions. Manual verification is error-prone and not scalable.","Creating ground-truth answers for a question-answering dataset where questions require complex logic, data retrieval, filtering, or computation from reference corpora using defined tools.","Implement 'operators' (functions corresponding to predefined tools) and 'tool chains' (schemas for composing operators). For each generated question with known arguments, these tool chains are run programmatically over the reference data to extract the correct answer.","Guarantees precise and verifiable ground-truth answers for complex questions, even those involving multi-step reasoning, in an automated and highly efficient manner, ensuring the integrity of evaluation benchmarks.",[],"['Ground-truth answer generation for QA benchmarks', 'Automated data labeling', 'Ensuring correctness in complex data-driven tasks']",,40,
204,Chain-of-Thought (CoT) Prompting,"Vanilla LLMs may struggle with complex reasoning tasks, often failing to break down problems into logical steps or perform multi-step operations, leading to low success rates.","Using LLMs for question answering or problem-solving where an explicit, step-by-step reasoning process is beneficial or required to arrive at the correct answer.","Add a prompt like 'Let's think step by step' after the question, encouraging the LLM to generate intermediate reasoning traces before providing the final answer, leveraging its inherent reasoning ability.","Elicits and leverages the LLM's reasoning ability, significantly improving its success rate on complex questions by making the thought process explicit.","['Few-shot Reasoning', 'Decomposed Prompting', 'Tree-of-Thoughts']","['Enhancing LLM reasoning capabilities', 'Improving accuracy on complex question answering', 'Debugging LLM thought processes', 'Solving mathematical and logical problems']",,26,
205,Tool-Level Demonstrations,"When augmenting LLMs with multiple external tools, providing task-level few-shot exemplars (examples of full problem-solving traces) can be too lengthy for the LLM's context window or may not cover all necessary tool interactions and compositions. LLMs might not grasp the specific usage of each tool.",Designing prompts for tool-augmented LLMs that need to learn how to effectively select and use a diverse set of tools within a limited context window.,"Instead of task-level exemplars, provide tool-level demonstrations within the prompt. These demonstrations concisely illustrate how to use each individual tool in the pool, ensuring every tool is covered at least once.","Offers a concise and comprehensive tutorial for LLMs on tool usage, fitting within context limits, and enabling them to better understand and apply different tools, leading to improved tool-call accuracy.","['Few-shot Prompting', 'Context Window Management']","['Prompt engineering for multi-tool LLM agents', 'Teaching LLMs new tool functionalities', 'Managing context window limitations in tool-augmented systems']",,18,
206,Context Window Management (as a Design Challenge),"The encoding of interaction history, observations, tool descriptions, and tool-use plans can easily exceed the length limitation of LLMs' context windows, resulting in runtime errors or truncation of crucial information ('Too Long Context' errors).","Designing and operating tool-augmented LLMs, especially for complex tasks that involve multiple turns of interaction, many tools, or verbose descriptions/observations.","This pattern describes a critical design constraint and challenge. Solutions involve careful prompt engineering, such as using concise tool descriptions, providing tool-level demonstrations instead of lengthy task-level ones, and potentially strategies for selective history retention or summarization to keep the prompt within the LLM's context limit.","If not managed, leads to runtime errors and inability of the LLM to complete tasks. Effective management allows the LLM to operate within its architectural constraints, though it might limit the complexity of tasks or the number of examples that can be provided.","['Prompt Compression', 'Tool-Level Demonstrations']","['Prompt engineering for LLMs with limited context windows', 'System design for conversational agents', 'Managing computational constraints of LLMs']",,33,
207,Innovation-Hallucination Trade-off (in Tool Use),"When LLMs are faced with challenging tasks requiring tool compositions not explicitly covered by few-shot exemplars, they may 'innovate' by inferring new logical relationships or tool sequences. However, this innovative behavior is often a 'double-edged sword,' accompanied by 'hallucinations,' where the LLM generates non-existent observations or incorrect plans.","LLM agents attempting to solve complex, novel problems with external tools, where the required compositional logic goes beyond direct examples provided in the prompt, forcing the LLM to generalize or create new strategies.","This pattern describes an observed behavior and a design consideration. Designers must be aware of this trade-off when pushing LLMs to generalize tool use. Potential mitigation strategies (implied but not fully detailed in the paper) could involve implementing more robust self-reflection mechanisms, external verification steps, or fine-tuning with diverse tool-use corpora to guide innovation while minimizing hallucinations.","Leads to a dual outcome: LLMs can find novel solutions (innovation) but also generate plausible yet incorrect information (hallucination), making them less reliable without strong verification or improved foundational reasoning.",['Self-Reflection / Feedback Loop'],"['Understanding LLM capabilities and limitations in complex tool-use scenarios', 'Guiding research into more reliable LLM innovation', 'Designing verification layers for agentic systems', 'Benchmarking LLM generalization abilities in tool use']",,23,
208,Tool-Integrated Reasoning,"Large language models (LLMs) struggle with complex mathematical problems, precise computation, symbolic manipulation, and algorithmic processing, despite their strong natural language understanding and generation capabilities.","Designing AI systems for tasks that require both high-level semantic analysis, planning, and abstract reasoning (where natural language excels) and rigorous operations, intricate calculations, and outsourcing to specialized tools (where programs excel).","Develop an agent that seamlessly interleaves natural language reasoning with program-based tool utilization. The agent generates natural language rationales for analysis and planning, and when precise computation or symbolic manipulation is needed, it generates and executes a program using external tools (e.g., computation libraries, symbolic solvers). The execution output is then integrated back into the natural language reasoning process.","Amalgamates the analytical prowess of language and the computational efficiency of tools, significantly outperforming models that rely solely on natural language or program generation. Enhances LLM capabilities for complex quantitative tasks.","['Program-Aided Language Models (PAL) Prompting', 'Chain-of-Thought (CoT) Prompting', 'Multi-Tool Agent', 'Self-Correction with Tool-Interactive Critiquing']","Mathematical problem solving, scientific computation, data analysis, complex reasoning tasks requiring both abstract thought and precise execution.",,11,
209,Output Space Shaping,"When training LLMs via imitation learning on limited, curated datasets, the model's output space can become restricted, hindering its flexibility in exploring diverse, plausible reasoning trajectories during inference and leading to improper tool-use behavior.","Refining the reasoning behavior of LLMs, particularly those trained with imitation learning on interactive tool-use trajectories, to improve diversity, robustness, and reduce errors.","1. **Sampling:** Apply a sampling strategy (e.g., nucleus sampling) to an initial imitation-learned model to generate diverse candidate trajectories for a given problem. Retain only the valid trajectories (those with correct answers and no tool-use errors).
2. **Correction:** For invalid trajectories (e.g., those that are partially correct but fail later), leverage a 'teacher model' to correct the subsequent portions of the trajectory, thereby transforming invalid paths into valid ones.
3. **Retraining:** Retrain the model on a combined dataset consisting of the initial curated corpus, the newly sampled valid trajectories, and the corrected trajectories.","Encourages the diversity of plausible reasoning steps, mitigates improper tool-use behavior, and significantly boosts reasoning performance and generalization, especially for smaller models.","['Knowledge Distillation (KD) for Trajectory Learning', 'Rejection Sampling Fine-Tuning', 'Self-Correction']","Refining LLM reasoning, expanding output diversity, improving robustness to improper tool use, enhancing generalization, particularly for tool-augmented LLMs.",,42,
210,Imitation Learning with Tool-Use Trajectories,"Training LLMs to effectively integrate natural language reasoning with external tool use requires learning complex, interactive, multi-turn behaviors, but existing datasets often lack annotations for such interleaved rationales and tool interactions.","Developing agentic LLMs that can dynamically use tools, where the desired behavior involves a sequence of natural language thoughts, program generation, tool execution, and response processing.","1. **Trajectory Curation:** Utilize a powerful teacher model (e.g., a larger, more capable LLM like GPT-4) to synthesize high-quality, interactive tool-use trajectories on relevant datasets. These trajectories explicitly demonstrate the interleaved format of natural language reasoning, program generation, and tool output processing.
2. **Supervised Fine-Tuning:** Apply imitation learning (e.g., minimizing negative log-likelihood loss) on this curated corpus of interactive tool-use trajectories to finetune a base LLM.","Enables the student LLM to learn and reproduce the complex interleaved reasoning and tool-use format, leading to significant performance improvements and the acquisition of agentic capabilities in the target domain.","['Supervised Fine-Tuning (SFT) with Rationale Data', 'Knowledge Distillation (KD) for Trajectory Learning', 'Few-Shot Prompting with Interleaved Format']","Training agentic LLMs, acquiring complex interactive behaviors, developing tool-augmented LLMs, transferring advanced reasoning strategies.",,42,
211,Chain-of-Thought (CoT) Prompting,"Large language models (LLMs) often struggle with complex reasoning tasks, providing incorrect or superficial answers, especially for multi-step problems, without showing their intermediate thought processes.","Improving the reasoning ability and transparency of LLMs for tasks that require logical deduction, step-by-step problem-solving, or abstract thinking.","Augment the prompt with instructions or few-shot examples that encourage the LLM to generate intermediate, step-by-step natural language rationales before producing the final answer. This makes the LLM 'think aloud' and break down the problem.","Elicits and enhances the LLM's reasoning capabilities, leading to improved performance on complex tasks, greater interpretability of the model's decision-making process, and better error identification.","['Program-Aided Language Models (PAL) Prompting', 'Tool-Integrated Reasoning', 'Few-Shot Prompting']","Mathematical reasoning, common-sense reasoning, logical puzzles, complex question answering, code generation (with natural language planning).",,26,
212,Program-Aided Language Models (PAL) Prompting,"While natural language reasoning (like CoT) improves LLM performance, LLMs still inherently struggle with precise computation, symbolic manipulation, and algorithmic execution, leading to errors in quantitative or rigorous tasks.","Tasks that demand rigorous operations, intricate calculations, symbolic processing, or the ability to leverage specialized computational tools that LLMs cannot perform reliably on their own.","The LLM is prompted to synthesize and execute programs (e.g., Python code) to solve sub-problems or the entire task. The program's execution output (e.g., numerical results, symbolic simplifications) is then used by the LLM to derive or refine the final answer.","Leverages external computational tools for accuracy and efficiency, overcoming LLM limitations in exact calculations and symbolic processing. It allows LLMs to 'outsource' complex computations to reliable interpreters.","['Tool-Integrated Reasoning', 'Chain-of-Thought (CoT) Prompting', 'Multi-Tool Agent', 'Tools Integration Patterns']","Mathematical problem solving, data analysis, scientific computation, tasks benefiting from external code execution and symbolic manipulation.",,44,
213,Few-Shot Prompting with Interleaved Format,"Guiding LLMs to produce outputs that adhere to a specific, complex, multi-modal structure (e.g., alternating between natural language and code/tool calls) can be challenging without explicit demonstrations.","When training data needs to be curated or when an LLM needs to be guided during inference to perform a task that involves a specific sequence of actions and output formats, such as interactive tool use.","Design a detailed prompt that includes clear instructions on the desired interleaved format and provides several diverse, high-quality examples (few-shot examples). These examples showcase the exact sequence of natural language rationales, program generation, and tool output processing that the LLM should follow.","Effectively guides the LLM to generate outputs that conform to the desired interactive, interleaved format, improving the consistency, structure, and effectiveness of its reasoning and tool-use process. This is crucial for collecting high-quality training data for agentic models.","['Prompt Engineering', 'Tool-Integrated Reasoning', 'Imitation Learning with Tool-Use Trajectories', 'Chain-of-Thought (CoT) Prompting']","Data curation for training tool-augmented LLMs, guiding LLM behavior in interactive agents, ensuring specific output formats in complex reasoning.",,18,
214,ZeRO (Zero Redundancy Optimizer),"Training very large deep learning models (especially LLMs with billions of parameters) is severely constrained by the limited GPU memory, as model parameters, gradients, and optimizer states consume vast amounts of memory.",Training massive neural networks in a distributed environment where memory efficiency is critical to scale model size and batch size beyond single-GPU limits.,"A memory optimization technology that partitions model states (optimizer states, gradients, and optionally model parameters themselves) across data-parallel devices. Instead of replicating all states on every GPU, each GPU only stores a portion, drastically reducing memory redundancy. ZeRO Stage 3 partitions all three components.","Drastically reduces GPU memory consumption, enabling the training of models with hundreds of billions or even trillions of parameters on existing hardware. This improves training efficiency, scalability, and allows for larger batch sizes.","['Data Parallelism', 'Model Parallelism', 'Distributed Training']","Training large-scale LLMs and deep learning models with high memory requirements, enabling research and development of frontier AI models.",,34,
215,Supervised Fine-Tuning (SFT) with Rationale Data,General-purpose pre-trained LLMs may not inherently follow specific reasoning styles (like Chain-of-Thought) or perform optimally on particular downstream tasks without explicit task-specific training.,"Adapting a pre-trained LLM to a specific domain or task where a structured reasoning process (e.g., step-by-step rationales) is desired for improved performance and interpretability.","Collect or generate a dataset of problem-solution pairs, where the solutions are augmented with explicit, step-by-step natural language rationales. Then, finetune the LLM on this dataset using standard supervised learning techniques (e.g., minimizing negative log-likelihood), training it to generate both the rationale and the final answer.","Improves the LLM's ability to generate desired reasoning styles, enhances its performance on the target task by guiding its thought process, and makes its decision-making more transparent.","['Imitation Learning', 'Chain-of-Thought (CoT) Prompting', 'Knowledge Distillation']","Task-specific adaptation of LLMs, instilling reasoning capabilities, improving instruction following, creating domain-specific reasoning agents.",,16,
216,Rejection Sampling Fine-Tuning (RFT),"LLMs can generate diverse reasoning paths, but many of these paths might be incorrect, suboptimal, or lead to wrong answers. Simple supervised fine-tuning on a fixed dataset might not effectively filter out poor reasoning.","Improving the quality, reliability, and diversity of reasoning paths generated by LLMs, especially in tasks with multiple valid solution approaches or where correctness can be objectively verified.","1. **Generate Diverse Paths:** Prompt the LLM (or multiple LLMs) to generate several diverse reasoning paths or solutions for a given problem.
2. **Verify and Select:** Implement a verification mechanism (e.g., automatic checking of final answers, rule-based validation, or an external verifier model) to evaluate the correctness or quality of each generated path.
3. **Fine-Tune:** Only the 'accepted' (correct and high-quality) reasoning paths are used to fine-tune the LLM, effectively 'rejecting' the poor-quality ones.","Produces models that generate more reliable and diverse reasoning, improving overall performance by focusing training on successful problem-solving strategies and implicitly learning to avoid common errors.","['Self-Correction', 'Output Space Shaping', 'Reward Modeling']","Enhancing LLM reasoning capabilities, improving robustness, generating more diverse and correct outputs, reducing hallucinations.",,38,
217,Reinforcement Learning from Human Feedback (RLHF),"Aligning LLMs with complex, subjective human preferences, values, and instructions is difficult using traditional supervised learning, which relies on fixed datasets and cannot capture nuanced human judgments.","After an initial supervised fine-tuning phase, when LLMs can generate diverse outputs but might still produce undesirable, unhelpful, unsafe, or unaligned responses.","1. **Reward Model Training:** Collect human preference data by having annotators rank or score multiple LLM-generated responses for various prompts. Train a separate 'reward model' (RM) to predict these human preferences.
2. **Reinforcement Learning:** Use the trained reward model to provide a reward signal to the LLM during a reinforcement learning phase (e.g., using Proximal Policy Optimization - PPO). The LLM is optimized to maximize this reward, thereby learning to generate outputs that are highly preferred by humans.","Significantly improves the LLM's helpfulness, harmlessness, and adherence to complex instructions, making it more aligned with human expectations, safer, and more user-friendly.","['Supervised Fine-Tuning (SFT)', 'AI-Human Interaction Patterns', 'Personalization Patterns', 'Human-in-the-Loop']","Chatbot development, instruction-following models, safety alignment, improving conversational AI, personalization of LLM behavior.",,12,
218,Multi-Tool Agent,"Complex, real-world tasks often require diverse capabilities that exceed the scope of any single specialized tool or the inherent abilities of an LLM alone.","Building LLM-based agents that need to interact with a varied ecosystem of external resources (e.g., calculators, search engines, symbolic solvers, code interpreters, APIs, databases) to achieve a sophisticated goal.","Design an agent where an LLM acts as the central orchestrator, capable of:
1. **Problem Analysis:** Understanding the task and identifying which types of external tools might be relevant.
2. **Tool Selection:** Dynamically choosing the most appropriate tool(s) for a given sub-problem or step.
3. **Input Generation:** Formulating correct and effective queries or inputs for the selected tool(s).
4. **Execution & Parsing:** Executing the tool(s) and parsing their outputs.
5. **Iterative Refinement:** Using outputs from one tool to inform subsequent reasoning, select other tools, or refine the overall solution in an iterative process.","Enhances the LLM's ability to tackle a wider range of complex, multi-faceted problems, leverages the strengths of various specialized tools, improves accuracy, efficiency, and expands the agent's overall problem-solving scope.","['Tool-Integrated Reasoning', 'Agentic AI Patterns', 'Tools Integration Patterns', 'Planning Patterns']","Complex problem solving, scientific discovery, data analysis, interactive systems, general-purpose AI assistants, automation of multi-step workflows.",,44,
219,Knowledge Distillation (KD) for Trajectory Learning,"Training smaller, more efficient LLMs to achieve complex, interactive behaviors and reasoning performance comparable to larger, more capable teacher models, without requiring the teacher model's computational resources at inference.","Transferring the knowledge embedded in the detailed reasoning process and tool-use strategies of a high-performing (often larger) LLM to a smaller, more deployment-friendly student model.","1. **Teacher Trajectory Generation:** Use a powerful 'teacher' LLM to generate extensive, high-quality, step-by-step reasoning trajectories for a given set of problems. These trajectories capture the teacher's thought process, including interleaved natural language rationales, program calls, and tool interactions.
2. **Student Fine-Tuning:** Finetune a 'student' LLM on this teacher-generated data. The student model learns to imitate the teacher's behavior by minimizing a loss function (e.g., negative log-likelihood) over the generated sequences, effectively learning the teacher's reasoning and tool-use patterns.","Enables smaller models to acquire sophisticated reasoning and tool-use capabilities from larger models, leading to significant performance improvements while being more computationally efficient for deployment. It 'compresses' the teacher's expertise into the student.","['Imitation Learning with Tool-Use Trajectories', 'Supervised Fine-Tuning (SFT)', 'Model Compression']","Model compression, transferring specialized skills to smaller models, improving efficiency of agentic LLMs, creating domain-specific expert models.",,16,
220,Self-Correction with Tool-Interactive Critiquing,"LLMs can make errors in complex reasoning tasks (e.g., logical flaws, computational mistakes, incorrect tool usage), and a single-pass generation often leads to suboptimal or incorrect solutions that lack robustness.","Improving the reliability, accuracy, and robustness of LLM-generated solutions, particularly in tasks where errors can be identified and corrected through external feedback or internal reflection.","1. **Initial Generation:** The LLM generates an initial solution or a reasoning path (e.g., rationale, program).
2. **Critiquing and Tool Feedback:** A 'critic' component (which can be the LLM itself, or another LLM/module) evaluates the generated output. This evaluation often involves executing generated code with external tools, checking logical consistency, or comparing against expected outcomes. The tool feedback (e.g., runtime errors, incorrect outputs) highlights discrepancies.
3. **Revision:** Based on the critique and tool feedback, the LLM identifies errors, analyzes the failure, and revises its original solution or reasoning path. This process can be iterative, allowing for multiple rounds of generation, critique, and refinement until a satisfactory result is achieved or a stopping condition is met.","Enhances the reliability and accuracy of LLM outputs, allows the model to recover from initial errors, and leads to more robust problem-solving, especially in complex and error-prone domains like mathematical reasoning and code generation.","['Agentic AI Patterns', 'Rejection Sampling Fine-Tuning', 'Iterative Refinement', 'Tools Integration Patterns']","Complex problem solving, code generation and debugging, mathematical reasoning, factual consistency checking, improving robustness against errors.",,38,
221,Backward Reasoning for Verification,"Verifying the correctness of a complex, multi-step solution or reasoning chain generated by an LLM can be challenging, especially when errors might be subtle or accumulate over steps, and a simple forward check is insufficient.","Tasks where the desired final state or answer is known or can be objectively checked, and where the correctness of intermediate steps can be rigorously assessed by working backward from the goal.","1. **Forward Generation:** An LLM generates a solution or a sequence of reasoning steps from the problem statement to a proposed answer.
2. **Backward Verification:** A 'verifier' component (which can be the LLM itself, or a specialized module) starts from the proposed final answer (or a later intermediate step) and logically works backward through the generated steps to the initial problem statement.
3. **Consistency Check:** At each backward step, the verifier checks for logical consistency with the preceding steps (in the backward direction) or known facts. This can involve symbolic checks, tool execution, or formal logical inference. Any inconsistency flags an error.","Provides a robust method for verifying the validity of LLM-generated solutions, helps in identifying errors earlier in the reasoning chain, and significantly improves the overall reliability and trustworthiness of the system's outputs.","['Self-Correction', 'Knowledge & Reasoning Patterns', 'Planning Patterns']","Theorem proving, mathematical reasoning, logical puzzles, formal verification of code, debugging complex reasoning chains.",,38,
222,Iterative Retrieval-Generation Synergy,"LLMs can suffer from factual inaccuracies, hallucinations, or lack up-to-date domain-specific information. A single retrieval step might not provide all necessary context for complex or evolving generation tasks.","Enhancing the factual grounding, comprehensiveness, and dynamism of LLM-generated text, particularly for knowledge-intensive or investigative tasks that benefit from continuous external information gathering.","1. **Initial Generation/Query:** The LLM generates an an initial response or formulates a query based on the prompt.
2. **Retrieval:** This query (or parts of the generated response) is used to retrieve relevant information from an external knowledge base (e.g., search engines, databases, document repositories).
3. **Contextualized Generation:** The retrieved information is integrated into the prompt or context and fed back to the LLM to refine, expand, or continue its generation.
4. **Iteration:** This cycle of retrieval and generation is repeated iteratively. The LLM can generate new queries based on prior retrievals and generations, progressively gathering more context and refining its output until a comprehensive and accurate response is formed.","Improves the factual accuracy, relevance, and depth of LLM-generated content by dynamically incorporating and iteratively refining based on external knowledge. Reduces hallucination and supports more informed, nuanced, and up-to-date responses.","['Retrieval-Augmented Generation (RAG)', 'Multi-Tool Agent', 'Knowledge & Reasoning Patterns', 'Tools Integration Patterns']","Complex question answering, summarization, report generation, knowledge synthesis, domain-specific content creation, research assistance.",,3,
223,Tool Learning with Onestep Task Solving,"Large Language Models (LLMs) need to interact with external tools to solve complex tasks, but early approaches lacked adaptability to dynamic feedback and potential errors during execution.","This paradigm is found in early studies on tool learning, where the primary objective was to leverage tools for problem-solving without complex iterative refinement. User queries require assistance from external tools.","Upon receiving a user question, the LLM analyzes the request to understand user intent and immediately plans *all* subtasks needed to solve the problem. It then selects the appropriate tools, calls them, and directly generates a response based on the results returned by these tools, without considering the possibility of errors or altering the plan based on tool feedback during the process.","LLMs can solve complex problems by integrating external tool capabilities and providing tool-augmented responses. However, this approach lacks robustness and adaptability to execution failures or dynamic changes in the environment, as it commits to a complete task plan upfront.","['Task Planning', 'Tool Selection', 'Tool Calling', 'Response Generation']","['Toolformer (17)', 'Chameleon (69)', 'HuggingGPT (89)']",,33,
224,Tool Learning with Iterative Task Solving,"The 'Onestep Task Solving' paradigm lacks robustness and adaptability, failing to account for potential errors or dynamic changes that may occur during tool execution. LLMs need a mechanism to refine their plans based on real-time feedback from tools.",This paradigm is particularly suited for complex real-world tasks where initial plans might fail or require adjustments based on the actual outputs received from tools. The system needs to be resilient and adaptive to uncertainties.,"Instead of committing to a complete task plan upfront, the LLM engages in iterative interactions with external tools. It adjusts the subtasks progressively based on the feedback received from tool executions. This allows the LLM to address the problem step-by-step, continuously refining its plan in response to the results returned by tools, and handling errors by reassessing tool selection or modifying the plan.","Enhanced problem-solving capabilities, improved robustness, and greater adaptability to dynamic environments and execution failures. This approach fosters more effective human-machine collaboration by allowing for transparency in decision-making and error handling.","['Task Planning', 'Tool Selection', 'Tool Calling', 'Response Generation']","['ToolLLaMA (18)', 'APIBank (30)', 'Confucius (34)', 'RestGPT (93)']",,33,
225,Task Planning,"User queries in real-world scenarios are often complex and embody multi-step intent. LLMs need to analyze and structure these complex requests by breaking them down into simpler, solvable sub-questions and identifying their dependencies.","This is the preliminary stage of the tool learning workflow, where a user provides a complex query that cannot be answered directly by the LLM or a single tool.","The LLM conducts a comprehensive analysis of the user's intent, decomposing the original query into multiple discrete sub-questions. Additionally, it delineates the dependency relationships and determines the optimal execution sequence among these decomposed tasks, thereby establishing interconnections between the sub-questions. This stage can employ either tuning-free or tuning-based methods.","Complex user queries are transformed into a structured, sequential plan of simpler sub-questions, making them amenable to tool-assisted resolution. This improves the LLM's logical analysis capabilities and its ability to handle multi-step tasks effectively.","['Tool Learning with Onestep Task Solving', 'Tool Learning with Iterative Task Solving', 'Tool Selection']","['CoT (91)', 'ReACT (92)', 'ART (50)', 'RestGPT (93)', 'HuggingGPT (89)', 'TPTU (94)', 'ToolChain (95)', 'ControlLLM (96)', 'Attention Buckets (97)', 'PLUTO (98)', 'ATC (99)', 'SGC (100)', 'Sum2Act (101)', 'BTP (102)', 'DRAFT (103)', 'Toolformer (17)', 'TaskMatrixAI (104)', 'Toolink (105)', 'TPTUv2 (106)', 'UMi (107)', 'COA (108)', 'DEER (109)', 'OpenAGI (110)', 'SOAY (111)', 'TPLLAMA (112)', 'APIGen (113)', 'ToolPlanner (181)']",,33,
226,Tool Selection,"After task planning, the LLM needs to identify the most appropriate tools from a potentially vast array of available tools to address specific sub-questions. This process is challenged by context length limitations and latency constraints when dealing with a large number of tools.","This stage follows task decomposition. For each sub-question, the system must choose one or more tools that can effectively resolve it. This is especially critical in real-world systems with a vast number of tools.","The tool selection process involves either an initial filtering step using a retriever or direct selection by the LLM from a provided list. When the number of tools is large, a retriever (term-based or semantic-based) is typically used to identify the top-K most relevant tools. Subsequently, or if the initial tool set is limited, the LLM selects the optimal tool based on tool descriptions, parameter lists, and the sub-question. This often requires sophisticated reasoning, particularly for serial tool calling where the output of one tool serves as input for another.","The LLM efficiently identifies and selects the most suitable tools for each sub-question, effectively managing context length and latency issues in environments with numerous tools. This leads to improved accuracy and relevance in tool usage.","['Task Planning', 'Tool Calling']","['TFIDF (114)', 'BM25 (115)', 'SentenceBert (116)', 'ANCE (117)', 'TASB (118)', 'Contriever (119)', 'coCondensor (120)', 'CRAFT (121)', 'ProTIP (122)', 'ToolRerank (123)', 'COLT (124)', 'CoT (91)', 'ReACT (92)', 'ToolLLaMA (18)', 'Confucius (34)', 'ToolBench (33)', 'RestGPT (93)', 'HuggingGPT (89)', 'ChatCoT (125)', 'ToolNet (126)', 'ToolVerifier (127)', 'TRICE (128)', 'AnyTool (129)', 'GeckOpt (130)']",,33,
227,Tool Calling,"After selecting a tool, the LLM must accurately extract all required parameters from the user query and format them precisely according to the tool's specifications to successfully invoke the tool. Incorrect parameter content, format, or superfluous sentences can lead to tool call failures.","This stage immediately follows tool selection, where the LLM is prepared to interact with an external tool. It demands precise parameter identification and strict adherence to the tool's API specifications.","The LLM extracts the necessary parameters (both content and format) from the user query, guided by the tool's documentation. It then generates the request to the tool server, strictly adhering to the prescribed output format. Critical to this solution are integrated error handling mechanisms designed to refine actions based on error messages returned upon calling failure, ensuring a more resilient and adaptive system.","Successful invocation of external tools with correctly formatted parameters, enabling the LLM to access and utilize external functionalities. The inclusion of error handling mechanisms enhances the system's resilience and continuity in tool learning.","['Tool Selection', 'Response Generation']","['RestGPT (93)', 'Reverse Chain (131)', 'ControlLLM (96)', 'EasyTool (132)', 'ToolNet (126)', 'ConAgents (133)', 'Gorilla (53)', 'GPT4Tools (134)', 'ToolkenGPT (54)', 'Themis (135)', 'STE (136)', 'ToolVerifier (127)', 'TRICE (128)']",,33,
228,Response Generation,"Tool outputs are diverse, complex, and can be in various formats (e.g., text, numbers, code, videos, images), making direct presentation to users often impractical or suboptimal. LLMs need to synthesize this information with their internal knowledge to construct a comprehensive, user-friendly, and accurate response.","This is the final stage of the tool learning workflow, where the LLM has received and processed outputs from one or more external tools and needs to formulate a coherent and helpful answer for the user.","Upon receiving outputs from tools, the LLM synthesizes the information relevant to the user's query and integrates it with its own internal knowledge to construct a comprehensive response. Methods address challenges such as overly lengthy tool outputs and the inclusion of multiple modalities. Solutions include direct insertion of tool outputs or more sophisticated information integration techniques.","A comprehensive, accurate, and user-friendly response that effectively combines the LLM's internal knowledge with real-time or specialized information obtained from external tools. This approach enhances user experience and can help mitigate biases and harmful content originating from the LLM itself, provided tool outputs are rigorously validated.",['Tool Calling'],"['TALM (26)', 'Toolformer (17)', 'ToolkenGPT (54)', 'RestGPT (93)', 'ToolLLaMA (18)', 'ReCOMP (137)', 'ConAgents (133)']",,33,
229,Data Lake,We cannot foresee the kind of analyses that will be performed on the data and which frameworks will be used to perform these analyses.,"ML application systems dealing with diverse and evolving data analysis requirements, where the types of future analyses and frameworks are unknown.","Store data, ranging from structured to unstructured, as raw as possible in a centralized data repository. This repository should allow parallel analyses of different kinds and with different frameworks.","Improved flexibility for future data analyses, accommodating various frameworks and analytical approaches.","[{'Name': 'Gateway Routing Architecture', 'Relationship': 'uses'}, {'Name': 'Distinguish Business Logic from ML Models', 'Relationship': 'uses'}]",5,,45,
230,Distinguish Business Logic from ML Models,"ML application systems are complex because their ML components must be retrained regularly and have an intrinsic nondeterministic behavior. Business requirements and ML algorithms change over time, making it difficult to change them without impacting the rest of the business logic.","ML application systems where business logic and ML models evolve independently and require clear separation for maintainability, debugging, and adaptability to changing requirements.","Separate the business logic and the inference engine, loosely coupling the business logic and ML-specific dataflows. Define clear APIs between traditional and ML components, placing them into distinct architectural layers (Data Layer, Logic Layer, Presentation Layer) and dividing data flows accordingly.","Easier debugging, improved maintainability, and the ability to monitor and adjust ML components independently to meet user requirements and changing inputs.","[{'Name': 'Gateway Routing Architecture', 'Relationship': 'similar to'}, {'Name': 'ClosedLoop Intelligence', 'Relationship': 'can be combined with'}, {'Name': 'DataAlgorithmServingEvaluator', 'Relationship': 'can be combined with'}, {'Name': 'Data Lake', 'Relationship': 'uses'}]",4,,28,
231,Microservice Architecture,ML applications may be confined to some known ML frameworks and miss opportunities for more appropriate frameworks.,"ML applications needing to integrate or swap diverse ML frameworks easily, and to make ML capabilities available to other parts of the system.",Enable data scientists to make ML frameworks available through microservices.,Increased flexibility in using and swapping ML frameworks and easier integration into broader systems.,"[{'Name': 'Daisy Architecture', 'Relationship': 'uses'}, {'Name': 'Event-driven ML Microservices', 'Relationship': 'uses'}]",4,,45,
232,DataAlgorithmServingEvaluator,Prediction systems should connect different pieces in the data processing pipeline into one coherent system and prototyping predictive model.,"Predictive model systems requiring clear separation and connection of data processing, algorithms, serving, and evaluation components for coherence and ease of prototyping.","Separate components like MVC for ML: data, data source and data preparator, algorithms, serving, and evaluator.",A coherent system for prediction and easier prototyping of predictive models.,"[{'Name': 'ClosedLoop Intelligence', 'Relationship': 'can be combined with'}]",2,,4,
233,Event-driven ML Microservices,"Due to frequent prototyping of ML models and constant changes, development teams must be agile to build, deploy, and maintain complex data pipelines.","ML systems with rapidly evolving models and data pipelines that require agility in development, deployment, and maintenance.","Construct pipelines by chaining together multiple microservices, where each microservice listens for data arrival and performs its designated task.","Agile development, deployment, and maintenance of complex data pipelines.","[{'Name': 'Microservice Architecture', 'Relationship': 'uses'}]",2,,45,
234,Lambda Architecture,"Real-time data processing requires scalability, fault tolerance, predictability, and extensibility.","Systems needing to process both batch and real-time data streams, with high demands for scalability, fault tolerance, predictability, and extensibility.","Employ a batch layer that produces views at set intervals, a speed layer that creates real-time views, and a serving layer that orchestrates queries by merging results from both batch and speed layers.","Scalable, fault-tolerant, predictable, and extensible real-time data processing capabilities.",[],2,,32,
235,ParameterServer Abstraction,"For distributed learning, widely accepted abstractions for managing shared parameters are lacking.",Distributed machine learning systems where global parameters need to be shared and managed across multiple worker nodes.,"Distribute both data and workloads over worker nodes, while server nodes maintain globally shared parameters represented as vectors and matrices.","Provides a structured and abstracted way to manage shared parameters in distributed learning, improving efficiency and coordination.",[],2,,30,
236,Daisy Architecture,The ability to scale content production processes must be acquired via the use of ML. Then the coverage of that tooling must be extended over as much of their remaining content.,Organizations aiming to scale content production processes using ML and extend the reach of ML tooling across their content.,"Utilize Kanban scaling and microservices to realize pull-based, automated, on-demand, and iterative processes.","Scalable, automated, on-demand, and iterative content production processes leveraging ML.","[{'Name': 'Microservice Architecture', 'Relationship': 'uses'}]",1,,19,
237,Gateway Routing Architecture,"When a client uses multiple services, it can be difficult to set up and manage individual endpoints for each service.","Systems where clients interact with multiple backend services, requiring simplified endpoint management and unified access.","Install a gateway before a set of applications, services, or deployments, and use application layer routing to direct requests to the appropriate instance.",Simplified client interaction with multiple services and easier management of service endpoints.,"[{'Name': 'Data Lake', 'Relationship': 'uses'}, {'Name': 'Distinguish Business Logic from ML Models', 'Relationship': 'similar to'}]",1,,45,
238,Kappa Architecture,It is necessary to deal with huge amounts of data with less code resource.,Systems needing to process large volumes of data efficiently with minimal code and resource overhead.,Support both real-time data processing and continuous reprocessing with a single stream processing engine.,"Efficient processing of huge data volumes with less code, supporting both real-time and reprocessing needs through a unified approach.",[],1,,32,
239,ClosedLoop Intelligence,"It is necessary to address big, open-ended, time-changing, or intrinsically hard problems.","AI systems tackling complex, evolving, and challenging problems that require continuous feedback and adaptation to improve performance.",Connect machine learning to the user and close the loop. Design clear interactions along with implicit and direct outputs.,"AI systems that can adapt and improve over time by incorporating user feedback and continuous learning, effectively addressing complex problems.","[{'Name': 'Undeclared Consumers', 'Relationship': 'mitigates'}, {'Name': 'Distinguish Business Logic from ML Models', 'Relationship': 'can be combined with'}, {'Name': 'DataAlgorithmServingEvaluator', 'Relationship': 'can be combined with'}]",0,,11,
240,Federated Learning,"Standard machine learning approaches require centralizing the training data on one machine or in a datacenter, which can raise privacy and logistical concerns.","Scenarios where training data cannot be centralized due to privacy, security, or logistical constraints (e.g., data residing on mobile devices).","Employ Federated Learning, which enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on the device.","Collaborative model training without centralizing sensitive data, preserving privacy and reducing data transfer costs.","[{'Name': 'Secure Aggregation', 'Relationship': 'uses'}]",0,,41,
241,ML Versioning,"ML models and their several versions may change the behavior of the overall ML applications, leading to reproducibility and traceability issues.","ML applications requiring reproducibility, traceability, and robust management of model evolution and its impact on system behavior.","Record the ML model structure, training data, and training system to ensure a reproducible training process.","A reproducible training process, better understanding, and management of model changes and their impact on ML applications.",[],4,,0,
242,Wrap BlackBox Packages into Common APIs,"Using generic, independent ML frameworks often results in different glue code for each framework, leading to a massive amount of supporting code for data ingress/egress.","Integrating multiple ML frameworks or black-box components into an application, which typically leads to complex, redundant, and costly integration code.",Wrap black-box packages into common APIs to make supporting infrastructure more reusable and to reduce the cost of changing packages.,"Reduced 'glue code', increased reusability of infrastructure, and a lower cost associated with swapping or updating ML packages.","[{'Name': 'Glue Code', 'Relationship': 'mitigates'}]",4,,45,
243,Test Infrastructure Independently from ML,It is difficult to identify errors when infrastructure and machine learning components are mixed.,"Testing ML systems where infrastructure and ML components are tightly coupled, making error diagnosis challenging.","Ensure that the infrastructure is testable and the learning parts of the system are encapsulated, allowing everything around the ML components to be tested independently.",Easier error identification and improved testability of both the infrastructure and the ML components.,"[{'Name': 'Separation of Concerns and Modularization of ML Components', 'Relationship': 'related to'}]",3,,0,
244,Handshake,"An ML system depends on inputs delivered outside of the normal release process, making it vulnerable to unannounced changes.","ML systems relying on external, often changing, data inputs or dependencies that are not part of the standard release cycle.","Create a handshake normalization process, regularly check for significant changes in inputs, and send alerts when detected.","Early detection of significant changes in external inputs, preventing unexpected behavior or failures in the ML system.",[],2,,0,
245,Isolate and Validate Output of Model,"Machine learning models are known to be unstable and vulnerable to adversarial attacks, noise in data, and data drift over time.","Deploying ML models that might be unreliable, susceptible to attacks, or degrade in performance over time due to various factors.",Encapsulate ML models within rule-based safeguards and use redundant and diverse architecture to mitigate and absorb the low robustness of ML models.,"Increased robustness and reliability of ML models in production, offering protection against instability, adversarial attacks, and data drift.",[],2,,0,
246,Canary Model,A surrogate ML model that approximates the behavior of the best ML model must be built to provide explainability.,"Situations requiring monitoring of prediction differences, potentially for explainability or detecting performance degradation of a new model.",Run the canary inference pipeline in parallel with the primary inference pipeline to monitor prediction differences.,"Enables monitoring of prediction differences between a canary model and the primary model, useful for explainability or detecting issues before full deployment.",[],1,,0,
247,Decouple Training Pipeline from Production Pipeline,It is necessary to separate and quickly change the ML data workload and stabilize the training workload to maximize efficiency.,"ML systems where training and production (serving) workloads have different requirements for resources, stability, and frequency of change.",Physically isolate different workloads to different machines. Then optimize the machine configurations and network usage for each workload.,"Maximized efficiency, the ability to quickly change the ML data workload, and a stabilized training workload.",[],1,,43,
248,Descriptive Data Type for Rich Information,"The rich information used and produced by ML systems is often encoded with plain data types like raw floats and integers, obscuring semantic meaning.",ML systems where the semantic meaning of model parameters and predictions is lost or difficult to infer due to the use of plain data types.,"Design a robust system where model parameters know their specific role (e.g., log-odds multiplier, decision threshold) and predictions carry information about the model they originated from.","Improved robustness and better understanding of model parameters and predictions through richer, semantically meaningful data types.","[{'Name': 'PlainOldData Type Smell', 'Relationship': 'mitigates'}]",1,,0,
249,Design Holistically about Data Collection and Feature Extraction,"The system to prepare data in an ML-friendly format may become a 'pipeline jungle,' making these pipelines difficult and costly to manage.","ML systems with complex, sprawling, and unmanaged data collection and feature extraction pipelines leading to high costs and difficulty.","Avoid 'pipeline jungles' by thinking holistically about data collection and feature extraction processes, which can dramatically reduce ongoing costs.",Reduced pipeline complexity and lower ongoing costs for data preparation by adopting a holistic design approach.,"[{'Name': 'Pipeline Jungles', 'Relationship': 'mitigates'}]",1,,4,
250,Reexamine Experimental Branches Periodically,The code-paths accumulated by individual changes can create a growing technical debt due to increasing difficulties in maintaining backward compatibility.,ML development environments with numerous experimental branches that lead to accumulating technical debt and challenges in maintaining backward compatibility.,"Reexamine each experimental branch periodically to identify and remove unnecessary code, such as 'glue code' and 'pipeline jungles'.","Reduced technical debt, elimination of unnecessary code, and improved maintainability of the ML system.","[{'Name': 'Dead Experimental Codepaths', 'Relationship': 'mitigates'}, {'Name': 'Glue Code', 'Relationship': 'mitigates'}, {'Name': 'Pipeline Jungles', 'Relationship': 'mitigates'}]",1,,36,
251,Reuse Code between Training Pipeline and Serving Pipeline,Training/serving skew can be caused by a discrepancy between how data in the training and serving pipelines are handled.,ML systems experiencing performance degradation or incorrect behavior due to inconsistencies in data handling between training and serving environments.,Reuse code between the training pipeline and serving pipeline by preparing objects that store results in an understandable way for humans.,"Reduced training/serving skew, improved consistency in data handling across pipelines, and more understandable results.",[],0,,43,
252,Separation of Concerns and Modularization of ML Components,ML applications must accommodate regular and frequent changes to their ML components.,ML applications requiring high flexibility and adaptability to frequent changes in their ML components.,"Decouple ML components at different levels of complexity, from the simplest to the most complex.","Increased flexibility, easier accommodation of changes, and improved maintainability of ML applications.","[{'Name': 'Big Ass Script Architecture', 'Relationship': 'mitigates'}, {'Name': 'Test Infrastructure Independently from ML', 'Relationship': 'related to'}]",0,,28,
253,Secure Aggregation,"The system needs to communicate and aggregate model updates in a secure, efficient, scalable, and fault-tolerant way.","Distributed ML systems, such as Federated Learning, that require secure and efficient aggregation of model updates from multiple sources while preserving data privacy.",Encrypt data from each mobile device in Federated Learning and calculate totals and averages without individual examination.,"Secure, efficient, scalable, and fault-tolerant aggregation of model updates, preserving individual data privacy during the aggregation process.","[{'Name': 'Federated Learning', 'Relationship': 'uses'}]",0,,41,
254,Workflow Pipeline,Creating an end-to-end reproducible training and deployment pipeline for a machine learning component is difficult. Data science notebooks can run a whole pipeline but they do not scale.,ML projects requiring scalable and reproducible training and deployment pipelines.,Make each pipeline step a separate containerized service. Services are orchestrated and chained together to form pipelines that can be run via REST API calls.,"The portability, scalability, and maintainability of the individual pipeline steps is improved at the cost of an overall more complex solution.",,Presented at AWS Blog.,MLOps Patterns,43,
255,Two-Phase Predictions,Executing large complex models can be time-consuming and costly especially if lightweight clients like mobile or IoT devices are involved.,"Applications where predictions are needed on lightweight clients (mobile, IoT) and involve large, complex models.",Split the prediction into two phases. A simple fast model is executed first on the client. Afterwards a large complex model is optionally executed in the cloud for deeper insights.,Prediction response time is reduced for some cases. The number of large expensive predictions is reduced. The client has a fallback model when there is no Internet connection.,,Voice activation in AI assistants like Alexa or Google Assistant.,MLOps Patterns,39,
256,Encapsulating ML Models within Rule-based Safeguards,"It is impossible to guarantee the correctness of ML model predictions so they should not be directly used for safety or security-related functions. Furthermore ML models can be unstable and vulnerable to adversarial attacks, data noise and drift.","Systems where ML model predictions are used for safety or security-critical functions, or where model instability, adversarial attacks, data noise, and drift are concerns.",Introduce a deterministic rule-based mechanism that decides what to do with the prediction results eg based on additional quality checks.,Reduced risk for negative impacts of incorrect predictions but a more complex architecture.,,,AI–Human Interaction Patterns,0,
257,AI Pipelines,Complex prediction or synthesis use cases are often difficult to accomplish with a single AI tool or model.,Use cases requiring complex prediction or synthesis that cannot be handled by a single AI tool or model.,Divide the problem into smaller consecutive steps then combine several existing AI tools or custom models into an inferencetime AI pipeline where each specialized tool or model is responsible for a single step.,More tools and models need to be integrated but the provided is result is of higher quality Each step can be optimized individually.,Pipes and Filters,Typical computer vision inference pipelines.,MLOps Patterns,39,
258,Ethics Credentials,Responsible AI requirements are either omitted or mostly stated as highlevel objectives and not specified explicitly in a verifiable way as expected system outputs. Because of this users may trust an AI system less or even refrain from using it.,"AI systems where user trust and ethical compliance are critical, and where responsible AI requirements need to be verifiable.",Provide verifiable ethics credentials for your AI system or component Using publicly accessible and trusted data infrastructure the credentials can be verified as proof of ethical compliance Additionally users may also have to verify their credentials before getting access to the AI system.,Trust and system acceptance increases and awareness of ethical issues is raised However a trusted public data infrastructure is needed and credentials need to be maintained and potentially refreshed from time to time.,,,AI–Human Interaction Patterns,21,
259,MultiLayer Pattern,An application comprises several groups of subtasks each of which is at a different level of abstraction.,Applications with subtasks at different levels of abstraction.,Divide the application into different layers Each layer consisting of submodules can be independently designed to feed into foreign prototypes Each layer has an input and corresponding output to the succeeding layer The succeeding return is used as a feed to the next layers for further processing Every layer communicates only with its direct neighbor.,This pattern enables inference of results at the individual steps Extra components can be added or modified to meet the computational requirements giving it high flexibility.,,Self-learning student platform.,Classical AI,25,
260,Continuous Integration and Deployment,Reduce the risk of releasing broken applications.,Software development projects aiming to reduce the risk of releasing broken applications through automated processes.,Always build with unit and component tests and deploy with verification tests using code that itself is under version control After you commit to a development branch the system deploys to a development environment Once all endtoend and manual smoke testing is complete a manual action deploys to production.,,"Continued Model Evaluation, End-to-End Tests",easemlci presented in 58,MLOps Patterns,36,
261,Strategy Pattern,How can an ML model that performs a task in a given context be flexibly changed?,Systems requiring flexible switching or behavior modification of ML models based on context.,Define an interface strategy that different models implement The context will call the methods exposed by the interface and the implemented models will behave differently based on the contextual data.,Switching models or achieving flexibility in model behavior is easier but code complexity is increased.,,XGBoosts custom objective functions Hugging Faces pipeline interface.,Classical AI,46,
262,Deploy Canary Model,You trained a new model with assumed better prediction quality but its not certain if this will carry over to production Additionally there could be other quality issues with the new model that should not affect all users in production at once.,Deploying new ML models to production where uncertainties exist regarding their real-world performance or potential side effects.,Deploy the new model in addition to the existing ones and route a small number of requests to it to evaluate its performance If this test is successful all existing models can be replaced If not the new model needs to be improved.,Only a small number of users are subjected to potential bugs or lowquality predictions Additional serving and monitoring infrastructure is required.,,,MLOps Patterns,0,
263,Batch Serving,Predictions need to be carried out asynchronously over large volumes of data contrary to predictions for small individual requests eg generating personalized playlists every week This is only applicable if there is no need for nearrealtime predictions.,Scenarios requiring asynchronous predictions on large datasets where near-realtime results are not necessary.,Use distributed data processing infrastructure eg based on MapReduce to asynchronously carry out complex ML inference tasks on a large number of computing nodes The individual predictions are aggregated back into a single result.,Positive You can manage server resources flexibly and in strict accordance with demand You may rerun the job in case of error There is no requirement for high availability in your server system Negative You need a job management server This pattern depends on the ability to split a task across multiple workers.,Stateless Serving Function,,MLOps Patterns,32,
264,Lambda Architecture Pattern,Different components in a system have different performance requirements that need to be satisfied eg some are focused on throughput and others on response time.,Systems dealing with large datasets where both batch processing (high throughput) and real-time processing (low latency) are required.,Group the components based on their latency requirements into three layers 1 batch layer ingests and stores large amounts of data 2 speed layer processes updates to the data in lowlatency 3 serving layer provides precalculated results in a lowlatency fashion.,,,,Classical AI,32,
265,Distinguish Business Logic from ML Model,Machine Learning ML systems are complex because their ML components must be retrained regularly and have an intrinsic nondeterministic behavior Similar to other systems the business requirements for these systems as well as ML algorithms change over time.,"ML systems where business logic and ML model logic are intertwined, leading to complexity in maintenance and adaptation to changing requirements or model retraining.",Define clear APIs between traditional and ML components Place business and ML components with different responsibilities into three layers Divide data flows into three.,,,,Classical AI,28,
266,Microservice Vertical Pattern,When to use when you need to run several inferences in order or when you have several inferences and they have dependencies.,Scenarios where multiple ML inferences need to be executed sequentially or have dependencies on each other.,The pattern deploys prediction models in separate servers or containers as services You execute prediction requests from top to bottom synchronously and gather the results to respond to the client.,,,,Tools Integration Patterns,9,
267,Microservice Horizontal Pattern,When to use when the workow can execute multiple predictions in parallel or when you want to integrate prediction results at last. Required to run several predictions to one request.,"Scenarios where multiple ML predictions can be run in parallel for a single request, or where their results need to be integrated at the end.",Multiple are deployed models in parallel You can send one request to the models at once to acquire multiple predictions or an integrated prediction.,,,,Tools Integration Patterns,9,
268,Role Profiling,"Autonomous agents need to assume specific roles (e.g., coders, teachers, domain experts) to perform diverse tasks effectively, mimicking human specialization.","Designing an LLM-based autonomous agent that needs to exhibit specific behaviors, personalities, or expertise relevant to a defined role within an environment or task.","The agent is equipped with a profiling module that defines its role. This can be achieved through: 1. **Handcrafting:** Manually specifying the agent's profile information (e.g., name, objectives, personality traits) in the prompt. 2. **LLM-Generation:** Using an LLM to automatically generate diverse agent profiles based on predefined rules or few-shot examples. 3. **Dataset Alignment:** Extracting agent profiles from real-world datasets (e.g., demographic backgrounds) to reflect attributes of a real population.","The agent's behavior and responses are influenced by its assigned role, leading to more focused, consistent, and human-like task performance. It lays the foundation for memory, planning, and action procedures.",['Prompt Engineering for Capability'],"['Simulating human cognitive processes', 'Software development teams (MetaGPT, ChatDev, Selfcollaboration)', 'Exploring personality traits displayed in LLM-generated texts (PTLLM)', 'Studying toxicity of LLM output by manually prompting with different roles', 'Predicting social developments via agent simulation (combining strategies)']",,5,
269,In-Context Memory (Unified Short-Term Memory),"LLM-based agents need to perceive recent or contextually sensitive information and maintain internal states to guide their immediate actions, but LLMs have a limited context window.","Agents operating in dynamic environments where recent observations, current states, or immediate task plans are crucial for guiding the next action, and the total memory required fits within the LLM's context window.","The agent's short-term memory is implemented by directly writing relevant input information (e.g., conversation states, scene graphs, environment feedback, task plans, scene descriptions, monster information, previous summaries) into the LLM's prompt, realized by in-context learning.","Enhances the agent's ability to perceive and react to recent or contextually sensitive behaviors and observations, making actions more guided by immediate context. However, it is limited by the LLM's context window length and ability to handle long contexts.","['Prompt Engineering for Capability', 'Hybrid Memory System']","['Conversation agents maintaining internal states (RLP)', 'Embodied agents for task planning (SayPlan)', 'Game agents for story creation and narration (CALYPSO)', 'Minecraft agents generating task plans (DEPS)']",,29,
270,Hybrid Memory System,"The limited context window of LLMs restricts the amount of comprehensive memories that can be incorporated into prompts, hindering long-range reasoning, consistent behavior, and accumulation of valuable experiences for complex tasks.","Agents need to operate effectively in complex, dynamic environments requiring both immediate contextual awareness and access to a broad history of past behaviors, thoughts, and consolidated knowledge.","The agent employs a memory module that explicitly models both short-term and long-term memories. Short-term memory (e.g., context window) temporarily buffers recent perceptions, while long-term memory (e.g., external vector database or structured storage) consolidates and stores important information over extended periods, which can be retrieved as needed (e.g., using embedding similarities).","Agents gain the ability for long-range reasoning, maintain consistency in behavior, accumulate valuable experiences, and overcome the context window limitations of LLMs. This is crucial for accomplishing tasks in complex environments.","['In-Context Memory', 'Contextual Memory Retrieval', 'Adaptive Memory Management', 'Self-Reflective Learning']","['Simulating human daily life (Generative Agent, AgentSims)', 'Planning in open-world environments (GITM)', 'Retaining condensed insights from feedback (Reflexion)', 'Reasoning over complex contextual dialogues (SCM)', 'Enhancing model accuracy while guaranteeing user privacy (SimplyRetrieve)', 'Storing and sharing memory objects across conversations (MemorySandbox)']",,29,
271,Contextual Memory Retrieval,"With a large volume of stored long-term memories, agents need an efficient and effective mechanism to extract only the most meaningful and relevant information from their history to inform current actions and decisions.","An agent with a long-term memory system (e.g., Hybrid Memory System) that needs to query and retrieve specific past experiences, knowledge, or successful actions based on the current task or situation.","Memory reading operations are designed to extract valuable information based on a combination of criteria: recency (how recent the memory is), relevance (how similar it is to the current query/context), and importance (an intrinsic value of the memory). Scoring functions are used to quantify these criteria, and retrieved memories are then used to enhance agent actions.","Agents can efficiently access and utilize pertinent past information, leading to more informed, consistent, and effective actions without being overwhelmed by irrelevant data.","['Hybrid Memory System', 'Adaptive Memory Management', 'Memory-Guided Action']","['Extracting previously successful actions to achieve similar goals (GITM)', 'Guiding agent actions based on recent, relevant, and important information (Generative Agent)', 'Matching and reusing reference plans (GITM)', 'Retrieving relevant information using embedding similarities (AgentSims)']",,29,
272,Adaptive Memory Management,"Agents need to continuously write new information (perceptions, actions, thoughts) into memory while efficiently managing memory storage, including handling duplicate information and preventing overflow when storage limits are reached.","An agent operating in a dynamic environment, constantly generating new observations and actions that need to be stored in its memory module, which may have finite capacity or accumulate redundant information.","Memory writing operations incorporate strategies to: 1. **Handle Duplicates:** Integrate similar new information with existing memories (e.g., condensing successful action sequences into a unified plan solution, aggregating duplicate information via count accumulation). 2. **Manage Overflow:** Implement mechanisms to delete existing information when memory reaches its limit (e.g., explicit deletion based on user commands, fixed-size buffers with FIFO overwriting).","Ensures that memory remains valuable and manageable, preventing performance degradation due to redundant or excessive data, and allowing continuous learning and adaptation.","['Hybrid Memory System', 'Contextual Memory Retrieval']","['Condensing successful action sequences (GITM)', 'Aggregating duplicate information (Augmented LLM)', 'Explicitly deleting memories based on user commands (ChatDB)', 'Overwriting oldest entries in a fixed-size buffer (RETLLM)']",,29,
273,Self-Reflective Learning (Memory Reflection),"Agents need to move beyond simply recalling past events to independently summarize, infer, and learn abstract, complex, and high-level insights from their accumulated experiences to improve future decision-making.","An agent that has accumulated a history of low-level memories (e.g., observations, actions, thoughts) and needs to develop a deeper understanding, generalize patterns, and derive principles from these experiences.","The agent is equipped with a reflection mechanism that emulates human self-evaluation. It can: 1. Generate key questions based on its recent memories. 2. Query its memory for relevant information. 3. Generate higher-level insights or abstract patterns from these memories (e.g., summarizing experiences, comparing successful/failed trajectories). This process can occur hierarchically.","Agents gain the capability to learn from past successes and failures, generalize experiences, and form abstract knowledge, leading to more sophisticated, consistent, and adaptive behaviors.","['Hybrid Memory System', 'Contextual Memory Retrieval', 'Iterative Refinement', 'Experience-Based Skill Acquisition']","['Summarizing past experiences into broader and more abstract insights (Generative Agent)', 'Abstracting common patterns from successful action sequences (GITM)', 'Comparing successful or failed trajectories within the same task (ExpeL)', 'Learning from a collection of successful trajectories (ExpeL)']",,23,
274,Chain-of-Thought Reasoning (Single-Path Planning),"LLMs, when used as planners, struggle to directly generate correct plans for complex tasks, especially those requiring multiple intermediate reasoning steps.","Designing an agent to solve complex problems where breaking down the task into a sequence of logical, intermediate steps can significantly improve the LLM's performance and the plan's accuracy.","The agent's planning module employs single-path reasoning strategies where the final task is decomposed into several intermediate steps, connected in a cascading manner. This is achieved by: 1. **Few-shot CoT:** Inputting reasoning steps as examples into the prompt to inspire the LLM. 2. **Zero-shot CoT:** Prompting the LLM with trigger sentences (e.g., 'think step by step') to generate reasoning processes. 3. **Iterative CoT:** Generating plans and obtaining observations independently, then combining them (ReWOO); or decomposing tasks into sub-goals and querying LLMs multiple times (HuggingGPT).","LLMs are guided to produce more coherent, step-by-step reasoning, leading to improved performance on complex tasks and more explainable plans.","['Prompt Engineering for Capability', 'Plan-Driven Execution']","['Solving complex problems by inputting reasoning steps into prompts (CoT)', 'Generating task reasoning processes without examples (ZeroshotCoT)', 'Checking prerequisites before generating a plan and regenerating if failed (RePrompting)', 'Decomposing tasks into sub-goals and solving them (HuggingGPT)']",,6,
275,Tree-of-Thought Planning (Multi-Path Planning),"Single-path reasoning might miss optimal solutions or fail when a specific path leads to a dead end, as complex problems often have multiple valid reasoning trajectories.","Designing an agent to tackle complex problems that benefit from exploring multiple alternative reasoning paths, evaluating options at each step, and selecting the most promising one.","The agent's planning module organizes reasoning steps into a tree-like (or graph-like) structure. At each intermediate step (a 'thought'), multiple subsequent steps may be generated and evaluated (e.g., by LLMs). Search strategies like Breadth-First Search (BFS) or Depth-First Search (DFS) are then used to navigate this thought tree and derive the final plan. This includes generating various reasoning paths and answers, selecting based on frequency (Self-consistent CoT), or building a world model to simulate potential benefits of different plans using Monte Carlo Tree Search (MCTS).","Enables more deliberate and robust problem-solving by exploring diverse reasoning trajectories, potentially leading to higher-quality or more resilient plans compared to single-path approaches.","['Prompt Engineering for Capability', 'Self-Correction Planning']","['Deducing final answers with multiple ways of thinking (Self-consistent CoT)', 'Generating plans using a tree-like reasoning structure (ToT)', 'Leveraging discarded historical information to derive new reasoning steps (RecMind)', 'Expanding tree-like reasoning to graph structures (GoT)', 'Enhancing reasoning processes by incorporating algorithmic examples (AoT)', 'Generating multiple possible next steps and determining the final one based on distances to admissible actions', 'Simulating potential benefits of different plans based on MCTS (RAP)']",,17,
276,External Planning Integration,"LLMs, despite their reasoning capabilities, may struggle with generating accurate, efficient, or optimal plans for highly domain-specific problems that require specialized algorithms or formal knowledge representation.","An agent needs to perform tasks in domains where precise, verifiable, or optimal plans are critical, and where existing, well-developed external planners can provide superior performance compared to LLM-generated plans alone.","The agent leverages an external, specialized planner. LLMs are used to: 1. **Translate:** Convert natural language task descriptions, observations, or objectives into a formal planning language (e.g., Planning Domain Definition Languages - PDDL) understood by the external planner. 2. **Execute:** Pass the formal representation to the external planner, which uses efficient search algorithms to determine the action sequence. 3. **Interpret:** Convert the planner's output back into natural language or executable actions for the LLM.","Improves the accuracy, efficiency, and optimality of plans for domain-specific problems, offloading complex symbolic reasoning to dedicated tools while retaining LLMs for high-level understanding and natural language interaction.",['Tool Augmentation'],"['Transforming task descriptions into PDDL for external planners (LLMP)', 'Converting observations, world state, and objectives into PDDL for an external planner (LLMDP)', 'Employing a heuristically designed external low-level planner to execute actions based on high-level plans (COLLM)']",,10,
277,Environment-Adaptive Planning (Feedback-Driven Planning - Environmental),"Initial plans generated by agents may be flawed, become non-executable due to unpredictable environment dynamics, or lack sufficient detail for long-horizon tasks, leading to failure in real-world scenarios.","Agents operating in dynamic, uncertain, or long-horizon environments where the initial plan needs to be continuously validated and refined based on real-time observations and outcomes from the environment.","The agent's planning module incorporates environmental feedback. After taking an action, the agent receives objective signals from the world (e.g., task completion status, scene graphs, execution errors, self-verification results, environment states, success/failure information for executed actions). This feedback is then used to: 1. **Validate:** Check if the current plan step is valid or if the overall plan is still viable. 2. **Refine:** Dynamically update or replan based on the observed outcomes (e.g., object mismatches, unattainable plans), making subsequent thoughts and actions more adaptive.","Plans become more robust and adaptive to real-world complexities, improving the agent's success rate in dynamic and long-horizon tasks by allowing iterative correction and adjustment.","['Self-Correction Planning', 'Iterative Refinement', 'Plan-Driven Execution']","['Constructing prompts using thought-act-observation triplets with search engine results as observation (ReAct)', 'Incorporating intermediate program execution progress, execution errors, and self-verification results (Voyager)', 'Leveraging environment states and success/failure information (Ghost)', 'Validating and refining strategic formulations using a scene graph simulator (SayPlan)', 'Informing agents about detailed reasons for task failure (DEPS)', 'Dynamically updating plans when encountering object mismatches and unattainable plans (LLMPlanner)', 'Providing task completion status, passive scene descriptions, and active scene descriptions as feedback (Inner Monologue)']",,1,
278,Human-in-the-Loop Planning (Feedback-Driven Planning - Human),"Agents may produce plans or actions that do not align with human values, preferences, or common sense, or they might suffer from hallucination, leading to undesirable or incorrect behaviors.","Designing agents for tasks where alignment with human intent, values, or subjective assessment is critical, or where human expertise is needed to correct agent errors or guide complex decision-making.","The agent's planning process actively incorporates subjective human feedback. After an action or plan segment, the agent is given the capability to actively solicit feedback from humans (e.g., regarding scene descriptions). This human feedback is then integrated into the agent's prompts to enable more informed planning and reasoning, ensuring alignment and reducing errors.","Agent plans and behaviors are better aligned with human values and preferences, and the agent can effectively address hallucination by incorporating external human correction.","['Prompt Engineering for Capability', 'Environment-Adaptive Planning', 'Iterative Refinement']","['Actively soliciting feedback from humans regarding scene descriptions (Inner Monologue)', 'Adjusting action strategies based on human feedback (Inner Monologue)']",,1,
279,Self-Correction Planning (Feedback-Driven Planning - Model),"Agents need to improve the quality, correctness, and efficiency of their internal reasoning processes and generated outputs without constant external human or environmental intervention.","An agent that needs to autonomously identify and rectify errors in its reasoning steps or outputs, leveraging its own or other pretrained models' capabilities for internal evaluation and refinement.","The agent implements an internal self-refine mechanism, often involving: 1. **Output Generation:** The agent produces an initial output or reasoning step. 2. **Feedback Generation:** The agent (or an auxiliary LLM) provides feedback on this output, identifying errors or suggesting improvements (e.g., detailed verbal feedback). 3. **Refinement:** The agent uses this feedback to refine its output or reasoning. This process iterates until desired conditions are met, such as examining and evaluating reasoning steps (SelfCheck) or using different language models as auxiliary roles (InterAct).","Agents can iteratively improve their plans and reasoning, correct errors, and enhance the overall quality of their outputs, leading to more reliable and robust autonomous behavior.","['Iterative Refinement', 'Tree-of-Thought Planning', 'Prompt Engineering for Capability']","['Self-refine mechanism with output-feedback-refinement components (Madaan et al., 2024)', 'Examining and evaluating reasoning steps generated at various stages (SelfCheck)', 'Using different language models as auxiliary roles (checkers, sorters) to avoid erroneous actions (InterAct)', 'Improving reasoning process quality via an evaluation module (ChatCoT)', 'Enhancing planning capability through detailed verbal feedback (Reflexion)']",,1,
280,Memory-Guided Action,"Agents need to generate actions that are consistent with their past behaviors and current context, especially when encountering familiar tasks or situations.","An agent that has a memory module containing a history of its actions, observations, and experiences, and needs to decide on its next action based on this accumulated knowledge.","The agent's actions are produced by first recollecting relevant information from its memory. Before taking an action, it retrieves recent, relevant, and important information from its memory stream. This extracted memory, along with the current task, is then used as a prompt to guide the LLM in generating the action.","Actions are more informed, consistent with past experiences, and efficient, especially when similar tasks have been encountered and successfully completed before.","['Hybrid Memory System', 'Contextual Memory Retrieval', 'Experience-Based Skill Acquisition']","['Guiding agent actions by retrieving recent, relevant, and important information from a memory stream (Generative Agents)', 'Invoking previously successful actions for similar low-level subgoals (GITM)', 'Influencing utterances by conversation history remembered in agent memories (ChatDev, MetaGPT)']",,29,
281,Plan-Driven Execution,"For complex, multi-step tasks, agents need a structured approach to ensure all sub-goals are addressed and the overall task is completed logically and efficiently.","An agent that has already generated a high-level plan or decomposed a complex task into a sequence of sub-goals, and now needs to translate these plans into concrete, sequential actions.","The agent's actions are generated by strictly following its pre-generated plans. If a task is decomposed into sub-goals, the agent takes actions to solve each sub-goal sequentially. The execution continues according to the plan unless signals indicate a plan failure (which might trigger replanning).","Enables the agent to systematically complete complex tasks by adhering to a predefined logical flow, ensuring all necessary steps are taken in order.","['Chain-of-Thought Reasoning', 'Tree-of-Thought Planning', 'External Planning Integration', 'Environment-Adaptive Planning']","['Strictly adhering to action plans for a given task (DEPS)', 'Taking actions to solve subgoals sequentially based on high-level plans (GITM)']",,6,
282,Tool Augmentation (External Tool Integration),"LLMs have inherent limitations, such as a lack of up-to-date or domain-specific expert knowledge, inability to perform precise computations, or susceptibility to hallucination, which restrict their ability to act effectively in real-world scenarios.","An LLM-based agent needs to perform tasks that require capabilities beyond the LLM's internal knowledge, such as accessing real-time information, performing calculations, interacting with external systems, or leveraging specialized models.","The agent is empowered with the capability to call and utilize external tools. These tools can include: 1. **APIs:** For general web services, specific platforms (e.g., HuggingFace models), programming interpreters (TPTU), or specialized API calls (Gorilla, APIBank, ToolBench, RestGPT, TaskMatrixAI). 2. **Databases/Knowledge Bases:** For querying specific domain information (ChatDB, MRKL, OpenAGI). 3. **External Models:** Specialized ML models (MemoryBank for text retrieval, ViperGPT for Python code generation, ChemCrow for chemical tasks, MMREACT for multimodal scenarios).","Overcomes LLM limitations, expands the agent's action space and domain expertise, reduces hallucination, and enables interaction with the real world or specialized computational resources.",['External Planning Integration'],"['Leveraging models on HuggingFace to accomplish complex user tasks (HuggingGPT)', 'Automatically generating queries to extract relevant content from external Web pages', 'Interfacing with Python interpreters and LaTeX compilers (TPTU)', 'Generating accurate input arguments for API calls (Gorilla)', 'Automatically converting tools (ToolFormer)', 'API recommendation (APIBank)', 'Tool generation (ToolBench)', 'Connecting LLMs with RESTful APIs (RestGPT)', 'Connecting LLMs with millions of APIs (TaskMatrixAI)', 'Querying databases with SQL statements (ChatDB)', 'Incorporating expert systems such as knowledge bases and planners (MRKL, OpenAGI)', 'Enhancing text retrieval capability with language models (MemoryBank)', 'Generating Python code from text descriptions (ViperGPT)', 'Performing tasks in organic synthesis, drug discovery, and material design (ChemCrow)', 'Integrating various external multimodal models (MMREACT)']",,44,
283,Task-Specific Finetuning,"General-purpose LLMs may lack the necessary task-specific capabilities, skills, or experiences to perform particular tasks effectively, even with clever prompting.",Developing an LLM-based agent for a specific domain or task where high performance requires specialized knowledge and behavioral alignment that can only be achieved by modifying the LLM's parameters.,"The LLM that forms the core of the agent is finetuned on task-dependent datasets. These datasets can be: 1. **Human Annotated:** Collected from human workers completing specific annotation tasks (e.g., aligning with human values and preferences, converting natural language to structured memory). 2. **LLM Generated:** Created by using LLMs themselves to generate a large volume of annotation data (e.g., for tool-using capability, social interaction data). 3. **Real-world:** Directly collected from real-world applications and user interactions (e.g., web domain data, text-to-SQL pairs).","The agent acquires enhanced task-specific capabilities, skills, and experiences, leading to significant performance improvement for the target domain or task. This method is suitable for open-source LLMs.","['Role Profiling', 'Prompt Engineering for Capability']","['Aligning LLMs with human values and preferences (CoH)', 'Converting natural languages into structured memory information (RETLLM)', 'Enhancing agent capabilities in Web shopping (WebShop)', 'Enhancing educational functions (EduChat)', 'Solving complex interactive reasoning tasks (SWIFTSAGE)', 'Enhancing tool-using capability of open-source LLMs (ToolBench)', 'Empowering agents with social capability by training on social interaction data', 'Enhancing agent capability in the Web domain (MIND2WEB)', 'Improving performance on text-toSQL tasks (SQLPALM)']",,5,
284,Prompt Engineering for Capability,How to enhance an agent's capabilities or unleash existing LLM capabilities for specific tasks without requiring costly or complex finetuning of the underlying LLM.,"Utilizing LLMs, particularly closed-source models, where direct parameter modification (finetuning) is not feasible or desired, but the agent needs to perform complex reasoning, exhibit self-awareness, or adapt its behavior.","Valuable information is written into the prompts used to interact with the LLM. This includes: 1. **Few-shot Examples:** Providing intermediate reasoning steps or successful examples to guide the LLM's problem-solving (CoT, CoTSC, ToT). 2. **Trigger Sentences:** Using specific phrases (e.g., 'think step by step') to elicit desired reasoning processes. 3. **Internal State Description/Reflection:** Incorporating the agent's beliefs about mental states of others and itself, or reflections on past failures, into the prompt to guide future actions and conversations.","The agent's language comprehension and generation capabilities are leveraged more effectively, leading to improved task reasoning, self-awareness, and adaptive responses within the constraints of the LLM's architecture. This method is suitable for both open and closed-source LLMs.","['Role Profiling', 'Chain-of-Thought Reasoning', 'Tree-of-Thought Planning', 'Human-in-the-Loop Planning', 'Self-Correction Planning']","['Empowering agents with complex task reasoning capability (CoT, CoTSC, ToT)', 'Enhancing agent self-awareness and strategic planning in conversation (SocialAGI)', 'Guiding future actions based on reflections on past failures (Retroformer)', 'Creating LLM agents that adapt to specific tasks based on digital twin information (GPT4IA)']",,23,
285,Iterative Refinement (Trial-and-Error Learning),"Initial agent actions or plans may be unsatisfactory or incorrect, requiring a mechanism for continuous improvement and adaptation based on evaluation.","Agents operating in environments where actions can be judged, and feedback (from critics, environment, or humans) can be obtained to inform subsequent iterations of planning and action.","The agent employs a trial-and-error mechanism: 1. **Action/Plan Generation:** The agent performs an action or proposes a subtask plan. 2. **Critic Evaluation:** A predefined critic (or an LLM serving as a critic) evaluates the action/plan, generating feedback (e.g., failure information, specific details explaining failure causes, validation checks). 3. **Reaction/Revision:** If the action/plan is deemed unsatisfactory, the agent incorporates this feedback (e.g., by redesigning the plan, appending to its prompt) to revise its next attempt. This process iterates until success or desired conditions are met.","Agents can learn from their mistakes, improve their plans and actions over time, and adapt to complex tasks by continuously refining their strategies based on evaluative feedback.","['Self-Correction Planning', 'Environment-Adaptive Planning', 'Human-in-the-Loop Planning', 'Autonomous Goal-Driven Learning']","['Simulating human behavior in recommender systems and generating responses (RAH)', 'Redesigning plans based on detailed failure explanations (DEPS)', 'Validating subtask plans and 3D waypoints for multi-robot collaboration (RoCo)', 'Iteratively refining actions based on feedback on performance failures (PREFER)']",,1,
286,Multi-Agent Consensus (Crowdsourced Learning),"A single agent or LLM might struggle to provide a comprehensive, accurate, or socially aligned response to complex questions, or may exhibit biases.","Multi-agent systems where diverse perspectives, knowledge, or problem-solving approaches can be leveraged to achieve a more robust and collectively intelligent solution.","A debating mechanism is designed where different agents provide separate responses to a given question. If their responses are not consistent, they are prompted to incorporate the solutions from other agents and provide an updated response. This iterative process continues until reaching a final consensus answer.","The collective intelligence of the agents is leveraged to enhance the capability of each individual agent, leading to more robust, accurate, and often socially aligned solutions through collaborative reasoning and debate.","['Autonomous Goal-Driven Learning', 'Iterative Refinement']","['Leveraging the wisdom of crowds to enhance agent capabilities (Du et al., 2023)']",,23,
287,Experience-Based Skill Acquisition,"Agents need to efficiently learn and accumulate knowledge from successful past task completions, and then effectively utilize this knowledge to solve similar future tasks, rather than re-solving them from scratch.","An agent operating in an environment where it can perform exploratory actions, and where successful task trajectories or skill executions can be identified and generalized for future reuse.","The agent stores successful actions, task completion trajectories, or executable skill codes in a dedicated memory (e.g., a skill library). When encountering a similar task in the future, the agent attempts to retrieve relevant memories or skills and directly invokes them or refines their execution code based on environmental feedback and self-verification results.","Improves agent efficiency and task completion rates by leveraging accumulated successful experiences, reducing redundant computation, and allowing for continuous refinement of learned skills. This contributes to a knowledge base for intricate tasks.","['Hybrid Memory System', 'Contextual Memory Retrieval', 'Memory-Guided Action', 'Self-Reflective Learning', 'Autonomous Goal-Driven Learning']","['Storing successful actions for tasks into agent memory for future reuse (GITM)', 'Equipping agents with a skill library of executable codes, iteratively refined (Voyager)', 'Constructing a knowledge base through autonomous exploration and human demonstrations for app interaction (AppAgent)', 'Storing user feedback on problem-solving intentions in memory for future retrieval (MemPrompt)']",,29,
288,Autonomous Goal-Driven Learning (Self-Driven Evolution),"Agents need to develop capabilities and acquire knowledge autonomously, adapting to new situations and preferences without explicit, step-by-step external guidance or predefined learning objectives.","Designing agents that can operate in open-ended environments, requiring long-term learning, adaptation, and the ability to set and pursue their own developmental goals.","The agent is designed with mechanisms that enable it to autonomously set goals for itself, explore the environment, and gradually improve its capabilities based on internal feedback (e.g., a reward function) or interactions within a multi-agent system. This can involve integrating advanced LLMs into multi-agent systems for adaptation, teacher-student models for skill improvement via explanations, or dynamic adjustment of agent roles, tasks, and relationships based on task requirements and feedback.","Agents can acquire knowledge and develop capabilities according to their own preferences, adapt to complex tasks, and exhibit emergent behaviors, leading to a more generalized and self-sufficient form of intelligence.","['Multi-Agent Consensus', 'Experience-Based Skill Acquisition', 'Iterative Refinement']","['Autonomously setting goals and improving capabilities via environment exploration and reward feedback (LMA3)', 'Agents adapting and performing complex tasks in multi-agent systems (SALLMMS)', 'Teacher-student models for improving reasoning skills via natural language explanations and personalization (CLMTWA)', 'Dynamic adjustment of agent roles and tasks through natural language communication and collaboration (NLSOM)']",,11,
289,Solution Pattern for Machine Learning,"Organizations struggle to derive business value from ML due to inherent complexities in ML solution development. These complexities include: understanding ML capabilities and limitations, specifying well-defined business cases and translating them into ML problems, data preparation and feature selection, ML algorithm selection and trade-offs, and finding linkages between ML models and business processes. This is compounded by a lack of specialized ML skillset and talent, and insufficient ML knowledge among executives and stakeholders.","Organizations designing, developing, or deploying ML/ML systems, particularly those facing challenges in bridging the gap between business problems and ML solutions, managing ML development complexities, and needing to enhance design effectiveness, speed up development, and facilitate knowledge transfer. Applicable in ""real-world scenarios"" and ""business process management"" contexts where there's a need to represent generic and well-proven ML designs for recurring business analytics problems.","Introduces ""solution patterns"" as an explicit way of representing generic and well-proven ML designs for commonly-known and recurring business analytics problems. A solution pattern is an artifact in the form of a conceptual model, comprised of several parts: 
1.  **Business View:** Actors, Decision Goals, Question Goals, and Insights, which translate a business problem into a set of well-defined ML problems.
2.  **Analytics View:** Analytics Goals (e.g., Prediction, Classification), Algorithms (e.g., kNearest Neighbor, Naive Bayes, Support Vector Machines), Indicators (ML metrics like Accuracy, Precision), and Softgoals (non-functional requirements like Interpretability, Speed, Tolerance to missing values).
3.  **Data Preparation View:** Data Preparation Tasks (e.g., Dimensionality Reduction, normalization), Operators, Data Flows, and Entities (datasets).
4.  **Contexts:** User Contexts (user preferences, e.g., desire for algorithm simplicity), Data Contexts (data characteristics, e.g., feature independence, dataset noise), and Model Contexts (algorithm configurations, e.g., parameter tuning). These define applicability conditions.
5.  **Parameters:** Configuration values required as input by an Algorithm.
The solution is supported by a prototype architecture with components like Pattern Repository, Data Extractor, Context Analyzer (User, Data, Model Context Monitors), Quality Evaluator, Workflow Planner, Data Preparator, and Data Miner, providing semi-automated support for using these patterns.","1.  Positive impact on ML design and development efforts. 
2.  Organizes, stores, and presents knowledge on various aspects of ML solution design, providing reusable answers to critical ML development questions. 
3.  Enables efficient transfer of ML design knowledge among developers, helping new teams quickly acquire ML expertise. 
4.  Potentially reduces time and cost of ML development efforts by providing proven starting points and simplifying the coding phase. 
5.  Constrains the solution space into a narrower scope based on critical quality requirements and verified contexts. 
6.  Offers convenient and easy-to-understand visual representations of ML design. 
7.  Leads to metamodel enhancements, such as the classification of Context elements into Data, User, and Model Context subtypes.","['**Best practices catalogues and patterns for analytics and ML solution development (e.g., Chen et al., Sculley et al., Zinkevich, Breck et al.):** Solution patterns differ by offering a more explicit and systematic way to represent business requirements and link them to relevant ML algorithms, while capturing user, data, and model contexts.', '**Machine learning services on cloud platforms (e.g., Microsoft Azure ML Studio, Google Cloud AI, Amazon SageMaker):** Solution patterns are distinct in that business questions and decisions play a critical role in deriving solution design, integrating quality requirements, user preferences, data characteristics, and parameter configurations into the design process.', '**Data mining formal ontologies (e.g., Keet et al., Vanschoren):** Unlike these ontologies, solution patterns start from business decisions and questions, linking them to alternative ML algorithms and data preparation techniques while explicitly considering quality requirements.', '**Software design patterns (e.g., Buschmann et al.):** Solution patterns are more problem-domain specific and less generic than classical software design patterns. They are conceptually closer to a working solution, intended for adopters to use as a starting point for implementation.']","['MLOps Patterns', 'Knowledge & Reasoning Patterns', 'AI–Human Interaction Patterns', 'Tools Integration Patterns']",,28,
290,Browser-Assisted Question Answering (WebGPT),"Long-form question-answering (LFQA) systems lag human performance, particularly in information retrieval and synthesis, and struggle to provide up-to-date, factual answers with verifiable sources.","Large Language Models (LLMs) like GPT-3 excel at text generation but lack real-time access to external, up-to-date knowledge. Human evaluators find it difficult to assess factual accuracy without explicit references.","Create a text-based web-browsing environment that a fine-tuned LLM can interact with. The model receives a summary of the current state (question, page text) and issues commands (Search, Click, Find, Quote, Scroll, etc.). It actively searches the web, navigates pages, and collects 'references' (quoted passages) to support its final answer. Training is done end-to-end using imitation learning and reinforcement learning.","Improves both retrieval and synthesis by enabling the model to dynamically gather information. Generates answers with explicit references, which is crucial for easier and more accurate human evaluation of factual accuracy. Achieves performance competitive with or exceeding human demonstrators on LFQA tasks.","['Retrieval Augmented Generation (RAG)', 'Agentic Web Browsing', 'Human Feedback for Answer Quality', 'Reference Collection for Factual Verification']","Long-form question answering, information gathering, factual synthesis, interactive AI agents.",,3,
291,Behavior Cloning for Agentic Control,"A pre-trained language model needs to learn to interact with a novel, structured environment (like a text-based web browser) by issuing specific commands, a capability not inherent in its pre-training.",Human experts can successfully perform the desired task within the environment. Their sequence of actions and observations can serve as a strong supervised signal. An underlying LLM possesses general language understanding and generation capabilities.,"Collect demonstrations of humans using the environment to perform the task (e.g., answering questions via browsing). Fine-tune the pre-trained LLM using supervised learning, where the observed human commands are treated as labels for the model's actions in response to environmental states.","The model learns a policy to navigate and interact within the environment, mimicking human behavior. This provides a strong initial policy that can then be further refined or optimized.","['Imitation Learning', 'Reinforcement Learning from Human Feedback (as a starting point)']","Initial training for AI agents in structured interactive environments, learning complex multi-step tasks from expert demonstrations, bootstrapping policies for RL.",,48,
292,Reward Modeling from Human Comparisons,"Directly optimizing for subjective and complex qualities (e.g., factual accuracy, coherence, overall usefulness of an answer) with a simple loss function is difficult. Human evaluation is the gold standard but is slow and expensive to apply to every generated output during training.","A generative model can produce multiple candidate outputs for a given input. Human labelers can provide pairwise preferences (A is better than B, B is better than A, or equal) for these outputs.","Train a separate 'reward model' (RM) to predict a scalar score (e.g., an Elo score or preference logit) for a given input-output pair (e.g., question and answer with references). The RM is trained on human comparison data, typically using a cross-entropy loss to predict the likelihood of human preference.","The reward model serves as a scalable, automated proxy for human judgment, allowing for quantitative evaluation of model outputs and providing a learnable reward signal for subsequent optimization methods like Reinforcement Learning or Rejection Sampling.","['Human Feedback', 'Reinforcement Learning from Human Feedback', 'Rejection Sampling (Best-of-N)']","Evaluating model performance, providing a reward signal for RL, enabling inference-time selection of best outputs.",,12,
293,Reinforcement Learning from Human Feedback (RLHF) for Agentic Behavior,"Behavior Cloning (BC) can only imitate human performance and may not fully optimize for the desired objective (e.g., answer quality) or surpass human capabilities. The agent needs to explore and learn beyond expert demonstrations to achieve optimal performance.","An initial policy (e.g., from Behavior Cloning) is available. A Reward Model (RM), trained on human preferences, can provide a scalar reward signal for agent trajectories or final outputs. The agent operates in an interactive environment.","Fine-tune the BC-initialized policy using Reinforcement Learning algorithms, such as Proximal Policy Optimization (PPO). The reward signal for the RL agent is derived from the Reward Model's score at the end of an episode, often combined with a KL-divergence penalty from the initial BC policy to prevent catastrophic forgetting and over-optimization of the RM.","The agent learns to explore and exploit the environment more effectively, optimizing directly for human-aligned quality objectives as captured by the RM. Can potentially achieve performance beyond expert human demonstrators.","['Behavior Cloning', 'Reward Modeling', 'Agentic AI Patterns']","Optimizing policies for complex, multi-step tasks in interactive environments, aligning AI agent behavior with human preferences, improving generative model outputs.",,48,
294,Rejection Sampling (Best-of-N) with Reward Model,"Improving the quality of generated outputs at inference time without incurring the computational cost and complexity of further policy training (e.g., full Reinforcement Learning). The base generative policy might produce a range of qualities, and a mechanism is needed to select the best one.","A base generative policy (e.g., a fine-tuned LLM) can produce multiple diverse candidate outputs for a given input. A Reward Model (RM) is available that can accurately score the quality of these candidates according to human preferences.","For each query, sample a fixed number (N) of candidate outputs from the base generative policy. Evaluate each of these N candidates using the pre-trained Reward Model. Select the candidate that receives the highest score from the Reward Model as the final output. This trades inference-time compute for improved quality.","Significantly improves the quality of the final output by leveraging the RM to select the best among multiple attempts. It's a computationally efficient alternative or complement to RL for optimizing against a reward model, often outperforming RL for certain compute budgets.","['Reward Modeling', 'Generative AI Patterns']","Enhancing the quality of LLM responses, optimizing outputs without further policy fine-tuning, leveraging reward models for inference-time selection.",,12,
295,Reference Collection for Factual Verification,"Human evaluators face difficulty and subjectivity when assessing the factual accuracy of long-form AI-generated answers, especially without knowing the sources or having to perform independent research. AI models can also hallucinate or make unverified claims.","An AI agent is designed to gather information from external sources (e.g., the web) to compose answers. Transparency and verifiability are critical for building trust and enabling effective human oversight.","Design the AI agent's environment and workflow such that it explicitly identifies and 'quotes' relevant passages from the external sources (e.g., web pages) it browses while forming its answer. These quoted passages, along with their source (title, domain), are then presented alongside the final generated answer.","Enables more accurate, less noisy, and consistent human evaluation of factual accuracy by providing direct evidence. Increases transparency, allowing end-users to easily follow up on sources and verify claims themselves. Reduces non-imitative falsehoods (hallucinations) by grounding the answer in retrieved text.","['Browser-Assisted Question Answering', 'AI-Human Interaction Patterns', 'Knowledge & Reasoning Patterns']","Improving factual accuracy and verifiability of AI-generated content, facilitating human oversight and trust, reducing the burden on human labelers.",,3,
296,Retrieval Augmented Generation (RAG),"Large Language Models (LLMs) often suffer from factual inaccuracies, knowledge cutoff issues, and the tendency to 'hallucinate' information, as their knowledge is limited to their training data.","The task requires access to up-to-date, specific, or domain-specific factual information that is not reliably encoded within the LLM's parameters. External knowledge bases or search engines are available.","Combine a retrieval mechanism with a generative LLM. First, a retriever (e.g., dense passage retriever, search engine API) queries an external knowledge source based on the input prompt. Then, the retrieved relevant documents or passages are provided as context to the LLM, which generates a response conditioned on both the original prompt and the retrieved information.","Significantly improves factual accuracy, reduces hallucinations, and allows the LLM to provide answers grounded in up-to-date or specific external knowledge. Enhances the trustworthiness and applicability of LLMs for knowledge-intensive tasks.","[""Browser-Assisted Question Answering (WebGPT's approach is an agentic form of RAG)"", 'Knowledge & Reasoning Patterns', 'Tools Integration Patterns']","Open-domain question answering, factual summarization, knowledge-intensive dialogue systems, reducing LLM 'hallucinations.'",,3,
297,Implicit Reliable Source Preference,"AI models, especially LLMs, can generate 'imitative falsehoods' by reproducing common misconceptions or unreliable information found in their vast training data, or by quoting from untrustworthy sources if given web access.","The AI system has access to diverse information sources, including potentially unreliable ones (e.g., raw web search results). The goal is to produce truthful and reliable answers.","Design the training and evaluation pipeline to implicitly incentivize the model to prioritize and cite reliable sources. This can involve: 1. Filtering underlying search API results (e.g., Bing API filtering in WebGPT). 2. Explicitly instructing human labelers to rate the trustworthiness of references and prioritize answers supported by reliable sources during reward modeling data collection. 3. Designing the reward model to implicitly or explicitly penalize reliance on untrustworthy sources.",Reduces the generation of imitative falsehoods and improves the overall truthfulness and trustworthiness of the AI's answers by guiding it towards more credible information sources.,"['Reference Collection for Factual Verification', 'Human Feedback', 'Prompt Design Patterns (through instructions to labelers)']","Improving factual accuracy and trustworthiness, combating misinformation, aligning AI with epistemic goals.",,3,
298,ReAct (Reasoning and Acting),"Large Language Models (LLMs) typically study reasoning (e.g., Chain-of-Thought) and acting (e.g., action plan generation) as separate topics. Reasoning alone often lacks grounding in the external world, leading to hallucination and error propagation. Acting alone lacks abstract reasoning, high-level planning, or working memory to guide complex interactions.","Tasks requiring both complex multi-step reasoning (e.g., question answering, fact verification) and interaction with external environments or tools (e.g., Wikipedia API, text-based games, web navigation). The goal is to perform dynamic reasoning, create/maintain/adjust high-level plans, and effectively gather/incorporate information from external sources. Few-shot learning setup.","Augment the LLM's action space to include both domain-specific actions (to interact with the external environment) and free-form language thoughts (verbal reasoning traces). These thoughts and actions are generated in an interleaved manner. Thoughts serve to decompose task goals, create action plans, inject commonsense knowledge, extract important information from observations, track progress, handle exceptions, and adjust plans ('reason to act'). Actions allow the model to interface with and gather additional information from external sources ('act to reason').","Superior performance across diverse language reasoning and decision-making tasks, outperforming state-of-the-art baselines. Leads to improved human interpretability, trustworthiness, and diagnosability of the agent's decision-making process. Overcomes prevalent issues of hallucination and error propagation found in reasoning-only methods, and provides more informed acting than action-only methods. Shows strong generalization with few-shot prompting.","['Chain-of-Thought (CoT) Prompting', 'Acting-only Prompting', 'Inner Monologue (IM)', 'SayCan (Grounded Robotic Action Planning)', 'Self-Consistency (CoTSC)', 'Human-in-the-loop Behavior Correction']","['Multi-hop Question Answering (HotpotQA)', 'Fact Verification (FEVER)', 'Text-based Games (ALFWorld)', 'Web Navigation (WebShop)']",,26,
299,Chain-of-Thought (CoT) Prompting,"Large Language Models (LLMs) often struggle with complex reasoning tasks that require multiple sequential steps of thought, especially those involving arithmetic, commonsense, or symbolic reasoning.","Tasks where a direct answer is insufficient, and an explicit step-by-step reasoning process can help the LLM break down the problem and arrive at the correct solution. Typically applied in a few-shot learning setting where example reasoning traces are provided.","Prompt the LLM to generate a series of intermediate reasoning steps (a 'chain of thought') before producing the final answer. This makes the LLM's internal thought process explicit, allowing it to perform more complex multi-step reasoning.","Elicits emergent reasoning capabilities in LLMs, significantly improving performance on complex tasks compared to direct prompting. However, CoT reasoning can be a 'static black box' and prone to fact hallucination or error propagation as it relies solely on internal representations.","['ReAct (improves upon CoT by adding external interaction)', 'Self-Consistency (CoTSC)', 'Least-to-Most Prompting', 'Zero-shot CoT', 'Scratchpads', 'Selection-Inference']","['Arithmetic Reasoning', 'Commonsense Reasoning', 'Symbolic Reasoning', 'Question Answering']",,26,
300,Self-Consistency (CoTSC),"Chain-of-Thought (CoT) reasoning, while effective, can sometimes produce inconsistent or incorrect reasoning paths and final answers due to the probabilistic nature of LLM generation, especially for complex problems.","Tasks where a more robust and reliable answer is needed, and there might be multiple valid reasoning paths to a solution. Applied after generating CoT traces.","Instead of relying on a single Chain-of-Thought trace, generate multiple diverse reasoning paths (CoT trajectories) by sampling from the LLM (e.g., with a non-zero decoding temperature). Then, aggregate the final answers derived from these multiple paths (e.g., by taking the majority vote) to determine the most consistent and likely correct answer.","Consistently boosts performance over single Chain-of-Thought prompting, leading to more accurate and reliable answers by leveraging the diversity of reasoning paths. Can be combined with other methods like ReAct for enhanced performance.","['Chain-of-Thought (CoT) Prompting', 'Hybrid Reasoning (ReAct + CoTSC / CoTSC + ReAct)']","['Question Answering', 'Fact Verification', 'General Reasoning Tasks']",,26,
301,Acting-only Prompting,"Language models need to interact with external environments or tools to perform tasks or gather information, but without explicit reasoning, they may struggle with abstract goals, maintaining working memory, or handling exceptions, leading to inefficient or hallucinated actions.",Interactive environments where the primary goal is to perform a sequence of domain-specific actions. Multimodal observations are often converted into text for the language model. Used as a baseline to evaluate the impact of adding explicit reasoning.,"Prompt the LLM to generate domain-specific actions based on the current context (observations and previous actions), without explicitly generating verbal reasoning traces (thoughts). The model's focus is solely on predicting the next action to take in the environment.","Enables LLMs to interact with environments and generate action sequences, similar to imitation or reinforcement learning. However, it often lacks the ability to reason abstractly about high-level goals, track subgoals, or recover from errors, leading to lower success rates on complex tasks compared to methods that incorporate reasoning.","['ReAct (adds reasoning to acting)', 'WebGPT']","['Interactive Decision-Making Tasks (e.g., text-based games, web navigation)', 'Planning in Interactive Environments']",,26,
302,Hybrid Reasoning (ReAct + CoTSC / CoTSC + ReAct),"ReAct excels at factual grounding and external interaction but might sometimes be less flexible in formulating complex internal reasoning structures. Conversely, Chain-of-Thought (and Self-Consistency, CoTSC) is strong in internal reasoning structure but prone to hallucinating facts or being ungrounded.","Knowledge-intensive reasoning tasks where both accurate factual retrieval (requiring external interaction) and robust, complex logical reasoning (requiring internal thought processes) are crucial for optimal performance.","Combine ReAct and CoTSC in a synergistic manner, leveraging their complementary strengths. Heuristics are employed to decide when to switch between the two methods: (A) If ReAct fails to return an answer within a given number of steps, back off to CoTSC. (B) If the majority answer among 'n' CoTSC samples occurs less than 'n/2' times (indicating low confidence in internal knowledge), back off to ReAct to retrieve external knowledge.","Achieves the best overall performance by effectively integrating the benefits of both externally grounded, interactive reasoning (ReAct) and internally consistent, structured reasoning (CoTSC). This leads to more factual, robust, and accurate problem-solving across knowledge-intensive tasks.","['ReAct (Reasoning and Acting)', 'Self-Consistency (CoTSC)', 'Chain-of-Thought (CoT) Prompting']","['Knowledge-intensive Question Answering (HotpotQA)', 'Fact Verification (FEVER)']",,26,
303,Human-in-the-loop Behavior Correction,"AI agents, particularly those based on LLMs, can exhibit unexpected, erroneous, or undesirable behaviors (e.g., hallucination, repetitive loops, incorrect reasoning) that are difficult to diagnose and correct in a black-box system. Traditional methods often require retraining or complex policy adjustments.","Interactive AI systems where transparency, controllability, and the ability to course-correct the agent's behavior in real-time are important, especially during development, debugging, or critical operational phases.","Design the AI system (e.g., a ReAct agent) to generate explicit, interpretable reasoning traces (thoughts) alongside its actions. A human operator can then inspect these traces to understand the agent's decision-making process. By directly editing or injecting new thoughts into the agent's context, the human can guide or drastically change the agent's internal reasoning and subsequent actions to align with desired behavior.","Enables humans to easily inspect reasoning and factual correctness, diagnose errors, and control or correct agent behavior on the go. This significantly simplifies problem-solving and alignment, shifting the effort from typing many actions to editing a few thoughts, fostering new forms of human-machine collaboration.","['ReAct (provides the interpretable reasoning traces)', ""Inner Monologue (IM) (ReAct provides more flexible thought editing than IM's fixed feedback)""]","['Debugging and steering LLM-based agents in interactive environments (e.g., ALFWorld)', 'Human-AI collaboration for complex tasks']",,1,
304,Least-to-Most Prompting,"Large Language Models (LLMs) often struggle with highly complex problems that require a long chain of reasoning, especially when subsequent steps depend on the correct solution of preceding steps.","Complex reasoning tasks that can be naturally decomposed into a sequence of simpler, interdependent subproblems. The goal is to improve the LLM's ability to tackle these problems systematically.","Instead of attempting to solve the entire complex problem at once, the LLM is first prompted to decompose the problem into a series of simpler, more manageable subproblems ('least'). Then, the LLM is prompted to solve these subproblems sequentially, using the solutions (or intermediate results) of the earlier subproblems as input or context for solving later ones ('most').","Enables LLMs to tackle and solve significantly more complex reasoning tasks by mimicking a structured, step-by-step problem-solving approach. This method enhances the LLM's ability to build upon prior reasoning steps.",['Chain-of-Thought (CoT) Prompting (Least-to-Most can be seen as a structured approach to CoT)'],"['Solving complicated multi-step reasoning tasks', 'Mathematical problem-solving', 'Logic puzzles']",,26,
305,Zero-shot CoT,"While Chain-of-Thought (CoT) prompting is effective for eliciting reasoning, it typically requires providing few-shot examples of reasoning traces, which can be costly to create or unavailable for new, unseen tasks.","Tasks where CoT reasoning would be beneficial, but no in-context examples demonstrating the reasoning process are provided to the LLM.","Simply append a phrase like 'Let's think step by step' to the prompt, without any further examples of reasoning traces. This simple instruction acts as a meta-prompt that implicitly triggers the LLM's latent Chain-of-Thought capabilities.","Elicits emergent reasoning capabilities in LLMs in a zero-shot setting, demonstrating that CoT can be activated without explicit demonstrations. This makes the reasoning pattern more broadly applicable to novel tasks and reduces the need for extensive prompt engineering or data annotation.",['Chain-of-Thought (CoT) Prompting'],"['General reasoning tasks in a zero-shot setting', 'Quick prototyping for reasoning tasks']",,26,
306,Selection-Inference,"Large Language Models (LLMs) can struggle with complex logical reasoning, especially when faced with large amounts of information. They might get distracted by irrelevant details or fail to identify the core premises needed for a sound conclusion.","Tasks requiring interpretable logical reasoning over potentially extensive textual information, where the process of identifying relevant facts is as important as the inference itself.","Decompose the reasoning process into two distinct, sequential steps performed by the LLM: 1. A 'selection' step where the LLM identifies, extracts, or synthesizes only the most relevant information or premises from the given input. 2. An 'inference' step where the LLM uses only the selected information to derive a logical conclusion or answer.",Improves the interpretability of the reasoning process by clearly separating the relevant information extraction from the inferential step. Potentially enhances accuracy for logical reasoning tasks by focusing the LLM on pertinent facts.,"['Chain-of-Thought (CoT) Prompting (as a more structured approach to multi-step reasoning)', 'Faithful Reasoning (Multi-LM)']","['Interpretable logical reasoning', 'Complex reasoning tasks with large context windows']",,16,
307,STaR (Self-Taught Reasoner),"Manually annotating high-quality reasoning rationales or intermediate steps for finetuning Language Models (LMs) is an expensive and time-consuming process, limiting the scalability of reasoning-enhanced models.","Improving the reasoning capabilities of smaller LMs through finetuning, particularly when extensive human-labeled rationales are not available or feasible to create.","A bootstrapping approach where an LLM first generates its own reasoning rationales (thoughts) to solve problems. These self-generated rationales are then filtered for correctness (e.g., by checking if the final answer derived from the rationale is correct). The model is then finetuned on this dataset of self-generated, correct rationales, iteratively improving its reasoning abilities without direct human annotation of the rationales themselves.","Enables the training of more capable reasoning models with less reliance on human annotation. Leverages the model's own ability to generate and refine rationales, making the training process more scalable and efficient.","['Chain-of-Thought (CoT) Prompting (STaR leverages CoT-like rationales for finetuning)', 'Scratchpads']","['Training LLMs for improved reasoning', 'Reducing annotation costs for reasoning datasets']",,16,
308,Faithful Reasoning (Multi-LM),"Ensuring the 'faithfulness' and accuracy of each step in a multi-step reasoning process can be challenging for a single large language model, which might generate plausible but incorrect intermediate steps or conclusions.","Multi-step reasoning tasks where accuracy, verifiability, and robustness of each reasoning step are critical, and the complexity might overwhelm a single model.","Decompose the multi-step reasoning process into several distinct, specialized steps. Each of these steps is then performed by a dedicated large language model (or a specialized module). This modular approach allows for better control, potential verification, and specialized processing at each stage of the reasoning chain.","Aims to produce more faithful, accurate, and verifiable multi-step reasoning by distributing the cognitive load across specialized components. This can enhance the reliability of complex reasoning chains.","['Selection-Inference', 'Chain-of-Thought (CoT) Prompting (as a more modular and controlled approach to multi-step reasoning)']","['Multi-step reasoning requiring high faithfulness and precision', 'Complex problem-solving requiring verification at each stage']",,26,
309,Scratchpads,"Large Language Models (LLMs) often struggle with multi-step computation problems or tasks that require maintaining and manipulating intermediate states, as their typical output is a single, final answer.","Tasks requiring complex calculations, symbolic manipulation, or multi-step problem-solving where explicitly showing intermediate steps is beneficial for both the model's performance and human interpretability.","Finetune an LLM on trajectories that explicitly show intermediate computation steps, analogous to a human using a 'scratchpad' to work through a problem. The training data includes not just the final answer, but also the step-by-step working that leads to it.",Significantly improves the LLM's ability to perform multi-step computation and complex problem-solving by making the intermediate thought process explicit and trainable. This enhances the model's internal 'working memory' and reasoning transparency.,"['Chain-of-Thought (CoT) Prompting (Scratchpads can be seen as a finetuning approach for CoT-like behaviors)', 'STaR (Self-Taught Reasoner)']","['Multi-step computation problems', 'Symbolic reasoning tasks', 'Code generation with intermediate steps']",,26,
310,WebGPT (Browser-assisted Question Answering),"Language models have limited internal knowledge, which can be outdated or insufficient for answering open-domain questions requiring real-time, accurate, or specific information from the internet.",Open-domain question answering (QA) tasks that necessitate searching for and synthesizing information from the web to provide comprehensive and up-to-date answers.,"Train a language model to interact with web browsers (e.g., search queries, navigate web pages, click links, scroll) to find information relevant to a given question. The model learns to browse and extract answers effectively, often through human feedback and reinforcement learning.","Enables LLMs to answer complicated questions by augmenting their internal knowledge with real-time external information from the internet, leading to more accurate, current, and grounded answers. However, it can rely on expensive human feedback for policy learning.","['Acting-only Prompting (WebGPT uses action generation for browsing)', 'API Call Integration (browser as a specialized API)', 'ReAct (WebGPT does not explicitly model thinking/reasoning procedure like ReAct does)']","['Open-domain question answering', 'Information retrieval from the web', 'Fact-checking']",,3,
311,API Call Integration (Chatbots/Task-oriented Dialogue Systems),"Conversational AI agents or task-oriented dialogue systems need to perform actions in the real world or retrieve specific, dynamic information that goes beyond their internal language generation capabilities (e.g., checking weather, booking appointments).","Dialogue systems where users might ask for information or actions that require interacting with external services, databases, or tools via Application Programming Interfaces (APIs).","Train language models to identify when an external API call is necessary, generate the correct parameters for that API call based on the user's intent and dialogue context, execute the API call, and then integrate the API's response back into the conversation or task flow in a natural and helpful manner.","Significantly enhances the functionality and utility of dialogue agents by allowing them to interface with external tools and services, making them more capable of performing real-world tasks and providing accurate, dynamic information. Reduces reliance on expensive, manually curated datasets for policy learning.","['WebGPT (browser interaction as a form of API call)', 'SayCan (robot affordance model as an API)', 'ReAct (actions can be API calls)']","['Task-oriented dialogue systems', 'Chatbots', 'Personal assistants', 'Interactive information retrieval']",,49,
312,SayCan (Grounded Robotic Action Planning),"Large Language Models (LLMs) can generate high-level plans in natural language, but grounding these abstract plans into concrete, executable actions for a physical robot in a real-world, dynamic environment is challenging due to the need for physical feasibility and context awareness.","Robotic control tasks where high-level natural language instructions from humans need to be translated into a sequence of physically feasible and effective robot actions, considering the robot's capabilities and the current environment state.","Combine the planning and reasoning capabilities of an LLM with an affordance model. The LLM is prompted to suggest a set of possible actions a robot could take to achieve a goal. An affordance model, which is grounded in the visual observations of the physical environment, then reranks these LLM-generated actions based on their physical feasibility, likelihood of success, and alignment with the current environmental context.","Enables robots to follow high-level natural language instructions by effectively grounding LLM-generated plans in the physical world. This leads to more robust, capable, and adaptable robotic agents that can perform complex tasks in real-world settings.","['Inner Monologue (IM)', 'API Call Integration (affordance model as a specialized tool/API)', 'ReAct (for more flexible reasoning alongside action planning)']","['Robotic action planning', 'Embodied AI', 'Human-robot interaction']",,8,
313,Inner Monologue (IM),"Embodied agents need to effectively track their progress, understand the current environment state, and plan their next steps in complex, long-horizon tasks. Simple action generation might not provide sufficient internal state or reasoning guidance.",Embodied AI tasks where agents interact with a physical or simulated environment and need a mechanism to reason about their observations and high-level goals to inform their actions.,"Introduce an 'inner monologue' for the embodied agent, which is implemented as injected textual feedback from the environment or internal states. This monologue explicitly reiterates observations of the environment state and what needs to be completed by the agent for the goal to be satisfied, thereby motivating and guiding the agent's actions.","Improves embodied reasoning and planning by providing the agent with a textual representation of its current state and goal progress. This helps the agent make more informed decisions and track subgoals, enhancing its ability to complete complex tasks.","[""ReAct (ReAct's reasoning traces offer a more flexible and diverse form of inner monologue)"", 'SayCan (Grounded Robotic Action Planning)']","['Embodied reasoning', 'Robotic planning', 'Text-based games (in a limited form)']",,8,
314,Retrieval-Augmented Generation (RAG),"Large Language Models (LLMs) often lack access to up-to-date or domain-specific knowledge, can hallucinate, and struggle with long-tail information, requiring frequent, costly retraining for knowledge updates or domain adaptation.","When LLMs need to leverage external, dynamic, or specialized knowledge beyond their pre-training data, or when grounded, attributable, and up-to-date responses are critical for knowledge-intensive tasks.","Integrate an external retrieval mechanism with an LLM. A retriever component first fetches relevant information (e.g., documents, passages, facts) from a vast, up-to-date corpus based on a given query or context. These retrieved contexts are then provided as additional input to the LLM, which uses this information to generate a more informed, accurate, and grounded response.","Enables LLMs to access and incorporate external knowledge, reducing hallucinations, providing current information, and adapting to specific domains without modifying the model's weights. Enhances the factual accuracy and trustworthiness of generated content.","['Unified LLM for Retrieve-Rerank-Generate', 'Context Reranking with LLM', 'Multi-Stage Instruction Tuning']","['Knowledge-intensive Question Answering (QA)', 'Fact Verification', 'Conversational AI', 'Personalized Content Generation', 'Document Summarization']","['Generative AI Patterns', 'LLM-specific Patterns', 'Knowledge & Reasoning Patterns']",3,
315,Unified LLM for Retrieve-Rerank-Generate,"Traditional RAG pipelines often rely on separate, specialized models for retrieval, reranking, and generation. This can lead to inefficiencies, limited zero-shot generalization for external rerankers, and challenges in managing the trade-off between recall (with large context sets) and precision/efficiency (with small, highly relevant context sets).","When aiming to streamline and enhance the RAG pipeline by having a single LLM perform both context ranking (reranking) and answer generation, especially to improve robustness against imperfect initial retrieval and leverage the LLM's inherent reasoning capabilities across stages.","An LLM is instruction-tuned to perform both context ranking (reranking) and answer generation within a 'Retrieve-Rerank-Generate' inference pipeline. The steps are: 1. An initial retriever fetches a broader set of 'top-N' contexts. 2. The instruction-tuned LLM then evaluates these 'top-N' contexts for relevance to the query, reranks them, and selects a refined 'top-k' subset (where k < N). 3. The *same* instruction-tuned LLM then uses these 'top-k' contexts to generate the final answer.","Significantly improves the quality of retrieved contexts and the accuracy of generated answers, outperforming traditional RAG setups with separate rerankers. Enhances zero-shot generalization, robustness to various retrievers, and data efficiency for ranking, as the model's ranking and generation capabilities mutually enhance each other.","['Retrieval-Augmented Generation (RAG)', 'Context Reranking with LLM', 'Multi-Stage Instruction Tuning', 'Unified Input Format for Multi-Task Instruction Tuning']","['Knowledge-intensive NLP tasks requiring high accuracy', 'Open-domain Question Answering (QA)', 'Fact Verification', 'Conversational QA in noisy retrieval environments']","['Generative AI Patterns', 'Agentic AI Patterns', 'LLM-specific Patterns', 'Planning Patterns']",20,
316,Multi-Stage Instruction Tuning,"Training LLMs for complex, specialized tasks (like advanced RAG) while retaining strong general instruction-following capabilities can be challenging. A single-stage tuning approach might overfit to specific tasks or dilute general abilities.","When developing LLMs that require both broad instruction-following proficiency and deep, integrated expertise in particular multi-faceted tasks, suggesting a progressive skill acquisition approach.","The LLM undergoes a structured training process in multiple distinct stages: 
1.  **Stage I (Supervised Fine-Tuning - SFT):** The LLM is initially fine-tuned on a diverse blend of high-quality, general instruction-following datasets (e.g., conversational data, long-form QA, LLM-generated instructions). This stage aims to instill robust basic instruction-following abilities and improve zero-shot performance across a wide range of tasks.
2.  **Stage II (Unified Instruction-Tuning for Specialized Tasks):** Building upon the foundation from Stage I, the LLM is further instruction-tuned on a specialized blend of datasets that specifically target the desired complex capabilities (e.g., context-rich QA, retrieval-augmented QA, context ranking, retrieval-augmented ranking data, often unified under a common input format).","Produces LLMs with both strong general instruction-following capabilities and enhanced, integrated performance on specialized, complex tasks. This staged approach allows for systematic skill development, preventing catastrophic forgetting of general abilities while fostering deep expertise.","['Instruction Tuning', 'Unified LLM for Retrieve-Rerank-Generate', 'Unified Input Format for Multi-Task Instruction Tuning']","['Developing specialized LLMs from foundational models', 'Enhancing RAG capabilities with integrated ranking and generation', 'Improving zero-shot generalization across diverse NLP tasks']","['LLM-specific Patterns', 'Generative AI Patterns']",25,
317,Context Reranking with LLM,"Initial retrieval mechanisms (e.g., dense retrievers) often return a list of contexts that may contain irrelevant or noisy passages alongside relevant ones. Providing too many contexts can overwhelm the LLM and degrade generation quality, while relying solely on a small 'top-k' from initial retrieval might compromise recall.","In Retrieval-Augmented Generation (RAG) pipelines, particularly when the quality of initial retrieval is variable, and there's a need to precisely filter and prioritize the most relevant contexts before they are consumed by the LLM for answer generation.","An LLM is specifically trained or adapted to act as a reranker. After an initial retriever fetches a set of 'top-N' candidate contexts, the LLM assesses the relevance of each context (or a small batch of contexts) to the query. This assessment can be done by having the LLM generate a relevance score or a binary 'True/False' output indicating relevance. Based on these scores, the contexts are reordered, and a smaller, highly relevant subset of 'top-k' contexts is selected for the final generation step.","Significantly improves the precision of the contexts fed to the LLM, leading to more accurate and less error-prone generations. Enhances the robustness of the RAG system to noisy initial retrieval and can achieve strong reranking performance even with a modest amount of ranking-specific training data.","['Retrieval-Augmented Generation (RAG)', 'Unified LLM for Retrieve-Rerank-Generate', 'Instruction Tuning for RAG Enhancement']","['Filtering noisy retrieved documents in RAG', 'Improving precision in knowledge-intensive QA', 'Enhancing relevance in document summarization', 'Information retrieval systems']","['LLM-specific Patterns', 'Knowledge & Reasoning Patterns']",20,
318,Unified Input Format for Multi-Task Instruction Tuning,"When instruction-tuning an LLM for multiple diverse tasks (e.g., standard QA, conversational QA, context ranking, retrieval-augmented ranking) that naturally have different input/output structures, it can be challenging to blend these tasks effectively and enable knowledge transfer. Inconsistent formats can confuse the model and hinder learning.","Instruction tuning an LLM for a suite of related but structurally distinct NLP tasks, aiming for a single model to acquire multiple capabilities and benefit from synergistic learning across these tasks.","All diverse training tasks are cast into a single, standardized input-output format. This involves designing specific instruction templates that consistently structure the 'question/instruction', 'context', and 'target output/answer' for every task type. For example, a context ranking task (e.g., 'Is passage relevant to question?') and a QA task (e.g., 'Answer question from passage') are both framed as a question (x), context (c), and target output (y), facilitating unified training and knowledge transfer.","Enables an LLM to effectively learn and generalize across multiple diverse tasks with a unified instruction-tuning approach. Facilitates knowledge transfer between tasks, allowing the model to leverage insights from one task to improve another. Simplifies data preparation and the training pipeline for multi-task learning.","['Instruction Tuning', 'Multi-Stage Instruction Tuning', 'Unified LLM for Retrieve-Rerank-Generate']","['Multi-task learning for LLMs', 'Instruction tuning for complex RAG frameworks', 'Developing generalist LLMs with specialized skills', 'Creating robust models for diverse NLP benchmarks']","['Prompt Design Patterns', 'LLM-specific Patterns']",25,
319,Plan-and-Solve Paradigm,LLMs need to unleash their reasoning ability to handle complex tasks.,Complex tasks that require LLMs to perform multi-step reasoning.,Prompt LLMs to first generate a plan and then execute each reasoning step. This allows LLMs to decompose complex reasoning tasks into a series of subtasks and solve them step by step.,"LLMs can decompose complex reasoning tasks into a series of subtasks and solve them step by step, improving their ability to handle complexity.","['Chain-of-Thought Prompting', 'Tree of Thoughts', 'Graph of Thoughts', 'Planning Module (RoG Specific)']",['Complex reasoning tasks'],Planning Patterns,6,
320,Knowledge Graph Augmentation,"Large Language Models (LLMs) lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness.","LLMs are used for reasoning tasks where factual accuracy, up-to-date information, and trustworthiness are critical (e.g., legal judgment, medical diagnosis, KGQA).","Incorporate Knowledge Graphs (KGs), which capture vast amounts of facts in a structured format, to offer a reliable source of knowledge for reasoning.","Tackles issues of lack of knowledge and hallucinations, improving the reasoning ability and trustworthiness of LLMs by providing a faithful knowledge source.","['Retrieval-Augmented Generation', 'Reasoning on Graphs (RoG Framework)', 'Knowledge-Driven CoT (KDCoT)']","['Improving LLM reasoning ability', 'Knowledge Graph Question Answering (KGQA)', 'Reducing hallucinations']",Knowledge & Reasoning Patterns,40,
321,Semantic Parsing (for KGQA),"Obtaining answers based on knowledge from Knowledge Graphs (KGs) where high accuracy and interpretability are desired, but direct LLM reasoning might be less precise. Generated logical queries can often be non-executable and yield no answers due to syntax and semantic limitations.",Knowledge Graph Question Answering (KGQA) tasks requiring precise interaction with structured knowledge.,"Use LLMs to convert natural language questions into formal logical queries (e.g., SPARQL) that are executed on KGs to obtain answers.","Can generate more accurate and interpretable results by leveraging reasoning on KGs. However, the generated logical queries can often be non-executable and yield no answers.","['Retrieval-Augmented Generation', 'Reasoning on Graphs (RoG Framework)']","['Knowledge Graph Question Answering (KGQA)', 'Generating structural queries for KGs']",Classical AI,40,
322,Retrieval-Augmented Generation (RAG - General),"LLMs are limited by their static training data, leading to knowledge cut-offs, hallucinations, and inability to access external or proprietary information, making them prone to errors in knowledge-intensive tasks.","LLMs need to provide responses based on current, specific, or external factual information not present in their training data, particularly for Knowledge Graph Question Answering (KGQA).","Retrieve relevant external information (e.g., triples from KGs) as knowledge context and use LLMs to obtain the final answers.","More flexible than semantic parsing and exploits the ability of LLMs for reasoning. However, it often only treats KGs as factual knowledge bases and overlooks structural information.","['Knowledge Graph Augmentation', 'Reasoning on Graphs (RoG Framework)', 'Knowledge-Driven CoT (KDCoT)']","['Knowledge Graph Question Answering (KGQA)', 'Improving reasoning performance with external facts']",Generative AI Patterns,40,
323,Reasoning on Graphs (RoG Framework),"LLMs suffer from hallucinations and lack of knowledge, and existing KG-based methods only treat KGs as factual knowledge bases, overlooking the importance of their structural information for faithful and interpretable reasoning.","Synergizing LLMs with KGs to conduct faithful and interpretable reasoning for complex tasks, particularly Knowledge Graph Question Answering (KGQA).",A planning-retrieval-reasoning framework where RoG first generates relation paths grounded by KGs as faithful plans via a planning module. These plans are then used to retrieve valid reasoning paths from KGs by a retrieval module for LLMs to conduct faithful reasoning via a reasoning module. It also distills knowledge from KGs to improve LLM reasoning through training.,Achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results. RoG not only distills knowledge from KGs to improve reasoning ability but also allows seamless integration with any arbitrary LLMs during inference.,"['Plan-and-Solve Paradigm', 'Knowledge Graph Augmentation', 'Retrieval-Augmented Generation', 'Planning Module (RoG Specific)', 'Retrieval Module (RoG Specific)', 'Reasoning Module (RoG Specific)']","['Knowledge Graph Question Answering (KGQA)', 'Faithful and interpretable reasoning']",Knowledge & Reasoning Patterns,24,
324,Planning Module (RoG Specific),"LLMs are prone to generating incorrect plans due to hallucination issues, leading to wrong answers, especially when they have zero knowledge of specific relations contained in KGs.","Within the RoG framework, LLMs need to generate reliable, KG-grounded plans for multi-step reasoning, specifically for KGQA tasks.",Design a simple instruction template that prompts LLMs to generate relation paths (a sequence of relations) structurally formatted as a sentence. These relation paths are grounded by KGs and serve as faithful plans. This module is optimized via 'planning optimization' to distill KG knowledge into the LLM.,"Generates faithful relation paths as plans, ensuring plans are grounded by KGs, which enables LLMs to conduct faithful and interpretable reasoning on graphs. This module can be plug-and-play with different LLMs during inference.","['Plan-and-Solve Paradigm', 'Reasoning on Graphs (RoG Framework)', 'Planning Optimization (RoG Specific)', 'Plug-and-Play Planning Module']","['Generating KG-grounded plans', 'Multi-hop question answering', 'Ensuring plan faithfulness']",Planning Patterns,24,
325,Retrieval Module (RoG Specific),"Given a high-level plan (a relation path), the system needs to find concrete, factual instances of that plan within the Knowledge Graph to support reasoning.","Following a generated plan (relation path) to gather specific factual evidence from a Knowledge Graph for LLM reasoning, as part of the RoG framework.",Conduct retrieval by finding paths in the KG that start from the question entities and follow the relation paths. A constrained breadth-first search is adopted to retrieve reasoning paths (sequences of entities and relations).,"Retrieves valid reasoning paths from KGs according to the plans, providing concrete, factual evidence for LLM reasoning.","['Retrieval-Augmented Generation', 'Reasoning on Graphs (RoG Framework)']","['Instantiating relation path plans with factual data', 'Extracting factual evidence from KGs for LLM reasoning']",Tools Integration Patterns,24,
326,Reasoning Module (RoG Specific),"Even with retrieved reasoning paths, LLMs might not correctly understand them or struggle to identify important paths from noisy or irrelevant ones, leading to incorrect answers.","LLMs need to synthesize information from a set of retrieved reasoning paths to generate accurate answers and interpretable explanations, as part of the RoG framework.","Takes the question and a set of retrieved reasoning paths (formatted as structural sentences) as input. A reasoning instruction prompt guides LLMs to conduct reasoning based on these paths, identifying important ones and generating answers. This module is optimized via 'retrieval-reasoning optimization'.","Enables LLMs to identify important reasoning paths, filter out noise, conduct faithful reasoning, and generate answers with interpretable explanations, improving precision despite potential noise in retrieved data.","['Reasoning on Graphs (RoG Framework)', 'Retrieval-Reasoning Optimization (RoG Specific)', 'Few-Shot Prompting (for Explanations)']","['Generating answers from retrieved evidence', 'Producing interpretable explanations', 'Filtering noisy information for reasoning']",LLM-specific Patterns,24,
327,Planning Optimization (RoG Specific),LLMs have zero knowledge of relations contained in KGs and therefore cannot directly generate faithful relation paths grounded by KGs as plans.,Training LLMs within the RoG framework to generate valid and faithful plans (relation paths) that are grounded in a target Knowledge Graph.,"An instruction tuning task that distills knowledge from KGs into LLMs to generate faithful relation paths as plans. This is achieved by minimizing the KL divergence with the posterior distribution of faithful relation paths, which can be approximated by the valid (shortest) relation paths in KGs.","Maximizes the probability of LLMs generating faithful relation paths through distilling the knowledge from KGs, enabling more reliable planning.","['Instruction Tuning', 'Planning Module (RoG Specific)']","['Fine-tuning LLMs for domain-specific planning', 'Knowledge distillation into LLMs']",MLOps Patterns,24,
328,Retrieval-Reasoning Optimization (RoG Specific),"LLMs might not understand the retrieved reasoning paths correctly and conduct reasoning based on them, leading to errors.",Training LLMs within the RoG framework to effectively reason based on a set of retrieved reasoning paths to produce correct answers.,"An instruction tuning task that enables LLMs to conduct reasoning based on the retrieved reasoning paths. This maximizes the probability of LLMs generating correct answers based on the retrieved reasoning paths, potentially leveraging frameworks like FiD for multiple paths.","Enables LLMs to conduct faithful reasoning based on retrieved paths and generate interpretable results, improving their ability to utilize external evidence.","['Instruction Tuning', 'Reasoning Module (RoG Specific)']","['Fine-tuning LLMs for retrieval-augmented reasoning', 'Improving answer generation from external evidence']",MLOps Patterns,24,
329,ReACT (Reasoning and Acting),"LLMs often operate in a static, pre-trained knowledge space and cannot dynamically interact with external environments to get the latest knowledge for reasoning.",LLMs performing complex tasks that require dynamic information retrieval and interaction with external tools or environments.,Treats LLMs as agents which interact with the environment to get the latest knowledge for reasoning.,Enables LLMs to dynamically gather up-to-date knowledge for reasoning through environmental interaction.,"['ThinkonGraph', 'KGAgent']","['Dynamic knowledge acquisition', 'Tool use by LLMs', 'Agentic behavior']",Agentic AI Patterns,11,
330,Chain-of-Thought (CoT) Prompting,"LLMs need to harness their reasoning ability to handle complex tasks, often providing incorrect answers or superficial reasoning.","Improving LLM performance on complex reasoning tasks (e.g., arithmetic, common sense, symbolic reasoning) without extensive fine-tuning.","Enables LLMs to generate a reasoning chain (intermediate reasoning steps) that could be helpful to reasoning, typically through prompting.","Elicits reasoning in large language models, leading to more accurate answers and providing transparency into the model's thought process.","['Plan-and-Solve Paradigm', 'Tree of Thoughts', 'Graph of Thoughts']","['Complex reasoning tasks', 'Improving explainability of LLM outputs']",Prompt Design Patterns,26,
331,Tree of Thoughts (ToT),"Linear Chain-of-Thought reasoning can be insufficient for problems requiring exploration, backtracking, or considering multiple reasoning paths.","Complex reasoning tasks that benefit from exploring diverse intermediate thoughts and evaluating their progress, similar to human problem-solving.",Expands the reasoning chain to a tree structure to explore more reasoning paths.,"Allows for more deliberate and systematic problem-solving by exploring multiple reasoning paths, evaluating intermediate steps, and improving the quality of the final solution for tasks requiring search and planning.","['Chain-of-Thought Prompting', 'Graph of Thoughts', 'Plan-and-Solve Paradigm']","['Complex reasoning', 'Problem-solving requiring search and planning']",Planning Patterns,6,
332,Graph of Thoughts (GoT),"Tree of Thoughts can still be limited by its tree structure, as it doesn't naturally support arbitrary dependencies or aggregation across different reasoning branches.","Reasoning tasks that involve complex interdependencies between thoughts, requiring a more flexible structure than a tree and the ability to aggregate information from multiple lines of reasoning.",Models the reasoning chain as a graph with an aggregation operation to synergize the reasoning paths.,"Provides a highly flexible framework for complex reasoning, enabling more sophisticated aggregation and interaction between diverse thoughts, potentially leading to more robust and accurate solutions.","['Chain-of-Thought Prompting', 'Tree of Thoughts']","['Elaborate problem-solving', 'Complex logical deduction']",Planning Patterns,6,
333,Instruction Tuning,"Large Language Models (LLMs) may not effectively follow specific instructions or perform new tasks without explicit guidance, or they may perform suboptimally on specialized tasks.","Adapting pre-trained LLMs to follow instructions more effectively, perform specific tasks, or align with desired behaviors.","Fine-tune a pre-trained LLM on a dataset consisting of instruction-response pairs, where the instruction explicitly describes the task.","Greatly improves the LLM's ability to understand and follow instructions, enhances its performance on a wide range of tasks, and can specialize its behavior for specific domains or applications.","['Planning Optimization (RoG Specific)', 'Retrieval-Reasoning Optimization (RoG Specific)']","['Customizing LLMs for specific applications', 'Improving instruction following', 'Aligning LLM behavior']",MLOps Patterns,25,
334,Plug-and-Play Planning Module,"Integrating specialized planning capabilities with various existing LLMs often requires retraining or complex adaptation, limiting flexibility and widespread adoption.","Enhancing the performance of arbitrary, pre-existing LLMs on tasks requiring structured planning (e.g., KG traversal) without needing to retrain the entire LLM.","Adopt a separate, trained planning module (like RoG's planning module) to generate relation paths (plans). These plans are executed on KGs to retrieve reasoning paths, which are then fed as context into different LLMs using reasoning prompts during inference.","Substantially improves the performance of various LLMs by providing them with structured, faithful plans and retrieved knowledge, without requiring retraining of the target LLMs, thus offering seamless integration and flexibility.","['Planning Module (RoG Specific)', 'Tools Integration Patterns']","['Enhancing diverse LLMs with external planning capabilities', 'Modular AI system design']",Tools Integration Patterns,24,
335,Few-Shot Prompting (for Explanations),"LLMs may struggle to generate high-quality, structured, or specific types of outputs (like explanations) without clear examples of the desired format and content.","Guiding LLMs to produce interpretable and well-structured explanations for their reasoning, particularly when the desired explanation format is complex or specific.",Design an 'Explanation Prompt Template' that includes a few-shot human-annotated examples to demonstrate the explanation process.,"Helps empower the method in generating results with good explainability by guiding the LLM to understand the desired explanation style, structure, and level of detail.","['Prompt Design Patterns', 'Reasoning Module (RoG Specific)']","['Generating structured explanations', 'Improving interpretability of LLM outputs']",Prompt Design Patterns,18,
336,Zero-Shot Prompting,"Evaluating baseline LLM performance on a task, such as KGQA, without any specific training examples or fine-tuning.",Using LLMs for KGQA tasks or other general tasks without prior examples or specific training.,"Directly ask LLMs to answer the question using a clear instruction or question, without providing any input-output examples.","Provides a baseline performance for LLMs on tasks like KGQA, demonstrating their inherent capabilities.",['Prompt Design Patterns'],"['Baseline evaluation of LLMs', 'General knowledge question answering']",Prompt Design Patterns,18,
